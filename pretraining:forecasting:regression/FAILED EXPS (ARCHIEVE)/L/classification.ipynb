{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTDFCBgnk1o3"
   },
   "source": [
    "# Strats+Text Target Task\n",
    "\n",
    "- binary classification\n",
    "\n",
    "- physiological features + clinical text\n",
    "\n",
    "- best pretrained model: epoch = 125"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECKAJiLXiTlN"
   },
   "source": [
    "## Hardware check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e7odgm5VsOsb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 18 22:21:41 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    71W / 300W |      9MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    42W / 300W |      9MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    42W / 300W |      9MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    45W / 300W |      9MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1926      G   /usr/libexec/Xorg                   8MiB |\n",
      "|    1   N/A  N/A      1926      G   /usr/libexec/Xorg                   8MiB |\n",
      "|    2   N/A  N/A      1926      G   /usr/libexec/Xorg                   8MiB |\n",
      "|    3   N/A  N/A      1926      G   /usr/libexec/Xorg                   8MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# gpu check\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oEEUeIW3sOsc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of cores\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() \n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY9I5eXWeOom"
   },
   "source": [
    "## Environment Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis\n"
     ]
    }
   ],
   "source": [
    "cd /pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 22:21:43.181423: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-18 22:21:43.265550: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-18 22:22:06.533345: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Lambda\n",
    "# from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, pipeline, AutoModel\n",
    "import resources.smart_cond as sc\n",
    "# from google.colab import files\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfKWDkwoeSGU"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_path = 'Data/sepsis_removed_0.pkl'\n",
    "# pkl = pickle.load(open(data_path, 'rb'))\n",
    "# data = pkl[0]\n",
    "# oc = pkl[1]\n",
    "# train_ind = pkl[2]\n",
    "# valid_ind = pkl[3]\n",
    "# test_ind = pkl[4]\n",
    "# del pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Filter labeled data in first 24h.\n",
    "# data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# data = data.loc[(data.hour>=0)&(data.hour<=24)]\n",
    "# oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# # Get y and N.\n",
    "# y = np.array(oc.sort_values(by='ts_ind')['in_hospital_sepsis']).astype('float32')\n",
    "# N = data.ts_ind.max() + 1\n",
    "# # Get static data with mean fill and missingness indicator.\n",
    "# static_varis = ['Age', 'Gender']\n",
    "# ii = data.variable.isin(static_varis)\n",
    "# static_data = data.loc[ii]\n",
    "# data = data.loc[~ii]\n",
    "# def inv_list(l, start=0):\n",
    "#     d = {}\n",
    "#     for i in range(len(l)):\n",
    "#         d[l[i]] = i+start\n",
    "#     return d\n",
    "# static_var_to_ind = inv_list(static_varis)\n",
    "# D = len(static_varis)\n",
    "# demo = np.zeros((N, D))\n",
    "# for row in tqdm(static_data.itertuples()):\n",
    "#     demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# # Normalize static data.\n",
    "# means = demo.mean(axis=0, keepdims=True)\n",
    "# stds = demo.std(axis=0, keepdims=True)\n",
    "# stds = (stds==0)*1 + (stds!=0)*stds\n",
    "# demo = (demo-means)/stds\n",
    "# # Trim to max len.\n",
    "# data = data.sample(frac=1)\n",
    "# data = data.groupby('ts_ind').head(880)\n",
    "# # Get N, V, var_to_ind.\n",
    "# N = data.ts_ind.max() + 1\n",
    "# varis = sorted(list(set(data.variable)))\n",
    "# V = len(varis)\n",
    "# def inv_list(l, start=0):\n",
    "#     d = {}\n",
    "#     for i in range(len(l)):\n",
    "#         d[l[i]] = i+start\n",
    "#     return d\n",
    "# var_to_ind = inv_list(varis, start=1)\n",
    "# data['vind'] = data.variable.map(var_to_ind)\n",
    "# data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# # Add obs index.\n",
    "# data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "# data = data.reset_index().rename(columns={'index':'obs_ind'})\n",
    "# data = data.merge(data.groupby('ts_ind').agg({'obs_ind':'min'}).reset_index().rename(columns={ \\\n",
    "#                                                             'obs_ind':'first_obs_ind'}), on='ts_ind')\n",
    "# data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# # Find max_len.\n",
    "# max_len = data.obs_ind.max()+1\n",
    "# print ('max_len', max_len)\n",
    "# # Generate times_ip and values_ip matrices.\n",
    "# # times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "# # values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "# # varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "# texts_inp = np.empty([N, max_len], dtype=object)\n",
    "# for row in tqdm(data.itertuples()):\n",
    "#     ts_ind = row.ts_ind\n",
    "#     l = row.obs_ind\n",
    "#     # times_inp[ts_ind, l] = row.hour\n",
    "#     if isinstance(row.value, str):\n",
    "#         # values_inp[ts_ind, l] = 1.0\n",
    "#         texts_inp[ts_ind, l] = row.value\n",
    "#     else:\n",
    "#         # values_inp[ts_ind, l] = row.value\n",
    "#         texts_inp[ts_ind, l] = ''\n",
    "#     # varis_inp[ts_ind, l] = row.vind\n",
    "    \n",
    "# data.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\n",
    "# # Generate 3 sets of inputs and outputs.\n",
    "# # train_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp, texts_inp]]\n",
    "# # valid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp, texts_inp]]\n",
    "# # test_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp, texts_inp]]\n",
    "# # del times_inp, values_inp, varis_inp\n",
    "# train_ip = [ip[train_ind] for ip in [texts_inp]]\n",
    "# valid_ip = [ip[valid_ind] for ip in [texts_inp]]\n",
    "# test_ip = [ip[test_ind] for ip in [texts_inp]]\n",
    "# del texts_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_texts = train_ip[0]\n",
    "# valid_texts = valid_ip[0]\n",
    "# test_texts = test_ip[0]\n",
    "# del train_ip, valid_ip, test_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concat_train_texts = []\n",
    "# for train_text in tqdm(train_texts):\n",
    "#     train_text = train_text[train_text != None]\n",
    "#     train_text = train_text[train_text != '']\n",
    "#     concat_text = ' '.join(list(train_text))\n",
    "#     concat_train_texts.append(concat_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concat_valid_texts = []\n",
    "# for valid_text in tqdm(valid_texts):\n",
    "#     valid_text = valid_text[valid_text != None]\n",
    "#     valid_text = valid_text[valid_text != '']\n",
    "#     concat_text = ' '.join(list(valid_text))\n",
    "#     concat_valid_texts.append(concat_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concat_test_texts = []\n",
    "# for test_text in tqdm(test_texts):\n",
    "#     test_text = test_text[test_text != None]\n",
    "#     test_text = test_text[test_text != '']\n",
    "#     concat_text = ' '.join(list(test_text))\n",
    "#     concat_test_texts.append(concat_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del train_texts, valid_texts, test_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from simpletransformers.language_representation import RepresentationModel\n",
    "# from simpletransformers.config.model_args import ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_args = ModelArgs(max_seq_length=512, silent = False)\n",
    "# model = RepresentationModel(\n",
    "#     \"bert\", \"emilyalsentzer/Bio_ClinicalBERT\", args=model_args)\n",
    "# train_text_features = model.encode_sentences(concat_train_texts, combine_strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_args = ModelArgs(max_seq_length=512, silent = False)\n",
    "# model = RepresentationModel(\n",
    "#     \"bert\", \"emilyalsentzer/Bio_ClinicalBERT\", args=model_args)\n",
    "# valid_text_features = model.encode_sentences(concat_valid_texts, combine_strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_args = ModelArgs(max_seq_length=512, silent = False)\n",
    "# model = RepresentationModel(\n",
    "#     \"bert\", \"emilyalsentzer/Bio_ClinicalBERT\", args=model_args)\n",
    "# test_text_features = model.encode_sentences(concat_test_texts, combine_strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # dump to json just in case\n",
    "# pickle.dump([train_text_features, valid_text_features, test_text_features], open('text_features_target.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del concat_train_texts, concat_valid_texts, concat_test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_path = 'Data/text_features_target.pkl'\n",
    "# train_text, valid_text, test_text = pickle.load(open(data_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Data/classification_embs_1.pkl'\n",
    "train_text, valid_text, test_text = pickle.load(open(data_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from keras.utils import pad_sequences\n",
    "# train_text_features = pad_sequences(train_text, maxlen=33792, padding='post')\n",
    "# train_text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_text_features = pad_sequences(valid_text, maxlen=33792, padding='post')\n",
    "# valid_text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_text_features = pad_sequences(test_text, maxlen=33792, padding='post')\n",
    "# test_text_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = 'Data/sepsis_removed_0.pkl'\n",
    "pkl = pickle.load(open(data_path, 'rb'))\n",
    "data = pkl[0]\n",
    "oc = pkl[1]\n",
    "train_ind = pkl[2]\n",
    "valid_ind = pkl[3]\n",
    "test_ind = pkl[4]\n",
    "del pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_ind</th>\n",
       "      <th>hour</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>TABLE</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10223</td>\n",
       "      <td>467.816667</td>\n",
       "      <td>Text</td>\n",
       "      <td>1</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18407</td>\n",
       "      <td>28.016667</td>\n",
       "      <td>Text</td>\n",
       "      <td>1</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40300</td>\n",
       "      <td>155.166667</td>\n",
       "      <td>Text</td>\n",
       "      <td>1</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23747</td>\n",
       "      <td>52.383333</td>\n",
       "      <td>Text</td>\n",
       "      <td>1</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2357</td>\n",
       "      <td>73.133333</td>\n",
       "      <td>Text</td>\n",
       "      <td>1</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886223</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>MBP</td>\n",
       "      <td>0.195381</td>\n",
       "      <td>chart</td>\n",
       "      <td>78.552377</td>\n",
       "      <td>17.645628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886224</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>O2 Saturation</td>\n",
       "      <td>-0.678068</td>\n",
       "      <td>chart</td>\n",
       "      <td>96.820961</td>\n",
       "      <td>4.160290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886225</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>RR</td>\n",
       "      <td>0.179866</td>\n",
       "      <td>chart</td>\n",
       "      <td>26.278501</td>\n",
       "      <td>15.130729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886226</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>SBP</td>\n",
       "      <td>-0.404061</td>\n",
       "      <td>chart</td>\n",
       "      <td>120.239648</td>\n",
       "      <td>25.341836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886227</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>Urine</td>\n",
       "      <td>-0.24296</td>\n",
       "      <td>output</td>\n",
       "      <td>123.393012</td>\n",
       "      <td>137.442433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82447326 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ts_ind        hour       variable     value       TABLE        mean  \\\n",
       "0          10223  467.816667           Text         1  noteevents    1.000000   \n",
       "1          18407   28.016667           Text         1  noteevents    1.000000   \n",
       "2          40300  155.166667           Text         1  noteevents    1.000000   \n",
       "3          23747   52.383333           Text         1  noteevents    1.000000   \n",
       "4           2357   73.133333           Text         1  noteevents    1.000000   \n",
       "...          ...         ...            ...       ...         ...         ...   \n",
       "82886223   57281   20.400000            MBP  0.195381       chart   78.552377   \n",
       "82886224   57281   20.400000  O2 Saturation -0.678068       chart   96.820961   \n",
       "82886225   57281   20.400000             RR  0.179866       chart   26.278501   \n",
       "82886226   57281   20.400000            SBP -0.404061       chart  120.239648   \n",
       "82886227   57281   20.400000          Urine  -0.24296      output  123.393012   \n",
       "\n",
       "                 std  \n",
       "0           1.000000  \n",
       "1           1.000000  \n",
       "2           1.000000  \n",
       "3           1.000000  \n",
       "4           1.000000  \n",
       "...              ...  \n",
       "82886223   17.645628  \n",
       "82886224    4.160290  \n",
       "82886225   15.130729  \n",
       "82886226   25.341836  \n",
       "82886227  137.442433  \n",
       "\n",
       "[82447326 rows x 7 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['variable'] == 'Text', 'value'] = 1\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114564it [00:00, 766385.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19267073it [00:28, 677920.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Filter labeled data in first 24h.\n",
    "data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "data = data.loc[(data.hour>=0)&(data.hour<=24)]\n",
    "oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "# Get y and N.\n",
    "y = np.array(oc.sort_values(by='ts_ind')['in_hospital_sepsis']).astype('float32')\n",
    "N = data.ts_ind.max() + 1\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# Trim to max len.\n",
    "data = data.sample(frac=1)\n",
    "data = data.groupby('ts_ind').head(880)\n",
    "# Get N, V, var_to_ind.\n",
    "N = data.ts_ind.max() + 1\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Add obs index.\n",
    "data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "data = data.reset_index().rename(columns={'index':'obs_ind'})\n",
    "data = data.merge(data.groupby('ts_ind').agg({'obs_ind':'min'}).reset_index().rename(columns={ \\\n",
    "                                                            'obs_ind':'first_obs_ind'}), on='ts_ind')\n",
    "data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# Find max_len.\n",
    "max_len = data.obs_ind.max()+1\n",
    "print ('max_len', max_len)\n",
    "# Generate times_ip and values_ip matrices.\n",
    "times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "for row in tqdm(data.itertuples()):\n",
    "    ts_ind = row.ts_ind\n",
    "    l = row.obs_ind\n",
    "    times_inp[ts_ind, l] = row.hour\n",
    "    values_inp[ts_ind, l] = row.value\n",
    "    varis_inp[ts_ind, l] = row.vind\n",
    "data.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "valid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "test_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "del times_inp, values_inp, varis_inp\n",
    "# train_op = y[train_ind]\n",
    "# valid_op = y[valid_ind]\n",
    "# test_op = y[test_ind]\n",
    "# del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ip.append(train_text)\n",
    "valid_ip.append(valid_text)\n",
    "test_ip.append(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_path = 'Data/sepsis_removed_0.pkl'\n",
    "pkl = pickle.load(open(data_path, 'rb'))\n",
    "data = pkl[0]\n",
    "oc = pkl[1]\n",
    "train_ind = pkl[2]\n",
    "valid_ind = pkl[3]\n",
    "test_ind = pkl[4]\n",
    "del pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_ind</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>in_hospital_sepsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>110404</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>188028</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>173727</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>164716</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>158689</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54862</th>\n",
       "      <td>57277</td>\n",
       "      <td>182476</td>\n",
       "      <td>83967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29333</th>\n",
       "      <td>57278</td>\n",
       "      <td>118320</td>\n",
       "      <td>23200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54573</th>\n",
       "      <td>57279</td>\n",
       "      <td>146497</td>\n",
       "      <td>73807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14317</th>\n",
       "      <td>57280</td>\n",
       "      <td>118512</td>\n",
       "      <td>10762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430</th>\n",
       "      <td>57281</td>\n",
       "      <td>122737</td>\n",
       "      <td>4094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57282 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ts_ind  HADM_ID  SUBJECT_ID  in_hospital_sepsis\n",
       "0           0   110404         268                   1\n",
       "1           1   188028         270                   0\n",
       "2           2   173727         271                   0\n",
       "3           3   164716         272                   0\n",
       "4           4   158689         273                   0\n",
       "...       ...      ...         ...                 ...\n",
       "54862   57277   182476       83967                   0\n",
       "29333   57278   118320       23200                   0\n",
       "54573   57279   146497       73807                   0\n",
       "14317   57280   118512       10762                   0\n",
       "4430    57281   122737        4094                   0\n",
       "\n",
       "[57282 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc = oc.sort_values(by='ts_ind')\n",
    "oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ts_ind</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>in_hospital_sepsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110404</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>188028</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>173727</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>164716</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>158689</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57277</th>\n",
       "      <td>54862</td>\n",
       "      <td>57277</td>\n",
       "      <td>182476</td>\n",
       "      <td>83967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57278</th>\n",
       "      <td>29333</td>\n",
       "      <td>57278</td>\n",
       "      <td>118320</td>\n",
       "      <td>23200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57279</th>\n",
       "      <td>54573</td>\n",
       "      <td>57279</td>\n",
       "      <td>146497</td>\n",
       "      <td>73807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57280</th>\n",
       "      <td>14317</td>\n",
       "      <td>57280</td>\n",
       "      <td>118512</td>\n",
       "      <td>10762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57281</th>\n",
       "      <td>4430</td>\n",
       "      <td>57281</td>\n",
       "      <td>122737</td>\n",
       "      <td>4094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57282 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  ts_ind  HADM_ID  SUBJECT_ID  in_hospital_sepsis\n",
       "0          0       0   110404         268                   1\n",
       "1          1       1   188028         270                   0\n",
       "2          2       2   173727         271                   0\n",
       "3          3       3   164716         272                   0\n",
       "4          4       4   158689         273                   0\n",
       "...      ...     ...      ...         ...                 ...\n",
       "57277  54862   57277   182476       83967                   0\n",
       "57278  29333   57278   118320       23200                   0\n",
       "57279  54573   57279   146497       73807                   0\n",
       "57280  14317   57280   118512       10762                   0\n",
       "57281   4430   57281   122737        4094                   0\n",
       "\n",
       "[57282 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_oc = oc.reset_index()\n",
    "reset_oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_op = reset_oc['in_hospital_sepsis'][train_ind]\n",
    "valid_op = reset_oc['in_hospital_sepsis'][valid_ind]\n",
    "test_op = reset_oc['in_hospital_sepsis'][test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_op = train_op.to_numpy(dtype='float32')\n",
    "valid_op = valid_op.to_numpy(dtype='float32')\n",
    "test_op = test_op.to_numpy(dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_res(y_true, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    minrp = np.minimum(precision, recall).max()\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return [roc_auc, pr_auc, minrp]\n",
    "\n",
    "######################################################################################################## \n",
    "######################################################################################################## \n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=train_op)\n",
    "def mortality_loss(y_true, y_pred):\n",
    "    sample_weights = (1-y_true)*class_weights[0] + y_true*class_weights[1]\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    return K.mean(sample_weights*bce, axis=-1)\n",
    "######################################################################################################## \n",
    "######################################################################################################## \n",
    "\n",
    "# var_weights = np.sum(fore_train_op[:, V:], axis=0)\n",
    "# var_weights[var_weights==0] = var_weights.max()\n",
    "# var_weights = var_weights.max()/var_weights\n",
    "# var_weights = var_weights.reshape((1, V))\n",
    "def forecast_loss(y_true, y_pred):\n",
    "    return K.sum(y_true[:,V:]*(y_true[:,:V]-y_pred)**2, axis=-1)\n",
    "\n",
    "def get_min_loss(weight):\n",
    "    def min_loss(y_true, y_pred):\n",
    "        return weight*y_pred\n",
    "    return min_loss\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, validation_data, batch_size):\n",
    "        self.val_x, self.val_y = validation_data\n",
    "        self.batch_size = batch_size\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.val_x, verbose=0, batch_size=self.batch_size)\n",
    "        if type(y_pred)==type([]):\n",
    "            y_pred = y_pred[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(self.val_y, y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(self.val_y, y_pred)\n",
    "        logs['custom_metric'] = pr_auc + roc_auc\n",
    "        print ('val_aucs:', pr_auc, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Activation, Dropout, Softmax, Layer, InputSpec, Input, Dense, Lambda, TimeDistributed, Concatenate, Add\n",
    "from tensorflow.keras import initializers, regularizers, constraints, Model\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow import nn\n",
    "\n",
    "    \n",
    "class CVE(Layer):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        super(CVE, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        self.W1 = self.add_weight(name='CVE_W1',\n",
    "                            shape=(1, self.hid_units),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        self.b1 = self.add_weight(name='CVE_b1',\n",
    "                            shape=(self.hid_units,),\n",
    "                            initializer='zeros',\n",
    "                            trainable=True)\n",
    "        self.W2 = self.add_weight(name='CVE_W2',\n",
    "                            shape=(self.hid_units, self.output_dim),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        super(CVE, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = K.expand_dims(x, axis=-1)\n",
    "        x = K.dot(K.tanh(K.bias_add(K.dot(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.output_dim,)\n",
    "    \n",
    "    \n",
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, hid_dim):\n",
    "        self.hid_dim = hid_dim\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        self.W = self.add_weight(shape=(d, self.hid_dim), name='Att_W',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.hid_dim,), name='Att_b',\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(shape=(self.hid_dim,1), name='Att_u',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = K.dot(K.tanh(K.bias_add(K.dot(x,self.W), self.b)), self.u)\n",
    "        mask = K.expand_dims(mask, axis=-1)\n",
    "        attn_weights = mask*attn_weights + (1-mask)*mask_value\n",
    "        attn_weights = K.softmax(attn_weights, axis=-2)\n",
    "        return attn_weights\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (1,)\n",
    "    \n",
    "    \n",
    "class Transformer(Layer):\n",
    "    \n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0):\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = K.epsilon() * K.epsilon()\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        if self.dk==None:\n",
    "            self.dk = d//self.h\n",
    "        if self.dv==None:\n",
    "            self.dv = d//self.h\n",
    "        if self.dff==None:\n",
    "            self.dff = 2*d\n",
    "        self.Wq = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wq',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wk = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wk',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wv = self.add_weight(shape=(self.N, self.h, d, self.dv), name='Wv',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wo = self.add_weight(shape=(self.N, self.dv*self.h, d), name='Wo',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.W1 = self.add_weight(shape=(self.N, d, self.dff), name='W1',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b1 = self.add_weight(shape=(self.N, self.dff), name='b1',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.W2 = self.add_weight(shape=(self.N, self.dff, d), name='W2',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b2 = self.add_weight(shape=(self.N, d), name='b2',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.gamma = self.add_weight(shape=(2*self.N,), name='gamma',\n",
    "                                 initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(shape=(2*self.N,), name='beta',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Transformer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e-30):\n",
    "        mask = K.expand_dims(mask, axis=-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = K.dot(x, self.Wq[i,j,:,:])\n",
    "                k = K.permute_dimensions(K.dot(x, self.Wk[i,j,:,:]), (0,2,1))\n",
    "                v = K.dot(x, self.Wv[i,j,:,:])\n",
    "                A = K.batch_dot(q,k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask*A + (1-mask)*mask_value\n",
    "                # Mask for attention dropout.\n",
    "                def dropped_A():\n",
    "                    dp_mask = K.cast((K.random_uniform(shape=array_ops.shape(A))>=self.dropout), K.floatx())\n",
    "                    return A*dp_mask + (1-dp_mask)*mask_value\n",
    "                A = sc.smart_cond(K.learning_phase(), dropped_A, lambda: array_ops.identity(A))\n",
    "                A = K.softmax(A, axis=-1)\n",
    "                mha_ops.append(K.batch_dot(A,v))\n",
    "            conc = K.concatenate(mha_ops, axis=-1)\n",
    "            proj = K.dot(conc, self.Wo[i,:,:])\n",
    "            # Dropout.\n",
    "            proj = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(proj, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(proj))\n",
    "            # Add & LN\n",
    "            x = x+proj\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i] + self.beta[2*i]\n",
    "            # FFN\n",
    "            ffn_op = K.bias_add(K.dot(K.relu(K.bias_add(K.dot(x, self.W1[i,:,:]), self.b1[i,:])), \n",
    "                           self.W2[i,:,:]), self.b2[i,:,])\n",
    "            # Dropout.\n",
    "            ffn_op = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(ffn_op, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(ffn_op))\n",
    "            # Add & LN\n",
    "            x = x+ffn_op\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i+1] + self.beta[2*i+1]            \n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def build_strats(D, max_len, V, d, N, he, dropout, forecast=False):\n",
    "    demo = Input(shape=(D,))\n",
    "    demo_enc = Dense(2*d, activation='tanh')(demo)\n",
    "    demo_enc = Dense(d, activation='tanh')(demo_enc)\n",
    "    varis = Input(shape=(max_len,))\n",
    "    values = Input(shape=(max_len,))\n",
    "    times = Input(shape=(max_len,))\n",
    "    varis_emb = Embedding(V+1, d)(varis)\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    values_emb = CVE(cve_units, d)(values)\n",
    "    times_emb = CVE(cve_units, d)(times)\n",
    "    comb_emb = Add()([varis_emb, values_emb, times_emb]) # b, L, d\n",
    "#     demo_enc = Lambda(lambda x:K.expand_dims(x, axis=-2))(demo_enc) # b, 1, d\n",
    "#     comb_emb = Concatenate(axis=-2)([demo_enc, comb_emb]) # b, L+1, d\n",
    "    mask = Lambda(lambda x:K.clip(x,0,1))(varis) # b, L\n",
    "#     mask = Lambda(lambda x:K.concatenate((K.ones_like(x)[:,0:1], x), axis=-1))(mask) # b, L+1\n",
    "    cont_emb = Transformer(N, he, dk=None, dv=None, dff=None, dropout=dropout)(comb_emb, mask=mask)\n",
    "    attn_weights = Attention(2*d)(cont_emb, mask=mask)\n",
    "    fused_emb = Lambda(lambda x:K.sum(x[0]*x[1], axis=-2))([cont_emb, attn_weights])\n",
    "    # embed text input\n",
    "    texts = Input(shape=(33792,))\n",
    "    # text_enc = Dense(22528, activation='relu')(texts)\n",
    "    # text_enc = Dense(15000, activation='relu')(texts)\n",
    "    # text_enc = Dense(10000, activation='relu')(texts)\n",
    "    # text_enc = Dense(6000, activation='relu')(texts)\n",
    "    # text_enc = Dense(4000, activation='relu')(texts)\n",
    "    # text_enc = Dense(2600, activation='relu')(texts)\n",
    "    # text_enc = Dense(1000, activation='relu')(texts)\n",
    "    # text_enc = Dense(666, activation='relu')(texts)\n",
    "    # text_enc = Dense(444, activation='relu')(texts)\n",
    "    # text_enc = Dense(200, activation='relu')(texts)\n",
    "    # text_enc = Dense(140, activation='relu')(texts)\n",
    "    d1 = Dropout(0.1)(texts)\n",
    "    # te_1 = Dense(22528, activation='relu')(d1)\n",
    "    te_1 = Dense(10000, activation='relu')(d1)\n",
    "    d2 = Dropout(0.1)(te_1)\n",
    "    te_2 = Dense(1000, activation='relu')(d2)\n",
    "    d3 = Dropout(0.1)(te_2)\n",
    "    # te_3 = Dense(6000, activation='relu')(d3)\n",
    "    # d4 = Dropout(0.1)(te_3)\n",
    "    # te_4 = Dense(1000, activation='relu')(d4)\n",
    "    # d5 = Dropout(0.1)(te_4)\n",
    "    # text_enc = Dense(d, activation='relu')(d5)\n",
    "    text_enc = Dense(d, activation='relu')(d3)\n",
    "    conc = Concatenate(axis=-1)([fused_emb, text_enc, demo_enc])\n",
    "    fore_op = Dense(V)(conc)\n",
    "    op = Dense(1, activation='sigmoid')(fore_op)\n",
    "    model = Model([demo, times, values, varis, texts], op)\n",
    "    if forecast:\n",
    "        fore_model = Model([demo, times, values, varis, texts], fore_op)\n",
    "        return [model, fore_model]\n",
    "    return model\n",
    "\n",
    "# To tune:\n",
    "# 1. Transformer parameters. (N, h, dropout)\n",
    "# 2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "# lds = [50]\n",
    "# # batch_size, lr, patience = 32, 0.0005, 10\n",
    "# # batch_size, lr, patience = 8, 0.0005, 10\n",
    "# batch_size, lr, patience = 8, 0.0005, 10\n",
    "\n",
    "# d, N, he, dropout = 50,2,4,0.2\n",
    "# fore_savepath = 'Exp1/L/models/forecasting/forecasting_38_epochs.h5'\n",
    "# f = open('Exp1/L/logs/log_text_50.csv', 'a+')\n",
    "# f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "# train_inds = np.arange(len(train_op))\n",
    "# valid_inds = np.arange(len(valid_op))\n",
    "# gen_res = {}\n",
    "\n",
    "# np.random.seed(2023)\n",
    "# for ld in lds:\n",
    "#     np.random.shuffle(train_inds)\n",
    "#     np.random.shuffle(valid_inds)\n",
    "#     train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "#     valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "#     f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "#     all_test_res = []\n",
    "#     for i in range(repeats[ld]):\n",
    "#         gc.collect()\n",
    "#         print ('Repeat', i, 'ld', ld)\n",
    "#         # Get train and validation data.\n",
    "#         curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "#         curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "#         curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "#         curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "#         curr_train_op = train_op[curr_train_ind]\n",
    "#         curr_valid_op = valid_op[curr_valid_ind]\n",
    "#         print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "#         # Construct save_path.\n",
    "#         savepath = 'Exp1/L/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "#         print (savepath)\n",
    "#         # Build and compile model.\n",
    "#         model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "#         model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "#         fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "#         # Load pretrained weights here.\n",
    "#         fore_model.load_weights(fore_savepath)\n",
    "#         # Train model.\n",
    "#         es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "#                            restore_best_weights=True)\n",
    "#         cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "#         his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "#                         verbose=1, callbacks=[cus, es]).history\n",
    "#         model.save_weights(savepath)\n",
    "#         # Test and write to log.\n",
    "#         rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "#         f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "#         print ('Test res', rocauc, prauc, minrp)\n",
    "#         all_test_res.append([rocauc, prauc, minrp])\n",
    "#         gc.collect()\n",
    "        \n",
    "#     gen_res[ld] = []\n",
    "#     for i in range(len(all_test_res[0])):\n",
    "#         nums = [test_res[i] for test_res in all_test_res]\n",
    "#         gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "#     print ('gen_res', gen_res)\n",
    "# f.close()\n",
    "\n",
    "# # # save to local\n",
    "# # log_path = '/content/log.csv'\n",
    "# # files.download(log_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4617val_aucs: 0.48805429598100436 0.8877003342947616\n",
    "2285/2285 [==============================] - 98s 38ms/step - loss: 0.4617 - custom_metric: 1.3758\n",
    "Epoch 2/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4176val_aucs: 0.49846231951041253 0.8857353051432\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.4176 - custom_metric: 1.3842\n",
    "Epoch 3/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3922val_aucs: 0.5202421569078398 0.8974573972638988\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3922 - custom_metric: 1.4177\n",
    "Epoch 4/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3770val_aucs: 0.5446991224055654 0.8879528194543674\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3770 - custom_metric: 1.4327\n",
    "Epoch 5/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3621val_aucs: 0.5337517764018193 0.887486989847671\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3620 - custom_metric: 1.4212\n",
    "Epoch 6/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3519val_aucs: 0.5025821550225524 0.8932814691328623\n",
    "2285/2285 [==============================] - 78s 34ms/step - loss: 0.3519 - custom_metric: 1.3959\n",
    "Epoch 7/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3403val_aucs: 0.520095549444726 0.8837030201194288\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.3403 - custom_metric: 1.4038\n",
    "Epoch 8/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3244val_aucs: 0.5121136829544592 0.8886700316653876\n",
    "2285/2285 [==============================] - 78s 34ms/step - loss: 0.3244 - custom_metric: 1.4008\n",
    "Epoch 9/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3093val_aucs: 0.5289483457836224 0.8841385294558669\n",
    "2285/2285 [==============================] - 78s 34ms/step - loss: 0.3093 - custom_metric: 1.4131\n",
    "Epoch 10/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3013val_aucs: 0.5277484383736888 0.8879054095772362\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.3013 - custom_metric: 1.4157\n",
    "Epoch 11/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.2945val_aucs: 0.5308797434737741 0.8868464052287581\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.2944 - custom_metric: 1.4177\n",
    "Epoch 12/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2822val_aucs: 0.5139453721865805 0.8769482151835092\n",
    "2285/2285 [==============================] - 78s 34ms/step - loss: 0.2822 - custom_metric: 1.3909\n",
    "Epoch 13/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2724val_aucs: 0.5071791138653791 0.8803330598996234\n",
    "2285/2285 [==============================] - 78s 34ms/step - loss: 0.2724 - custom_metric: 1.3875\n",
    "Epoch 14/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2657val_aucs: 0.46167194545178486 0.875497252432237\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2657 - custom_metric: 1.3372\n",
    "Test res 0.8903537222504786 0.49704939055855013 0.484375\n",
    "Repeat 1 ld 50\n",
    "Num train: 18275 Num valid: 4631\n",
    "Exp1/L/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_50ld.h5\n",
    "Epoch 1/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4624val_aucs: 0.5018677441446692 0.8926649243857367\n",
    "2285/2285 [==============================] - 92s 37ms/step - loss: 0.4624 - custom_metric: 1.3945\n",
    "Epoch 2/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4154val_aucs: 0.5238269218056254 0.9014384752490405\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.4154 - custom_metric: 1.4253\n",
    "Epoch 3/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3921val_aucs: 0.5109790048002604 0.8961006440607538\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3921 - custom_metric: 1.4071\n",
    "Epoch 4/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3746val_aucs: 0.5341119582434026 0.897443986397087\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3746 - custom_metric: 1.4316\n",
    "Epoch 5/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3651val_aucs: 0.5212844180703478 0.8932346283802255\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3651 - custom_metric: 1.4145\n",
    "Epoch 6/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3486val_aucs: 0.5480138225363744 0.9002635291030168\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3486 - custom_metric: 1.4483\n",
    "Epoch 7/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3357val_aucs: 0.5235895863451487 0.891229729581962\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3357 - custom_metric: 1.4148\n",
    "Epoch 8/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3195val_aucs: 0.5161547177816763 0.8924560693704826\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3195 - custom_metric: 1.4086\n",
    "Epoch 9/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3123val_aucs: 0.5050172409890503 0.8803659883434845\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3123 - custom_metric: 1.3854\n",
    "Epoch 10/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2987val_aucs: 0.546673344583116 0.8923243048189741\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2987 - custom_metric: 1.4390\n",
    "Epoch 11/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2901val_aucs: 0.4605295812136528 0.8795682934030246\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2901 - custom_metric: 1.3401\n",
    "Epoch 12/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2928val_aucs: 0.5018996546560233 0.8868847798274485\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2928 - custom_metric: 1.3888\n",
    "Epoch 13/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2715val_aucs: 0.5002034725896967 0.8844102306153022\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2715 - custom_metric: 1.3846\n",
    "Epoch 14/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2603val_aucs: 0.4645648655669564 0.8719521929776602\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2603 - custom_metric: 1.3365\n",
    "Epoch 15/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2552val_aucs: 0.4986203284634045 0.8842626105783424\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2552 - custom_metric: 1.3829\n",
    "Epoch 16/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2385val_aucs: 0.4749074990870884 0.8750836513542771\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.2385 - custom_metric: 1.3500\n",
    "Test res 0.8921624899024654 0.49625805557582153 0.4971042471042471\n",
    "Repeat 2 ld 50\n",
    "Num train: 18275 Num valid: 4631\n",
    "Exp1/L/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_50ld.h5\n",
    "Epoch 1/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4619val_aucs: 0.49792139185215445 0.8984421515903878\n",
    "2285/2285 [==============================] - 92s 37ms/step - loss: 0.4619 - custom_metric: 1.3964\n",
    "Epoch 2/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4048val_aucs: 0.5276269292038885 0.8999120208678831\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.4048 - custom_metric: 1.4275\n",
    "Epoch 3/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3866val_aucs: 0.5600381664281977 0.9030578606585227\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3866 - custom_metric: 1.4631\n",
    "Epoch 4/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3717val_aucs: 0.5413647740550657 0.9048334201408306\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.3717 - custom_metric: 1.4462\n",
    "Epoch 5/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3591val_aucs: 0.528176730280736 0.9005265966455027\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.3591 - custom_metric: 1.4287\n",
    "Epoch 6/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3441val_aucs: 0.5151765826352669 0.8955775041832693\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.3441 - custom_metric: 1.4108\n",
    "Epoch 7/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3331val_aucs: 0.5287319387511423 0.8912696155652765\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3331 - custom_metric: 1.4200\n",
    "Epoch 8/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3206val_aucs: 0.5297210618902206 0.8959188759973543\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.3206 - custom_metric: 1.4256\n",
    "Epoch 9/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3108val_aucs: 0.5282553475332804 0.8919012333055337\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.3108 - custom_metric: 1.4202\n",
    "Epoch 10/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2948val_aucs: 0.501378265847895 0.8880220565601438\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.2948 - custom_metric: 1.3894\n",
    "Epoch 11/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2925val_aucs: 0.5145906330157576 0.8884465079420871\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.2925 - custom_metric: 1.4030\n",
    "Epoch 12/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2889val_aucs: 0.50895518494678 0.8850492992025428\n",
    "2285/2285 [==============================] - 78s 34ms/step - loss: 0.2889 - custom_metric: 1.3940\n",
    "Epoch 13/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2689val_aucs: 0.49735287371481324 0.8854471225178647\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2689 - custom_metric: 1.3828\n",
    "Test res 0.8967894514420776 0.5108939201394137 0.5205078125\n",
    "Repeat 3 ld 50\n",
    "Num train: 18275 Num valid: 4631\n",
    "Exp1/L/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_50ld.h5\n",
    "Epoch 1/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4574val_aucs: 0.49824676940758267 0.8943132682126966\n",
    "2285/2285 [==============================] - 97s 40ms/step - loss: 0.4574 - custom_metric: 1.3926\n",
    "Epoch 2/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4034val_aucs: 0.49589909330363824 0.8956354122896748\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.4034 - custom_metric: 1.3915\n",
    "Epoch 3/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3844val_aucs: 0.4964029332544414 0.891496150767632\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3844 - custom_metric: 1.3879\n",
    "Epoch 4/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3688val_aucs: 0.5004149348803951 0.8894594655902281\n",
    "2285/2285 [==============================] - 82s 36ms/step - loss: 0.3688 - custom_metric: 1.3899\n",
    "Epoch 5/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3593val_aucs: 0.48741631770497196 0.8939354741841647\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3592 - custom_metric: 1.3814\n",
    "Epoch 6/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3403val_aucs: 0.4755503203288901 0.8803637840541216\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3403 - custom_metric: 1.3559\n",
    "Epoch 7/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3343val_aucs: 0.46532247290830214 0.8826721352734862\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.3343 - custom_metric: 1.3480\n",
    "Epoch 8/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3204val_aucs: 0.48545877480102456 0.8823777853870673\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.3204 - custom_metric: 1.3678\n",
    "Epoch 9/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3138val_aucs: 0.4824359488820257 0.8822584251578958\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3138 - custom_metric: 1.3647\n",
    "Epoch 10/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3005val_aucs: 0.4662184947212431 0.8797237755854864\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.3005 - custom_metric: 1.3459\n",
    "Epoch 11/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2854val_aucs: 0.46920195359959377 0.8767467910461464\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2854 - custom_metric: 1.3459\n",
    "Test res 0.8897426811572522 0.4973011641970907 0.4936831875607386\n",
    "Repeat 4 ld 50\n",
    "Num train: 18275 Num valid: 4631\n",
    "Exp1/L/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_50ld.h5\n",
    "Epoch 1/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4616val_aucs: 0.4452638728303907 0.8795025687551336\n",
    "2285/2285 [==============================] - 92s 37ms/step - loss: 0.4616 - custom_metric: 1.3248\n",
    "Epoch 2/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.4060val_aucs: 0.5264893261200781 0.896391395609764\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.4059 - custom_metric: 1.4229\n",
    "Epoch 3/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3863val_aucs: 0.509699630619876 0.8970790811533361\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3863 - custom_metric: 1.4068\n",
    "Epoch 4/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3682val_aucs: 0.4876439878695279 0.889717463777063\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3682 - custom_metric: 1.3774\n",
    "Epoch 5/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3546val_aucs: 0.5030222104730885 0.8908394487778952\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3546 - custom_metric: 1.3939\n",
    "Epoch 6/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3435val_aucs: 0.5032178828709575 0.8926759610687309\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3435 - custom_metric: 1.3959\n",
    "Epoch 7/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3225val_aucs: 0.5052055912792182 0.8802576270822484\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3229 - custom_metric: 1.3855\n",
    "Epoch 8/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3206val_aucs: 0.5046321806489614 0.8889739474009137\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3205 - custom_metric: 1.3936\n",
    "Epoch 9/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2991val_aucs: 0.4914960524577954 0.8845407645603054\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2991 - custom_metric: 1.3760\n",
    "Epoch 10/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2975val_aucs: 0.48982280701898057 0.8822264691829909\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2975 - custom_metric: 1.3720\n",
    "Epoch 11/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2887val_aucs: 0.47099952043388255 0.8817245285247239\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2887 - custom_metric: 1.3527\n",
    "Epoch 12/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2673val_aucs: 0.4728773649417932 0.8837661117582954\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2673 - custom_metric: 1.3566\n",
    "Test res 0.8928098073240786 0.5130278532751873 0.49902534113060426\n",
    "Repeat 5 ld 50\n",
    "Num train: 18275 Num valid: 4631\n",
    "Exp1/L/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_50ld.h5\n",
    "Epoch 1/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4659val_aucs: 0.48349739173293427 0.8914176199960274\n",
    "2285/2285 [==============================] - 92s 37ms/step - loss: 0.4659 - custom_metric: 1.3749\n",
    "Epoch 2/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.4123val_aucs: 0.48626845745748654 0.8927403812601664\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.4123 - custom_metric: 1.3790\n",
    "Epoch 3/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3956val_aucs: 0.5093442567398264 0.8941581623066723\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3956 - custom_metric: 1.4035\n",
    "Epoch 4/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3781val_aucs: 0.49582879515774547 0.8960778840114453\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3781 - custom_metric: 1.3919\n",
    "Epoch 5/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3593val_aucs: 0.5111995247656145 0.8971773219452751\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3593 - custom_metric: 1.4084\n",
    "Epoch 6/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3512val_aucs: 0.5260061324296946 0.8912571063523678\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3512 - custom_metric: 1.4173\n",
    "Epoch 7/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3334val_aucs: 0.4795984211036673 0.8882057366180472\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3334 - custom_metric: 1.3678\n",
    "Epoch 8/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3225val_aucs: 0.47995450385811816 0.8816778238859333\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3225 - custom_metric: 1.3616\n",
    "Epoch 9/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3135val_aucs: 0.5067585022276003 0.8869071329256967\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3135 - custom_metric: 1.3937\n",
    "Epoch 10/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2950val_aucs: 0.5021485162319216 0.8848784337304122\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2950 - custom_metric: 1.3870\n",
    "Epoch 11/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2866val_aucs: 0.4942007645443831 0.8881729896874011\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2866 - custom_metric: 1.3824\n",
    "Epoch 12/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2784val_aucs: 0.48552494955092174 0.8832491397220268\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2784 - custom_metric: 1.3688\n",
    "Epoch 13/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2646val_aucs: 0.5029002676006694 0.8816488347997874\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2646 - custom_metric: 1.3845\n",
    "Epoch 14/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2572val_aucs: 0.4715785499735098 0.8765440714634657\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2572 - custom_metric: 1.3481\n",
    "Epoch 15/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.2568val_aucs: 0.49573091369600514 0.8794601587957719\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2568 - custom_metric: 1.3752\n",
    "Epoch 16/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2453val_aucs: 0.46674401268455723 0.8735657112794388\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.2453 - custom_metric: 1.3403\n",
    "Test res 0.8848177021002871 0.4971982109824315 0.4941747572815534\n",
    "Repeat 6 ld 50\n",
    "Num train: 18275 Num valid: 4631\n",
    "Exp1/L/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_50ld.h5\n",
    "Epoch 1/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4618val_aucs: 0.5035796395741613 0.8882016571361886\n",
    "2285/2285 [==============================] - 93s 38ms/step - loss: 0.4618 - custom_metric: 1.3918\n",
    "Epoch 2/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4093val_aucs: 0.5277457870042783 0.9002604525827773\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.4093 - custom_metric: 1.4280\n",
    "Epoch 3/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3909val_aucs: 0.5014972562562345 0.8979970082432446\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3909 - custom_metric: 1.3995\n",
    "Epoch 4/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3767val_aucs: 0.5000760138082918 0.8945713406677347\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3767 - custom_metric: 1.3946\n",
    "Epoch 5/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3623val_aucs: 0.5013133672175397 0.8899261608971027\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3623 - custom_metric: 1.3912\n",
    "Epoch 6/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3481val_aucs: 0.4992138570901654 0.890877528936229\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3481 - custom_metric: 1.3901\n",
    "Epoch 7/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3362val_aucs: 0.5286895839936032 0.891443787861106\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3362 - custom_metric: 1.4201\n",
    "Epoch 8/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3227val_aucs: 0.44734645998334355 0.8758908963600294\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3227 - custom_metric: 1.3232\n",
    "Epoch 9/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3184val_aucs: 0.5031308566433204 0.8923214228880001\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3184 - custom_metric: 1.3955\n",
    "Epoch 10/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2932val_aucs: 0.46305753404584016 0.8835933438716728\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2932 - custom_metric: 1.3467\n",
    "Epoch 11/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.2873val_aucs: 0.4833826876761327 0.8856156971748056\n",
    "2285/2285 [==============================] - 81s 35ms/step - loss: 0.2873 - custom_metric: 1.3690\n",
    "Epoch 12/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.2853val_aucs: 0.49624185389862346 0.8879687881263327\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.2854 - custom_metric: 1.3842\n",
    "Test res 0.8903983196954284 0.4942677096740008 0.501953125\n",
    "Repeat 7 ld 50\n",
    "Num train: 18275 Num valid: 4631\n",
    "Exp1/L/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_50ld.h5\n",
    "Epoch 1/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.4596val_aucs: 0.49156188817069973 0.890541519226982\n",
    "2285/2285 [==============================] - 94s 38ms/step - loss: 0.4599 - custom_metric: 1.3821\n",
    "Epoch 2/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.4127val_aucs: 0.5272333335380011 0.8932483741929199\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.4127 - custom_metric: 1.4205\n",
    "Epoch 3/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3950val_aucs: 0.5245548288274324 0.8920339158921451\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3950 - custom_metric: 1.4166\n",
    "Epoch 4/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3835val_aucs: 0.5308062305836336 0.8983100288632375\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3835 - custom_metric: 1.4291\n",
    "Epoch 5/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3654val_aucs: 0.4996272097192288 0.8929852592079657\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3654 - custom_metric: 1.3926\n",
    "Epoch 6/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3516val_aucs: 0.5010452525665895 0.8927099678756192\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3516 - custom_metric: 1.3938\n",
    "Epoch 7/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3451val_aucs: 0.5268903029689758 0.8959615820357583\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3451 - custom_metric: 1.4229\n",
    "Epoch 8/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3291val_aucs: 0.49782721998960966 0.8910354554059806\n",
    "2285/2285 [==============================] - 83s 36ms/step - loss: 0.3291 - custom_metric: 1.3889\n",
    "Epoch 9/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3179val_aucs: 0.49041669513711456 0.8848901058177531\n",
    "2285/2285 [==============================] - 79s 34ms/step - loss: 0.3179 - custom_metric: 1.3753\n",
    "Epoch 10/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3012val_aucs: 0.4869563366134734 0.8887320081232118\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3011 - custom_metric: 1.3757\n",
    "Epoch 11/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2968val_aucs: 0.4928935703514728 0.8831658291457287\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2968 - custom_metric: 1.3761\n",
    "Epoch 12/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2915val_aucs: 0.5350545623545212 0.8910759549962199\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2915 - custom_metric: 1.4261\n",
    "Epoch 13/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2854val_aucs: 0.5311888246435958 0.8845264036152104\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2854 - custom_metric: 1.4157\n",
    "Epoch 14/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2744val_aucs: 0.49951079890295197 0.8796119239264167\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.2744 - custom_metric: 1.3791\n",
    "Test res 0.8869707208891813 0.4967470780823 0.48841698841698844\n",
    "Repeat 8 ld 50\n",
    "Num train: 18275 Num valid: 4631\n",
    "Exp1/L/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_50ld.h5\n",
    "Epoch 1/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.4542val_aucs: 0.5120573593779207 0.8945283882783883\n",
    "2285/2285 [==============================] - 92s 37ms/step - loss: 0.4541 - custom_metric: 1.4066\n",
    "Epoch 2/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.4156val_aucs: 0.5295731863813072 0.8997363268914993\n",
    "2285/2285 [==============================] - 81s 35ms/step - loss: 0.4155 - custom_metric: 1.4293\n",
    "Epoch 3/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3960val_aucs: 0.5377261425355647 0.9028451433623849\n",
    "2285/2285 [==============================] - 81s 35ms/step - loss: 0.3960 - custom_metric: 1.4406\n",
    "Epoch 4/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3874val_aucs: 0.5008823420857518 0.9015315144625489\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3874 - custom_metric: 1.4024\n",
    "Epoch 5/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3684val_aucs: 0.514819608971613 0.8977648309544861\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3684 - custom_metric: 1.4126\n",
    "Epoch 6/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3531val_aucs: 0.5189566298324075 0.8986805818702369\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3531 - custom_metric: 1.4176\n",
    "Epoch 7/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3437val_aucs: 0.523276313146893 0.8971516988758369\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3437 - custom_metric: 1.4204\n",
    "Epoch 8/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3293val_aucs: 0.47686548518657257 0.8892793987621573\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.3293 - custom_metric: 1.3661\n",
    "Epoch 9/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3166val_aucs: 0.49730919088448644 0.8934555387141595\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3166 - custom_metric: 1.3908\n",
    "Epoch 10/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3053val_aucs: 0.49908125782020235 0.8830443981306051\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3053 - custom_metric: 1.3821\n",
    "Epoch 11/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.2949val_aucs: 0.48755291711107446 0.8856953391436149\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2949 - custom_metric: 1.3732\n",
    "Epoch 12/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2806val_aucs: 0.48599218837941077 0.8847648520062313\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2806 - custom_metric: 1.3708\n",
    "Epoch 13/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2651val_aucs: 0.49605178587567517 0.8852885141678244\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.2651 - custom_metric: 1.3813\n",
    "Test res 0.884646417993059 0.4897113364441643 0.498046875\n",
    "Repeat 9 ld 50\n",
    "Num train: 18275 Num valid: 4631\n",
    "Exp1/L/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_50ld.h5\n",
    "Epoch 1/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4680val_aucs: 0.5297158650832017 0.9038566555682795\n",
    "2285/2285 [==============================] - 92s 37ms/step - loss: 0.4680 - custom_metric: 1.4336\n",
    "Epoch 2/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4199val_aucs: 0.5432120344239227 0.9002179264083201\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.4199 - custom_metric: 1.4434\n",
    "Epoch 3/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.4001val_aucs: 0.5464961500283543 0.9029830141698012\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.4001 - custom_metric: 1.4495\n",
    "Epoch 4/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3830val_aucs: 0.5290025996987824 0.8975898704463671\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3830 - custom_metric: 1.4266\n",
    "Epoch 5/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3705val_aucs: 0.5275187629914426 0.8967895843606473\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3705 - custom_metric: 1.4243\n",
    "Epoch 6/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3572val_aucs: 0.5134633075699221 0.8985832236399721\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3572 - custom_metric: 1.4120\n",
    "Epoch 7/1000\n",
    "2284/2285 [============================>.] - ETA: 0s - loss: 0.3473val_aucs: 0.5311058848062775 0.8997462109943313\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3472 - custom_metric: 1.4309\n",
    "Epoch 8/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3332val_aucs: 0.5350196719191828 0.9009524983596937\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3332 - custom_metric: 1.4360\n",
    "Epoch 9/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3222val_aucs: 0.5168609258493428 0.8985052836201661\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3222 - custom_metric: 1.4154\n",
    "Epoch 10/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.3085val_aucs: 0.5108033882899418 0.8880852633251963\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.3085 - custom_metric: 1.3989\n",
    "Epoch 11/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2930val_aucs: 0.5010026639421374 0.8819068611669201\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2930 - custom_metric: 1.3829\n",
    "Epoch 12/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2859val_aucs: 0.4992606175017302 0.8852529331936865\n",
    "2285/2285 [==============================] - 79s 35ms/step - loss: 0.2859 - custom_metric: 1.3845\n",
    "Epoch 13/1000\n",
    "2285/2285 [==============================] - ETA: 0s - loss: 0.2743val_aucs: 0.5142238208503498 0.8826225848782098\n",
    "2285/2285 [==============================] - 80s 35ms/step - loss: 0.2743 - custom_metric: 1.3968\n",
    "Test res 0.8849356936632361 0.5007645040213328 0.4970703125\n",
    "gen_res {50: [(0.8893627006417544, 0.0038117882859382514), (0.4993219222950292, 0.0068667915456318215), (0.49743576464941314, 0.009124184932260125)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = {k:10 for k in [10,20,30,40,50,90]}\n",
    "lds = [90]\n",
    "# batch_size, lr, patience = 32, 0.0005, 10\n",
    "# batch_size, lr, patience = 8, 0.0005, 10\n",
    "batch_size, lr, patience = 8, 0.0005, 10\n",
    "\n",
    "d, N, he, dropout = 50,2,4,0.2\n",
    "fore_savepath = 'Exp1/L/models/forecasting/forecasting_38_epochs.h5'\n",
    "f = open('Exp1/L/logs/log_text_50.csv', 'a+')\n",
    "f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "train_inds = np.arange(len(train_op))\n",
    "valid_inds = np.arange(len(valid_op))\n",
    "gen_res = {}\n",
    "\n",
    "np.random.seed(2023)\n",
    "for ld in lds:\n",
    "    np.random.shuffle(train_inds)\n",
    "    np.random.shuffle(valid_inds)\n",
    "    train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "    valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "    f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "    all_test_res = []\n",
    "    for i in range(repeats[ld]):\n",
    "        gc.collect()\n",
    "        print ('Repeat', i, 'ld', ld)\n",
    "        # Get train and validation data.\n",
    "        curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "        curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "        curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "        curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "        curr_train_op = train_op[curr_train_ind]\n",
    "        curr_valid_op = valid_op[curr_valid_ind]\n",
    "        print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "        # Construct save_path.\n",
    "        savepath = 'Exp1/L/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "        print (savepath)\n",
    "        # Build and compile model.\n",
    "        model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "        model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "        fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "        # Load pretrained weights here.\n",
    "        fore_model.load_weights(fore_savepath)\n",
    "        # Train model.\n",
    "        es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "                           restore_best_weights=True)\n",
    "        cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "        his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "                        verbose=1, callbacks=[cus, es]).history\n",
    "        model.save_weights(savepath)\n",
    "        # Test and write to log.\n",
    "        rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "        f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "        print ('Test res', rocauc, prauc, minrp)\n",
    "        all_test_res.append([rocauc, prauc, minrp])\n",
    "        gc.collect()\n",
    "        \n",
    "    gen_res[ld] = []\n",
    "    for i in range(len(all_test_res[0])):\n",
    "        nums = [test_res[i] for test_res in all_test_res]\n",
    "        gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "    print ('gen_res', gen_res)\n",
    "f.close()\n",
    "\n",
    "# # save to local\n",
    "# log_path = '/content/log.csv'\n",
    "# files.download(log_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
