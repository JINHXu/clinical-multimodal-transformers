{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTDFCBgnk1o3"
   },
   "source": [
    "# Strats+Text Target Task\n",
    "\n",
    "- binary classification\n",
    "\n",
    "- physiological features + clinical text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECKAJiLXiTlN"
   },
   "source": [
    "## Hardware check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e7odgm5VsOsb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul  9 17:21:52 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    38W / 300W |      9MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1893      G   /usr/libexec/Xorg                   8MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# gpu check\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oEEUeIW3sOsc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of cores\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() \n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY9I5eXWeOom"
   },
   "source": [
    "## Environment Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis\n"
     ]
    }
   ],
   "source": [
    "cd /pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 17:22:04.236450: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-09 17:22:07.338368: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Lambda\n",
    "# from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, pipeline, AutoModel\n",
    "import resources.smart_cond as sc\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfKWDkwoeSGU"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_path = 'Data/sepsis_removed_0.pkl'\n",
    "# pkl = pickle.load(open(data_path, 'rb'))\n",
    "# data = pkl[0]\n",
    "# oc = pkl[1]\n",
    "# train_ind = pkl[2]\n",
    "# valid_ind = pkl[3]\n",
    "# test_ind = pkl[4]\n",
    "# del pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Filter labeled data in first 24h.\n",
    "# data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# data = data.loc[(data.hour>=0)&(data.hour<=24)]\n",
    "# oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# # Get y and N.\n",
    "# y = np.array(oc.sort_values(by='ts_ind')['in_hospital_sepsis']).astype('float32')\n",
    "# N = data.ts_ind.max() + 1\n",
    "# # Get static data with mean fill and missingness indicator.\n",
    "# static_varis = ['Age', 'Gender']\n",
    "# ii = data.variable.isin(static_varis)\n",
    "# static_data = data.loc[ii]\n",
    "# data = data.loc[~ii]\n",
    "# def inv_list(l, start=0):\n",
    "#     d = {}\n",
    "#     for i in range(len(l)):\n",
    "#         d[l[i]] = i+start\n",
    "#     return d\n",
    "# static_var_to_ind = inv_list(static_varis)\n",
    "# D = len(static_varis)\n",
    "# demo = np.zeros((N, D))\n",
    "# for row in tqdm(static_data.itertuples()):\n",
    "#     demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# # Normalize static data.\n",
    "# means = demo.mean(axis=0, keepdims=True)\n",
    "# stds = demo.std(axis=0, keepdims=True)\n",
    "# stds = (stds==0)*1 + (stds!=0)*stds\n",
    "# demo = (demo-means)/stds\n",
    "# # Trim to max len.\n",
    "# data = data.sample(frac=1)\n",
    "# data = data.groupby('ts_ind').head(880)\n",
    "# # Get N, V, var_to_ind.\n",
    "# N = data.ts_ind.max() + 1\n",
    "# varis = sorted(list(set(data.variable)))\n",
    "# V = len(varis)\n",
    "# def inv_list(l, start=0):\n",
    "#     d = {}\n",
    "#     for i in range(len(l)):\n",
    "#         d[l[i]] = i+start\n",
    "#     return d\n",
    "# var_to_ind = inv_list(varis, start=1)\n",
    "# data['vind'] = data.variable.map(var_to_ind)\n",
    "# data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# # Add obs index.\n",
    "# data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "# data = data.reset_index().rename(columns={'index':'obs_ind'})\n",
    "# data = data.merge(data.groupby('ts_ind').agg({'obs_ind':'min'}).reset_index().rename(columns={ \\\n",
    "#                                                             'obs_ind':'first_obs_ind'}), on='ts_ind')\n",
    "# data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# # Find max_len.\n",
    "# max_len = data.obs_ind.max()+1\n",
    "# print ('max_len', max_len)\n",
    "# # Generate times_ip and values_ip matrices.\n",
    "# # times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "# # values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "# # varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "# texts_inp = np.empty([N, max_len], dtype=object)\n",
    "# for row in tqdm(data.itertuples()):\n",
    "#     ts_ind = row.ts_ind\n",
    "#     l = row.obs_ind\n",
    "#     # times_inp[ts_ind, l] = row.hour\n",
    "#     if isinstance(row.value, str):\n",
    "#         # values_inp[ts_ind, l] = 1.0\n",
    "#         texts_inp[ts_ind, l] = row.value\n",
    "#     else:\n",
    "#         # values_inp[ts_ind, l] = row.value\n",
    "#         texts_inp[ts_ind, l] = ''\n",
    "#     # varis_inp[ts_ind, l] = row.vind\n",
    "    \n",
    "# data.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\n",
    "# # Generate 3 sets of inputs and outputs.\n",
    "# # train_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp, texts_inp]]\n",
    "# # valid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp, texts_inp]]\n",
    "# # test_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp, texts_inp]]\n",
    "# # del times_inp, values_inp, varis_inp\n",
    "# train_ip = [ip[train_ind] for ip in [texts_inp]]\n",
    "# valid_ip = [ip[valid_ind] for ip in [texts_inp]]\n",
    "# test_ip = [ip[test_ind] for ip in [texts_inp]]\n",
    "# del texts_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_texts = train_ip[0]\n",
    "# valid_texts = valid_ip[0]\n",
    "# test_texts = test_ip[0]\n",
    "# del train_ip, valid_ip, test_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concat_train_texts = []\n",
    "# for train_text in tqdm(train_texts):\n",
    "#     train_text = train_text[train_text != None]\n",
    "#     train_text = train_text[train_text != '']\n",
    "#     concat_text = ' '.join(list(train_text))\n",
    "#     concat_train_texts.append(concat_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concat_valid_texts = []\n",
    "# for valid_text in tqdm(valid_texts):\n",
    "#     valid_text = valid_text[valid_text != None]\n",
    "#     valid_text = valid_text[valid_text != '']\n",
    "#     concat_text = ' '.join(list(valid_text))\n",
    "#     concat_valid_texts.append(concat_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# concat_test_texts = []\n",
    "# for test_text in tqdm(test_texts):\n",
    "#     test_text = test_text[test_text != None]\n",
    "#     test_text = test_text[test_text != '']\n",
    "#     concat_text = ' '.join(list(test_text))\n",
    "#     concat_test_texts.append(concat_text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del train_texts, valid_texts, test_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from simpletransformers.language_representation import RepresentationModel\n",
    "# from simpletransformers.config.model_args import ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_args = ModelArgs(max_seq_length=512, silent = False)\n",
    "# model = RepresentationModel(\n",
    "#     \"bert\", \"emilyalsentzer/Bio_ClinicalBERT\", args=model_args)\n",
    "# train_text_features = model.encode_sentences(concat_train_texts, combine_strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_args = ModelArgs(max_seq_length=512, silent = False)\n",
    "# model = RepresentationModel(\n",
    "#     \"bert\", \"emilyalsentzer/Bio_ClinicalBERT\", args=model_args)\n",
    "# valid_text_features = model.encode_sentences(concat_valid_texts, combine_strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_args = ModelArgs(max_seq_length=512, silent = False)\n",
    "# model = RepresentationModel(\n",
    "#     \"bert\", \"emilyalsentzer/Bio_ClinicalBERT\", args=model_args)\n",
    "# test_text_features = model.encode_sentences(concat_test_texts, combine_strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # dump to json just in case\n",
    "# pickle.dump([train_text_features, valid_text_features, test_text_features], open('text_features_target.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del concat_train_texts, concat_valid_texts, concat_test_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_path = 'Data/text_features_target.pkl'\n",
    "# train_text, valid_text, test_text = pickle.load(open(data_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Data/classification_embs.pkl'\n",
    "train_text, valid_text, test_text = pickle.load(open(data_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from keras.utils import pad_sequences\n",
    "# train_text_features = pad_sequences(train_text, maxlen=33792, padding='post')\n",
    "# train_text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_text_features = pad_sequences(valid_text, maxlen=33792, padding='post')\n",
    "# valid_text_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_text_features = pad_sequences(test_text, maxlen=33792, padding='post')\n",
    "# test_text_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = 'Data/sepsis_removed_0.pkl'\n",
    "pkl = pickle.load(open(data_path, 'rb'))\n",
    "data = pkl[0]\n",
    "oc = pkl[1]\n",
    "train_ind = pkl[2]\n",
    "valid_ind = pkl[3]\n",
    "test_ind = pkl[4]\n",
    "del pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_ind</th>\n",
       "      <th>hour</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>TABLE</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10223</td>\n",
       "      <td>467.816667</td>\n",
       "      <td>Text</td>\n",
       "      <td>1</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18407</td>\n",
       "      <td>28.016667</td>\n",
       "      <td>Text</td>\n",
       "      <td>1</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40300</td>\n",
       "      <td>155.166667</td>\n",
       "      <td>Text</td>\n",
       "      <td>1</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23747</td>\n",
       "      <td>52.383333</td>\n",
       "      <td>Text</td>\n",
       "      <td>1</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2357</td>\n",
       "      <td>73.133333</td>\n",
       "      <td>Text</td>\n",
       "      <td>1</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886223</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>MBP</td>\n",
       "      <td>0.195381</td>\n",
       "      <td>chart</td>\n",
       "      <td>78.552377</td>\n",
       "      <td>17.645628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886224</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>O2 Saturation</td>\n",
       "      <td>-0.678068</td>\n",
       "      <td>chart</td>\n",
       "      <td>96.820961</td>\n",
       "      <td>4.160290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886225</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>RR</td>\n",
       "      <td>0.179866</td>\n",
       "      <td>chart</td>\n",
       "      <td>26.278501</td>\n",
       "      <td>15.130729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886226</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>SBP</td>\n",
       "      <td>-0.404061</td>\n",
       "      <td>chart</td>\n",
       "      <td>120.239648</td>\n",
       "      <td>25.341836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886227</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>Urine</td>\n",
       "      <td>-0.24296</td>\n",
       "      <td>output</td>\n",
       "      <td>123.393012</td>\n",
       "      <td>137.442433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82447326 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ts_ind        hour       variable     value       TABLE        mean  \\\n",
       "0          10223  467.816667           Text         1  noteevents    1.000000   \n",
       "1          18407   28.016667           Text         1  noteevents    1.000000   \n",
       "2          40300  155.166667           Text         1  noteevents    1.000000   \n",
       "3          23747   52.383333           Text         1  noteevents    1.000000   \n",
       "4           2357   73.133333           Text         1  noteevents    1.000000   \n",
       "...          ...         ...            ...       ...         ...         ...   \n",
       "82886223   57281   20.400000            MBP  0.195381       chart   78.552377   \n",
       "82886224   57281   20.400000  O2 Saturation -0.678068       chart   96.820961   \n",
       "82886225   57281   20.400000             RR  0.179866       chart   26.278501   \n",
       "82886226   57281   20.400000            SBP -0.404061       chart  120.239648   \n",
       "82886227   57281   20.400000          Urine  -0.24296      output  123.393012   \n",
       "\n",
       "                 std  \n",
       "0           1.000000  \n",
       "1           1.000000  \n",
       "2           1.000000  \n",
       "3           1.000000  \n",
       "4           1.000000  \n",
       "...              ...  \n",
       "82886223   17.645628  \n",
       "82886224    4.160290  \n",
       "82886225   15.130729  \n",
       "82886226   25.341836  \n",
       "82886227  137.442433  \n",
       "\n",
       "[82447326 rows x 7 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data['variable'] == 'Text', 'value'] = 1\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "114564it [00:00, 763543.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len 880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19267073it [00:27, 713306.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Filter labeled data in first 24h.\n",
    "data = data.loc[data.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "data = data.loc[(data.hour>=0)&(data.hour<=24)]\n",
    "oc = oc.loc[oc.ts_ind.isin(np.concatenate((train_ind, valid_ind, test_ind), axis=-1))]\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "# Get y and N.\n",
    "y = np.array(oc.sort_values(by='ts_ind')['in_hospital_sepsis']).astype('float32')\n",
    "N = data.ts_ind.max() + 1\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "# Trim to max len.\n",
    "data = data.sample(frac=1)\n",
    "data = data.groupby('ts_ind').head(880)\n",
    "# Get N, V, var_to_ind.\n",
    "N = data.ts_ind.max() + 1\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Add obs index.\n",
    "data = data.sort_values(by=['ts_ind']).reset_index(drop=True)\n",
    "data = data.reset_index().rename(columns={'index':'obs_ind'})\n",
    "data = data.merge(data.groupby('ts_ind').agg({'obs_ind':'min'}).reset_index().rename(columns={ \\\n",
    "                                                            'obs_ind':'first_obs_ind'}), on='ts_ind')\n",
    "data['obs_ind'] = data['obs_ind'] - data['first_obs_ind']\n",
    "# Find max_len.\n",
    "max_len = data.obs_ind.max()+1\n",
    "print ('max_len', max_len)\n",
    "# Generate times_ip and values_ip matrices.\n",
    "times_inp = np.zeros((N, max_len), dtype='float32')\n",
    "values_inp = np.zeros((N, max_len), dtype='float32')\n",
    "varis_inp = np.zeros((N, max_len), dtype='int32')\n",
    "for row in tqdm(data.itertuples()):\n",
    "    ts_ind = row.ts_ind\n",
    "    l = row.obs_ind\n",
    "    times_inp[ts_ind, l] = row.hour\n",
    "    values_inp[ts_ind, l] = row.value\n",
    "    varis_inp[ts_ind, l] = row.vind\n",
    "data.drop(columns=['obs_ind', 'first_obs_ind'], inplace=True)\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ip = [ip[train_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "valid_ip = [ip[valid_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "test_ip = [ip[test_ind] for ip in [demo, times_inp, values_inp, varis_inp]]\n",
    "del times_inp, values_inp, varis_inp\n",
    "# train_op = y[train_ind]\n",
    "# valid_op = y[valid_ind]\n",
    "# test_op = y[test_ind]\n",
    "# del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ip.append(train_text)\n",
    "valid_ip.append(valid_text)\n",
    "test_ip.append(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_path = 'Data/sepsis_removed_0.pkl'\n",
    "pkl = pickle.load(open(data_path, 'rb'))\n",
    "data = pkl[0]\n",
    "oc = pkl[1]\n",
    "train_ind = pkl[2]\n",
    "valid_ind = pkl[3]\n",
    "test_ind = pkl[4]\n",
    "del pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_ind</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>in_hospital_sepsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>110404</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>188028</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>173727</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>164716</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>158689</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54862</th>\n",
       "      <td>57277</td>\n",
       "      <td>182476</td>\n",
       "      <td>83967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29333</th>\n",
       "      <td>57278</td>\n",
       "      <td>118320</td>\n",
       "      <td>23200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54573</th>\n",
       "      <td>57279</td>\n",
       "      <td>146497</td>\n",
       "      <td>73807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14317</th>\n",
       "      <td>57280</td>\n",
       "      <td>118512</td>\n",
       "      <td>10762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430</th>\n",
       "      <td>57281</td>\n",
       "      <td>122737</td>\n",
       "      <td>4094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57282 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ts_ind  HADM_ID  SUBJECT_ID  in_hospital_sepsis\n",
       "0           0   110404         268                   1\n",
       "1           1   188028         270                   0\n",
       "2           2   173727         271                   0\n",
       "3           3   164716         272                   0\n",
       "4           4   158689         273                   0\n",
       "...       ...      ...         ...                 ...\n",
       "54862   57277   182476       83967                   0\n",
       "29333   57278   118320       23200                   0\n",
       "54573   57279   146497       73807                   0\n",
       "14317   57280   118512       10762                   0\n",
       "4430    57281   122737        4094                   0\n",
       "\n",
       "[57282 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oc = oc.sort_values(by='ts_ind')\n",
    "oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ts_ind</th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>in_hospital_sepsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110404</td>\n",
       "      <td>268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>188028</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>173727</td>\n",
       "      <td>271</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>164716</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>158689</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57277</th>\n",
       "      <td>54862</td>\n",
       "      <td>57277</td>\n",
       "      <td>182476</td>\n",
       "      <td>83967</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57278</th>\n",
       "      <td>29333</td>\n",
       "      <td>57278</td>\n",
       "      <td>118320</td>\n",
       "      <td>23200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57279</th>\n",
       "      <td>54573</td>\n",
       "      <td>57279</td>\n",
       "      <td>146497</td>\n",
       "      <td>73807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57280</th>\n",
       "      <td>14317</td>\n",
       "      <td>57280</td>\n",
       "      <td>118512</td>\n",
       "      <td>10762</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57281</th>\n",
       "      <td>4430</td>\n",
       "      <td>57281</td>\n",
       "      <td>122737</td>\n",
       "      <td>4094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57282 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  ts_ind  HADM_ID  SUBJECT_ID  in_hospital_sepsis\n",
       "0          0       0   110404         268                   1\n",
       "1          1       1   188028         270                   0\n",
       "2          2       2   173727         271                   0\n",
       "3          3       3   164716         272                   0\n",
       "4          4       4   158689         273                   0\n",
       "...      ...     ...      ...         ...                 ...\n",
       "57277  54862   57277   182476       83967                   0\n",
       "57278  29333   57278   118320       23200                   0\n",
       "57279  54573   57279   146497       73807                   0\n",
       "57280  14317   57280   118512       10762                   0\n",
       "57281   4430   57281   122737        4094                   0\n",
       "\n",
       "[57282 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_oc = oc.reset_index()\n",
    "reset_oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_op = reset_oc['in_hospital_sepsis'][train_ind]\n",
    "valid_op = reset_oc['in_hospital_sepsis'][valid_ind]\n",
    "test_op = reset_oc['in_hospital_sepsis'][test_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_op = train_op.to_numpy(dtype='float32')\n",
    "valid_op = valid_op.to_numpy(dtype='float32')\n",
    "test_op = test_op.to_numpy(dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_res(y_true, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    minrp = np.minimum(precision, recall).max()\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return [roc_auc, pr_auc, minrp]\n",
    "\n",
    "######################################################################################################## \n",
    "######################################################################################################## \n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=train_op)\n",
    "def mortality_loss(y_true, y_pred):\n",
    "    sample_weights = (1-y_true)*class_weights[0] + y_true*class_weights[1]\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    return K.mean(sample_weights*bce, axis=-1)\n",
    "######################################################################################################## \n",
    "######################################################################################################## \n",
    "\n",
    "# var_weights = np.sum(fore_train_op[:, V:], axis=0)\n",
    "# var_weights[var_weights==0] = var_weights.max()\n",
    "# var_weights = var_weights.max()/var_weights\n",
    "# var_weights = var_weights.reshape((1, V))\n",
    "def forecast_loss(y_true, y_pred):\n",
    "    return K.sum(y_true[:,V:]*(y_true[:,:V]-y_pred)**2, axis=-1)\n",
    "\n",
    "def get_min_loss(weight):\n",
    "    def min_loss(y_true, y_pred):\n",
    "        return weight*y_pred\n",
    "    return min_loss\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, validation_data, batch_size):\n",
    "        self.val_x, self.val_y = validation_data\n",
    "        self.batch_size = batch_size\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.val_x, verbose=0, batch_size=self.batch_size)\n",
    "        if type(y_pred)==type([]):\n",
    "            y_pred = y_pred[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(self.val_y, y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(self.val_y, y_pred)\n",
    "        logs['custom_metric'] = pr_auc + roc_auc\n",
    "        print ('val_aucs:', pr_auc, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Activation, Dropout, Softmax, Layer, InputSpec, Input, Dense, Lambda, TimeDistributed, Concatenate, Add\n",
    "from tensorflow.keras import initializers, regularizers, constraints, Model\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow import nn\n",
    "\n",
    "    \n",
    "class CVE(Layer):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        super(CVE, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        self.W1 = self.add_weight(name='CVE_W1',\n",
    "                            shape=(1, self.hid_units),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        self.b1 = self.add_weight(name='CVE_b1',\n",
    "                            shape=(self.hid_units,),\n",
    "                            initializer='zeros',\n",
    "                            trainable=True)\n",
    "        self.W2 = self.add_weight(name='CVE_W2',\n",
    "                            shape=(self.hid_units, self.output_dim),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        super(CVE, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = K.expand_dims(x, axis=-1)\n",
    "        x = K.dot(K.tanh(K.bias_add(K.dot(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.output_dim,)\n",
    "    \n",
    "    \n",
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, hid_dim):\n",
    "        self.hid_dim = hid_dim\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        self.W = self.add_weight(shape=(d, self.hid_dim), name='Att_W',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.hid_dim,), name='Att_b',\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(shape=(self.hid_dim,1), name='Att_u',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = K.dot(K.tanh(K.bias_add(K.dot(x,self.W), self.b)), self.u)\n",
    "        mask = K.expand_dims(mask, axis=-1)\n",
    "        attn_weights = mask*attn_weights + (1-mask)*mask_value\n",
    "        attn_weights = K.softmax(attn_weights, axis=-2)\n",
    "        return attn_weights\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (1,)\n",
    "    \n",
    "    \n",
    "class Transformer(Layer):\n",
    "    \n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0):\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = K.epsilon() * K.epsilon()\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        if self.dk==None:\n",
    "            self.dk = d//self.h\n",
    "        if self.dv==None:\n",
    "            self.dv = d//self.h\n",
    "        if self.dff==None:\n",
    "            self.dff = 2*d\n",
    "        self.Wq = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wq',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wk = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wk',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wv = self.add_weight(shape=(self.N, self.h, d, self.dv), name='Wv',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wo = self.add_weight(shape=(self.N, self.dv*self.h, d), name='Wo',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.W1 = self.add_weight(shape=(self.N, d, self.dff), name='W1',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b1 = self.add_weight(shape=(self.N, self.dff), name='b1',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.W2 = self.add_weight(shape=(self.N, self.dff, d), name='W2',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b2 = self.add_weight(shape=(self.N, d), name='b2',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.gamma = self.add_weight(shape=(2*self.N,), name='gamma',\n",
    "                                 initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(shape=(2*self.N,), name='beta',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Transformer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e-30):\n",
    "        mask = K.expand_dims(mask, axis=-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = K.dot(x, self.Wq[i,j,:,:])\n",
    "                k = K.permute_dimensions(K.dot(x, self.Wk[i,j,:,:]), (0,2,1))\n",
    "                v = K.dot(x, self.Wv[i,j,:,:])\n",
    "                A = K.batch_dot(q,k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask*A + (1-mask)*mask_value\n",
    "                # Mask for attention dropout.\n",
    "                def dropped_A():\n",
    "                    dp_mask = K.cast((K.random_uniform(shape=array_ops.shape(A))>=self.dropout), K.floatx())\n",
    "                    return A*dp_mask + (1-dp_mask)*mask_value\n",
    "                A = sc.smart_cond(K.learning_phase(), dropped_A, lambda: array_ops.identity(A))\n",
    "                A = K.softmax(A, axis=-1)\n",
    "                mha_ops.append(K.batch_dot(A,v))\n",
    "            conc = K.concatenate(mha_ops, axis=-1)\n",
    "            proj = K.dot(conc, self.Wo[i,:,:])\n",
    "            # Dropout.\n",
    "            proj = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(proj, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(proj))\n",
    "            # Add & LN\n",
    "            x = x+proj\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i] + self.beta[2*i]\n",
    "            # FFN\n",
    "            ffn_op = K.bias_add(K.dot(K.relu(K.bias_add(K.dot(x, self.W1[i,:,:]), self.b1[i,:])), \n",
    "                           self.W2[i,:,:]), self.b2[i,:,])\n",
    "            # Dropout.\n",
    "            ffn_op = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(ffn_op, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(ffn_op))\n",
    "            # Add & LN\n",
    "            x = x+ffn_op\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i+1] + self.beta[2*i+1]            \n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def build_strats(D, max_len, V, d, N, he, dropout, forecast=False):\n",
    "    demo = Input(shape=(D,))\n",
    "    demo_enc = Dense(2*d, activation='tanh')(demo)\n",
    "    demo_enc = Dense(d, activation='tanh')(demo_enc)\n",
    "    varis = Input(shape=(max_len,))\n",
    "    values = Input(shape=(max_len,))\n",
    "    times = Input(shape=(max_len,))\n",
    "    varis_emb = Embedding(V+1, d)(varis)\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    values_emb = CVE(cve_units, d)(values)\n",
    "    times_emb = CVE(cve_units, d)(times)\n",
    "    comb_emb = Add()([varis_emb, values_emb, times_emb]) # b, L, d\n",
    "#     demo_enc = Lambda(lambda x:K.expand_dims(x, axis=-2))(demo_enc) # b, 1, d\n",
    "#     comb_emb = Concatenate(axis=-2)([demo_enc, comb_emb]) # b, L+1, d\n",
    "    mask = Lambda(lambda x:K.clip(x,0,1))(varis) # b, L\n",
    "#     mask = Lambda(lambda x:K.concatenate((K.ones_like(x)[:,0:1], x), axis=-1))(mask) # b, L+1\n",
    "    cont_emb = Transformer(N, he, dk=None, dv=None, dff=None, dropout=dropout)(comb_emb, mask=mask)\n",
    "    attn_weights = Attention(2*d)(cont_emb, mask=mask)\n",
    "    fused_emb = Lambda(lambda x:K.sum(x[0]*x[1], axis=-2))([cont_emb, attn_weights])\n",
    "    # embed text input\n",
    "    texts = Input(shape=(33792,))\n",
    "    text_enc = Dense(2*d, activation='tanh')(texts)\n",
    "    text_enc = Dense(d, activation='tanh')(text_enc)\n",
    "    conc = Concatenate(axis=-1)([fused_emb, text_enc, demo_enc])\n",
    "    fore_op = Dense(V)(conc)\n",
    "    op = Dense(1, activation='sigmoid')(fore_op)\n",
    "    model = Model([demo, times, values, varis, texts], op)\n",
    "    if forecast:\n",
    "        fore_model = Model([demo, times, values, varis, texts], fore_op)\n",
    "        return [model, fore_model]\n",
    "    return model\n",
    "\n",
    "# To tune:\n",
    "# 1. Transformer parameters. (N, h, dropout)\n",
    "# 2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "lds = [10]\n",
    "batch_size, lr, patience = 32, 0.0005, 10\n",
    "d, N, he, dropout = 50,2,4,0.2\n",
    "fore_savepath = 'Exp0/models/forecasting/forecasting_50_epochs.h5'\n",
    "f = open('log_text_10.csv', 'a+')\n",
    "f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "train_inds = np.arange(len(train_op))\n",
    "valid_inds = np.arange(len(valid_op))\n",
    "gen_res = {}\n",
    "\n",
    "np.random.seed(2021)\n",
    "for ld in lds:\n",
    "    np.random.shuffle(train_inds)\n",
    "    np.random.shuffle(valid_inds)\n",
    "    train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "    valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "    f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "    all_test_res = []\n",
    "    for i in range(repeats[ld]):\n",
    "        print ('Repeat', i, 'ld', ld)\n",
    "        # Get train and validation data.\n",
    "        curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "        curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "        curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "        curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "        curr_train_op = train_op[curr_train_ind]\n",
    "        curr_valid_op = valid_op[curr_valid_ind]\n",
    "        print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "        # Construct save_path.\n",
    "        savepath = 'new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "        print (savepath)\n",
    "        # Build and compile model.\n",
    "        model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "        model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "        fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "        # Load pretrained weights here.\n",
    "        fore_model.load_weights(fore_savepath)\n",
    "        # Train model.\n",
    "        es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "                           restore_best_weights=True)\n",
    "        cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "        his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "                        verbose=1, callbacks=[cus, es]).history\n",
    "        model.save_weights(savepath)\n",
    "        # Test and write to log.\n",
    "        rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "        f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "        print ('Test res', rocauc, prauc, minrp)\n",
    "        all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "    gen_res[ld] = []\n",
    "    for i in range(len(all_test_res[0])):\n",
    "        nums = [test_res[i] for test_res in all_test_res]\n",
    "        gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "    print ('gen_res', gen_res)\n",
    "f.close()\n",
    "\n",
    "# # save to local\n",
    "# log_path = '/content/log.csv'\n",
    "# files.download(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "# lds = [20]\n",
    "# batch_size, lr, patience = 32, 0.0005, 10\n",
    "# d, N, he, dropout = 50,2,4,0.2\n",
    "# fore_savepath = 'Exp0/models/forecasting/forecasting_50_epochs.h5'\n",
    "# f = open('log_text_20.csv', 'a+')\n",
    "# f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "# train_inds = np.arange(len(train_op))\n",
    "# valid_inds = np.arange(len(valid_op))\n",
    "# gen_res = {}\n",
    "\n",
    "# np.random.seed(2021)\n",
    "# for ld in lds:\n",
    "#     np.random.shuffle(train_inds)\n",
    "#     np.random.shuffle(valid_inds)\n",
    "#     train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "#     valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "#     f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "#     all_test_res = []\n",
    "#     for i in range(repeats[ld]):\n",
    "#         print ('Repeat', i, 'ld', ld)\n",
    "#         # Get train and validation data.\n",
    "#         curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "#         curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "#         curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "#         curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "#         curr_train_op = train_op[curr_train_ind]\n",
    "#         curr_valid_op = valid_op[curr_valid_ind]\n",
    "#         print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "#         # Construct save_path.\n",
    "#         savepath = 'new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "#         print (savepath)\n",
    "#         # Build and compile model.\n",
    "#         model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "#         model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "#         fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "#         # Load pretrained weights here.\n",
    "#         fore_model.load_weights(fore_savepath)\n",
    "#         # Train model.\n",
    "#         es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "#                            restore_best_weights=True)\n",
    "#         cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "#         his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "#                         verbose=1, callbacks=[cus, es]).history\n",
    "#         model.save_weights(savepath)\n",
    "#         # Test and write to log.\n",
    "#         rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "#         f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "#         print ('Test res', rocauc, prauc, minrp)\n",
    "#         all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "#     gen_res[ld] = []\n",
    "#     for i in range(len(all_test_res[0])):\n",
    "#         nums = [test_res[i] for test_res in all_test_res]\n",
    "#         gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "#     print ('gen_res', gen_res)\n",
    "# f.close()\n",
    "\n",
    "# # # save to local\n",
    "# # log_path = '/content/log.csv'\n",
    "# # files.download(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "# lds = [30]\n",
    "# batch_size, lr, patience = 32, 0.0005, 10\n",
    "# d, N, he, dropout = 50,2,4,0.2\n",
    "# fore_savepath = 'Exp0/models/forecasting/forecasting_50_epochs.h5'\n",
    "# f = open('log_text_30.csv', 'a+')\n",
    "# f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "# train_inds = np.arange(len(train_op))\n",
    "# valid_inds = np.arange(len(valid_op))\n",
    "# gen_res = {}\n",
    "\n",
    "# np.random.seed(2021)\n",
    "# for ld in lds:\n",
    "#     np.random.shuffle(train_inds)\n",
    "#     np.random.shuffle(valid_inds)\n",
    "#     train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "#     valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "#     f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "#     all_test_res = []\n",
    "#     for i in range(repeats[ld]):\n",
    "#         print ('Repeat', i, 'ld', ld)\n",
    "#         # Get train and validation data.\n",
    "#         curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "#         curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "#         curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "#         curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "#         curr_train_op = train_op[curr_train_ind]\n",
    "#         curr_valid_op = valid_op[curr_valid_ind]\n",
    "#         print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "#         # Construct save_path.\n",
    "#         savepath = 'new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "#         print (savepath)\n",
    "#         # Build and compile model.\n",
    "#         model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "#         model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "#         fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "#         # Load pretrained weights here.\n",
    "#         fore_model.load_weights(fore_savepath)\n",
    "#         # Train model.\n",
    "#         es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "#                            restore_best_weights=True)\n",
    "#         cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "#         his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "#                         verbose=1, callbacks=[cus, es]).history\n",
    "#         model.save_weights(savepath)\n",
    "#         # Test and write to log.\n",
    "#         rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "#         f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "#         print ('Test res', rocauc, prauc, minrp)\n",
    "#         all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "#     gen_res[ld] = []\n",
    "#     for i in range(len(all_test_res[0])):\n",
    "#         nums = [test_res[i] for test_res in all_test_res]\n",
    "#         gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "#     print ('gen_res', gen_res)\n",
    "# f.close()\n",
    "\n",
    "# # # save to local\n",
    "# # log_path = '/content/log.csv'\n",
    "# # files.download(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 0 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_50ld.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 17:28:17.547859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30503 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3a:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 17:28:52.196962: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x562e7dac1a20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-09 17:28:52.196997: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2023-07-09 17:28:52.209720: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-07-09 17:28:52.488855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
      "2023-07-09 17:28:53.027911: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - ETA: 0s - loss: 0.4740val_aucs: 0.5246673385086463 0.8954150845150622\n",
      "572/572 [==============================] - 53s 55ms/step - loss: 0.4740 - custom_metric: 1.4201\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4139val_aucs: 0.5400186061459585 0.8978363981652623\n",
      "572/572 [==============================] - 28s 48ms/step - loss: 0.4139 - custom_metric: 1.4379\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3856val_aucs: 0.543232802622045 0.9032710318318244\n",
      "572/572 [==============================] - 27s 48ms/step - loss: 0.3856 - custom_metric: 1.4465\n",
      "Epoch 4/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3655val_aucs: 0.5333808986245889 0.8924187659674243\n",
      "572/572 [==============================] - 27s 48ms/step - loss: 0.3655 - custom_metric: 1.4258\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3403val_aucs: 0.5201640793418307 0.8896919333867231\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3403 - custom_metric: 1.4099\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3118val_aucs: 0.5079369848325905 0.8885525812289575\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3118 - custom_metric: 1.3965\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2858val_aucs: 0.4901810578510029 0.8855628602661972\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2858 - custom_metric: 1.3757\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2579val_aucs: 0.5100950317625863 0.8882967979383055\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2579 - custom_metric: 1.3984\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2302val_aucs: 0.4607527292744726 0.8713689176611258\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2302 - custom_metric: 1.3321\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2132val_aucs: 0.4633467628340167 0.878533387332028\n",
      "572/572 [==============================] - 27s 48ms/step - loss: 0.2132 - custom_metric: 1.3419\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1923val_aucs: 0.43930529330548673 0.8715089894631496\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1923 - custom_metric: 1.3108\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1865val_aucs: 0.4760028970553254 0.8837546145031155\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1865 - custom_metric: 1.3598\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1644val_aucs: 0.4820172695087498 0.8744241069661361\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1644 - custom_metric: 1.3564\n",
      "Test res 0.8864394783688369 0.4998660164417442 0.4892578125\n",
      "Repeat 1 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4700val_aucs: 0.5164808637322487 0.8993959657290991\n",
      "572/572 [==============================] - 36s 52ms/step - loss: 0.4700 - custom_metric: 1.4159\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4202val_aucs: 0.5252389573827452 0.9015944840130369\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.4202 - custom_metric: 1.4268\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3946val_aucs: 0.5291791274784187 0.9002937447790265\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3946 - custom_metric: 1.4295\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3748val_aucs: 0.52877852987354 0.8977414944443259\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3748 - custom_metric: 1.4265\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3515val_aucs: 0.49645329334184657 0.8927892457336465\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3515 - custom_metric: 1.3892\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3239val_aucs: 0.4676354250834839 0.8822980707646803\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3239 - custom_metric: 1.3499\n",
      "Epoch 7/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3010val_aucs: 0.4657958313526753 0.8845341445317694\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3010 - custom_metric: 1.3503\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2676val_aucs: 0.48846255494789953 0.8873487757419999\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2676 - custom_metric: 1.3758\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2376val_aucs: 0.4502497424503755 0.8765005699298323\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2376 - custom_metric: 1.3268\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2301val_aucs: 0.462152741381063 0.8825589291206231\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2301 - custom_metric: 1.3447\n",
      "Epoch 11/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.1904val_aucs: 0.4499923974442208 0.8710227982082989\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1904 - custom_metric: 1.3210\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1814val_aucs: 0.4272831907379041 0.864907598286252\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1814 - custom_metric: 1.2922\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1540val_aucs: 0.4329101480047187 0.8619632991578436\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1540 - custom_metric: 1.2949\n",
      "Test res 0.8841406997965532 0.47759614239302883 0.4970873786407767\n",
      "Repeat 2 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4808val_aucs: 0.5087066637848033 0.8844386937477001\n",
      "572/572 [==============================] - 36s 51ms/step - loss: 0.4808 - custom_metric: 1.3931\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4216val_aucs: 0.5303110181412639 0.893383764042054\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.4216 - custom_metric: 1.4237\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3976val_aucs: 0.5406826711586699 0.8988522018178006\n",
      "572/572 [==============================] - 27s 48ms/step - loss: 0.3976 - custom_metric: 1.4395\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3777val_aucs: 0.4926047227419488 0.8832827839175698\n",
      "572/572 [==============================] - 28s 48ms/step - loss: 0.3777 - custom_metric: 1.3759\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3500val_aucs: 0.5105818397414364 0.8818001824131559\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3500 - custom_metric: 1.3924\n",
      "Epoch 6/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3277val_aucs: 0.5025229201397916 0.880375945204829\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3277 - custom_metric: 1.3829\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2932val_aucs: 0.5232461572514108 0.8778380875504012\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2932 - custom_metric: 1.4011\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2731val_aucs: 0.5014988152133908 0.8797542170676985\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2731 - custom_metric: 1.3813\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2425val_aucs: 0.48295647202634284 0.8693910922235015\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2425 - custom_metric: 1.3523\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2209val_aucs: 0.4521778350921993 0.8558426617284536\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2209 - custom_metric: 1.3080\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2026val_aucs: 0.45958983900214784 0.8641195102938791\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2026 - custom_metric: 1.3237\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1772val_aucs: 0.4537992328673645 0.8708620703299661\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1772 - custom_metric: 1.3247\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1598val_aucs: 0.45785201718645907 0.8632054660636348\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1598 - custom_metric: 1.3211\n",
      "Test res 0.8880252120482289 0.49682837811872355 0.49612403100775193\n",
      "Repeat 3 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4725val_aucs: 0.49345297321201487 0.885283731675208\n",
      "572/572 [==============================] - 36s 51ms/step - loss: 0.4725 - custom_metric: 1.3787\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4162val_aucs: 0.5193992356110685 0.8934936647959977\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.4162 - custom_metric: 1.4129\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3900val_aucs: 0.5021091062747901 0.8898025698100607\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3900 - custom_metric: 1.3919\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3695val_aucs: 0.5348775161780462 0.8953003880394922\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3695 - custom_metric: 1.4302\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3390val_aucs: 0.5183721650097491 0.8826132323192337\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3390 - custom_metric: 1.4010\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3157val_aucs: 0.4916543678472182 0.8818930805544813\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3157 - custom_metric: 1.3735\n",
      "Epoch 7/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2837val_aucs: 0.5112051379319336 0.8891976220274074\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2837 - custom_metric: 1.4004\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2522val_aucs: 0.4805286830025213 0.8672687724127571\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2522 - custom_metric: 1.3478\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2282val_aucs: 0.4749318925060483 0.868674565498285\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2282 - custom_metric: 1.3436\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1956val_aucs: 0.4857677164913301 0.8754553602332094\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1956 - custom_metric: 1.3612\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1828val_aucs: 0.4495522720739324 0.8701732322786333\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1828 - custom_metric: 1.3197\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1626val_aucs: 0.46920243951793406 0.8671929001866608\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1626 - custom_metric: 1.3364\n",
      "Epoch 13/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.1388val_aucs: 0.4356774692476344 0.8581879580880808\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1388 - custom_metric: 1.2939\n",
      "Epoch 14/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1442val_aucs: 0.4536242320445595 0.8627773396304743\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1442 - custom_metric: 1.3164\n",
      "Test res 0.8834396691000479 0.48671146732971515 0.4819512195121951\n",
      "Repeat 4 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4727val_aucs: 0.5190260219125244 0.890232752718454\n",
      "572/572 [==============================] - 36s 52ms/step - loss: 0.4727 - custom_metric: 1.4093\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4212val_aucs: 0.5599356736113541 0.9023783786050773\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.4212 - custom_metric: 1.4623\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4013val_aucs: 0.5486620168764341 0.9010378932204046\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.4013 - custom_metric: 1.4497\n",
      "Epoch 4/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3786val_aucs: 0.5154540238007991 0.8924780788438169\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3785 - custom_metric: 1.4079\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3559val_aucs: 0.5307121920029033 0.8955868306354121\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3559 - custom_metric: 1.4263\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3292val_aucs: 0.5023142292119476 0.8875454910517226\n",
      "572/572 [==============================] - 27s 48ms/step - loss: 0.3292 - custom_metric: 1.3899\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3017val_aucs: 0.4776656574070723 0.8855229675421149\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3017 - custom_metric: 1.3632\n",
      "Epoch 8/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2758val_aucs: 0.483452651124109 0.884193491227868\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2761 - custom_metric: 1.3676\n",
      "Epoch 9/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2662val_aucs: 0.4473214817033184 0.8714984603028648\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2663 - custom_metric: 1.3188\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2231val_aucs: 0.4503908863366953 0.8722995012366854\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2231 - custom_metric: 1.3227\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2041val_aucs: 0.45510405511459723 0.8727524458484796\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2041 - custom_metric: 1.3279\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1693val_aucs: 0.4621889835654373 0.8673978436900726\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1693 - custom_metric: 1.3296\n",
      "Test res 0.8848982486387028 0.4993838875232969 0.5166015625\n",
      "Repeat 5 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4953val_aucs: 0.4707428334607461 0.8832498161571594\n",
      "572/572 [==============================] - 36s 51ms/step - loss: 0.4953 - custom_metric: 1.3540\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4200val_aucs: 0.45587592855197545 0.8867002836432398\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.4200 - custom_metric: 1.3426\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3959val_aucs: 0.5229084948617776 0.8958514549847674\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3959 - custom_metric: 1.4188\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3748val_aucs: 0.5206052672535438 0.8919035612984557\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3748 - custom_metric: 1.4125\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3529val_aucs: 0.49775585562380936 0.8899516755961759\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3529 - custom_metric: 1.3877\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3347val_aucs: 0.5032432933406806 0.8868268725706483\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3347 - custom_metric: 1.3901\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2995val_aucs: 0.47468457753906096 0.875176489127009\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2995 - custom_metric: 1.3499\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2799val_aucs: 0.497070785383838 0.8810368736211787\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2799 - custom_metric: 1.3781\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2490val_aucs: 0.4754645186249842 0.8779514654900725\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2490 - custom_metric: 1.3534\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2173val_aucs: 0.4497019678138045 0.8715857758167875\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2173 - custom_metric: 1.3213\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2054val_aucs: 0.45365621719990085 0.8631626221241727\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2054 - custom_metric: 1.3168\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1771val_aucs: 0.48135676270645794 0.8677818048114297\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1771 - custom_metric: 1.3491\n",
      "Epoch 13/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.1668val_aucs: 0.46299918177805327 0.8529898098539762\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1668 - custom_metric: 1.3160\n",
      "Test res 0.8849025961883676 0.4870649061872147 0.4833984375\n",
      "Repeat 6 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4871val_aucs: 0.43761970262297983 0.877188994992034\n",
      "572/572 [==============================] - 36s 51ms/step - loss: 0.4871 - custom_metric: 1.3148\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4284val_aucs: 0.49130172900548363 0.8886010305803247\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.4284 - custom_metric: 1.3799\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4039val_aucs: 0.5051907152839255 0.8926131129433026\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.4039 - custom_metric: 1.3978\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3788val_aucs: 0.5026275937745805 0.8884198610154939\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3788 - custom_metric: 1.3910\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3623val_aucs: 0.4670750274964266 0.8868266668423463\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3623 - custom_metric: 1.3539\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3399val_aucs: 0.47816088577183913 0.8856507116669906\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3399 - custom_metric: 1.3638\n",
      "Epoch 7/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3101val_aucs: 0.47009600346940755 0.8819026976697201\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3101 - custom_metric: 1.3520\n",
      "Epoch 8/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2940val_aucs: 0.4301897103021672 0.8722260469130353\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2940 - custom_metric: 1.3024\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2587val_aucs: 0.4819725968655365 0.8820701422675182\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2587 - custom_metric: 1.3640\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2328val_aucs: 0.47281076700113484 0.8797357998746087\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2328 - custom_metric: 1.3525\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2134val_aucs: 0.4225988737973948 0.8677967255522652\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2134 - custom_metric: 1.2904\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1805val_aucs: 0.4350200081869121 0.8770819402491795\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1805 - custom_metric: 1.3121\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1834val_aucs: 0.4342322039732104 0.8770226483915986\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1834 - custom_metric: 1.3113\n",
      "Test res 0.8865651365785064 0.4925396176753717 0.494140625\n",
      "Repeat 7 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.5007val_aucs: 0.45467125373091993 0.8724043231848004\n",
      "572/572 [==============================] - 36s 51ms/step - loss: 0.5007 - custom_metric: 1.3271\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4305val_aucs: 0.4872363450460639 0.88108216053614\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.4305 - custom_metric: 1.3683\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3963val_aucs: 0.48297237480940897 0.8834879030705264\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3963 - custom_metric: 1.3665\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3801val_aucs: 0.47748359176308963 0.8842157323348742\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3801 - custom_metric: 1.3617\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3596val_aucs: 0.4683219779408978 0.8813847350602453\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3596 - custom_metric: 1.3497\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3384val_aucs: 0.448501738309731 0.8794158001115675\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3384 - custom_metric: 1.3279\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3132val_aucs: 0.4526443793036101 0.8692019844487698\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3132 - custom_metric: 1.3218\n",
      "Epoch 8/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2900val_aucs: 0.4434397478489404 0.8701267622215352\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2899 - custom_metric: 1.3136\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2639val_aucs: 0.4218872590792835 0.8634547188972643\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2639 - custom_metric: 1.2853\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2519val_aucs: 0.402610078309606 0.8568855509111896\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2519 - custom_metric: 1.2595\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2250val_aucs: 0.4314058280754874 0.8633281877326385\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2250 - custom_metric: 1.2947\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2020val_aucs: 0.41958346717862044 0.8615875590157855\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2020 - custom_metric: 1.2812\n",
      "Test res 0.8815976169818095 0.488551346480842 0.490234375\n",
      "Repeat 8 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4785val_aucs: 0.4686147779738555 0.8837078988973163\n",
      "572/572 [==============================] - 37s 52ms/step - loss: 0.4785 - custom_metric: 1.3523\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4209val_aucs: 0.47414394507186564 0.88781433377583\n",
      "572/572 [==============================] - 27s 48ms/step - loss: 0.4209 - custom_metric: 1.3620\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4006val_aucs: 0.4620413867741502 0.8905382448229012\n",
      "572/572 [==============================] - 27s 48ms/step - loss: 0.4006 - custom_metric: 1.3526\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3785val_aucs: 0.46302120504508965 0.8838435385494563\n",
      "572/572 [==============================] - 27s 48ms/step - loss: 0.3785 - custom_metric: 1.3469\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3527val_aucs: 0.47739658708406246 0.882657247493446\n",
      "572/572 [==============================] - 27s 48ms/step - loss: 0.3527 - custom_metric: 1.3601\n",
      "Epoch 6/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3328val_aucs: 0.46249496253106065 0.8792306785762729\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3328 - custom_metric: 1.3417\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3060val_aucs: 0.46325780828310564 0.8769531554007929\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3060 - custom_metric: 1.3402\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2770val_aucs: 0.44571145455572575 0.8714113861705368\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2770 - custom_metric: 1.3171\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2570val_aucs: 0.4234827966356339 0.8669041472379542\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2570 - custom_metric: 1.2904\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2285val_aucs: 0.4004920305401881 0.8584005416691681\n",
      "572/572 [==============================] - 27s 48ms/step - loss: 0.2285 - custom_metric: 1.2589\n",
      "Epoch 11/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2066val_aucs: 0.42131225180569776 0.860900980830337\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2066 - custom_metric: 1.2822\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1812val_aucs: 0.41962212317830194 0.8654571389817243\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1812 - custom_metric: 1.2851\n",
      "Test res 0.8880942586165629 0.4712148559978942 0.4796511627906977\n",
      "Repeat 9 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4953val_aucs: 0.4865138109651647 0.8792242157182953\n",
      "572/572 [==============================] - 36s 51ms/step - loss: 0.4953 - custom_metric: 1.3657\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4215val_aucs: 0.5220407457404018 0.8910237318910781\n",
      "572/572 [==============================] - 27s 48ms/step - loss: 0.4215 - custom_metric: 1.4131\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3938val_aucs: 0.5162860188535559 0.8875273329163045\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3938 - custom_metric: 1.4038\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3825val_aucs: 0.49937654729448666 0.8889107244675963\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3825 - custom_metric: 1.3883\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3562val_aucs: 0.48859420049608193 0.8863360329489657\n",
      "572/572 [==============================] - 27s 48ms/step - loss: 0.3562 - custom_metric: 1.3749\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3294val_aucs: 0.5013386279493985 0.8874238138206296\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.3294 - custom_metric: 1.3888\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3069val_aucs: 0.473759971311914 0.8817568353049937\n",
      "572/572 [==============================] - 27s 48ms/step - loss: 0.3069 - custom_metric: 1.3555\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2817val_aucs: 0.4561801751999691 0.8756359227869334\n",
      "572/572 [==============================] - 30s 52ms/step - loss: 0.2817 - custom_metric: 1.3318\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2528val_aucs: 0.47566062996251135 0.8756558515967959\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2528 - custom_metric: 1.3513\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2402val_aucs: 0.42605732100315497 0.8609245860620117\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2402 - custom_metric: 1.2870\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2062val_aucs: 0.4390417156606007 0.8666707262390461\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.2062 - custom_metric: 1.3057\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1907val_aucs: 0.3948608406693176 0.8545396168132726\n",
      "572/572 [==============================] - 27s 47ms/step - loss: 0.1907 - custom_metric: 1.2494\n",
      "Test res 0.8857439639181427 0.4896114453483846 0.48787584869059164\n",
      "gen_res {50: [(0.8853846880235758, 0.001925897421069688), (0.48893680634962167, 0.008692435429895565), (0.4916322453142013, 0.010032248332424354)]}\n"
     ]
    }
   ],
   "source": [
    "repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "lds = [50]\n",
    "batch_size, lr, patience = 32, 0.0005, 10\n",
    "d, N, he, dropout = 50,2,4,0.2\n",
    "fore_savepath = 'Exp0/models/forecasting/forecasting_50_epochs.h5'\n",
    "f = open('Exp0/logs/log_text_50.csv', 'a+')\n",
    "f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "train_inds = np.arange(len(train_op))\n",
    "valid_inds = np.arange(len(valid_op))\n",
    "gen_res = {}\n",
    "\n",
    "np.random.seed(2021)\n",
    "for ld in lds:\n",
    "    np.random.shuffle(train_inds)\n",
    "    np.random.shuffle(valid_inds)\n",
    "    train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "    valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "    f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "    all_test_res = []\n",
    "    for i in range(repeats[ld]):\n",
    "        print ('Repeat', i, 'ld', ld)\n",
    "        # Get train and validation data.\n",
    "        curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "        curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "        curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "        curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "        curr_train_op = train_op[curr_train_ind]\n",
    "        curr_valid_op = valid_op[curr_valid_ind]\n",
    "        print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "        # Construct save_path.\n",
    "        savepath = 'Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "        print (savepath)\n",
    "        # Build and compile model.\n",
    "        model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "        model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "        fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "        # Load pretrained weights here.\n",
    "        fore_model.load_weights(fore_savepath)\n",
    "        # Train model.\n",
    "        es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "                           restore_best_weights=True)\n",
    "        cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "        his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "                        verbose=1, callbacks=[cus, es]).history\n",
    "        model.save_weights(savepath)\n",
    "        # Test and write to log.\n",
    "        rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "        f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "        print ('Test res', rocauc, prauc, minrp)\n",
    "        all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "    gen_res[ld] = []\n",
    "    for i in range(len(all_test_res[0])):\n",
    "        nums = [test_res[i] for test_res in all_test_res]\n",
    "        gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "    print ('gen_res', gen_res)\n",
    "f.close()\n",
    "\n",
    "# # save to local\n",
    "# log_path = '/content/log.csv'\n",
    "# files.download(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 0 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_40ld.h5\n",
      "Epoch 1/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4942val_aucs: 0.47223526544153976 0.8861455872671649\n",
      "457/457 [==============================] - 35s 55ms/step - loss: 0.4942 - custom_metric: 1.3584\n",
      "Epoch 2/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4266val_aucs: 0.4778473467423155 0.8831968271179439\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.4266 - custom_metric: 1.3610\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3981val_aucs: 0.5096354577377338 0.8978736655952703\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.3981 - custom_metric: 1.4075\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3691val_aucs: 0.47219513710573385 0.8861274348449056\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3691 - custom_metric: 1.3583\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3440val_aucs: 0.4943667328234731 0.8894549755386025\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3440 - custom_metric: 1.3838\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3132val_aucs: 0.4671399130824926 0.878220643095948\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3132 - custom_metric: 1.3454\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2898val_aucs: 0.4837708670373527 0.8818898527152798\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2898 - custom_metric: 1.3657\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2530val_aucs: 0.4430892351805785 0.8643957583033214\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2530 - custom_metric: 1.3075\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2339val_aucs: 0.4472109506067916 0.8655115271915217\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2339 - custom_metric: 1.3127\n",
      "Epoch 10/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.2023val_aucs: 0.4414224230430695 0.8666264893053995\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2022 - custom_metric: 1.3080\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1864val_aucs: 0.43914992768074823 0.8579520517884572\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1864 - custom_metric: 1.2971\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1510val_aucs: 0.4278602846936502 0.8613372768462224\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1510 - custom_metric: 1.2892\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1449val_aucs: 0.4182075944196805 0.8636599801210807\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1449 - custom_metric: 1.2819\n",
      "Test res 0.8803753945518189 0.4635064204964292 0.4756335282651072\n",
      "Repeat 1 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_40ld.h5\n",
      "Epoch 1/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4944val_aucs: 0.4980749580814566 0.8848165290908125\n",
      "457/457 [==============================] - 32s 52ms/step - loss: 0.4944 - custom_metric: 1.3829\n",
      "Epoch 2/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4250val_aucs: 0.4843035080170204 0.8781009222735463\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.4247 - custom_metric: 1.3624\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4015val_aucs: 0.5020541088797243 0.8840497896541708\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.4015 - custom_metric: 1.3861\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3799val_aucs: 0.5063240771632006 0.8896254376348822\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3799 - custom_metric: 1.3959\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3532val_aucs: 0.49375236981956905 0.8828239435080125\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3532 - custom_metric: 1.3766\n",
      "Epoch 6/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3320val_aucs: 0.4925155916849973 0.8870371064409237\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.3319 - custom_metric: 1.3796\n",
      "Epoch 7/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3071val_aucs: 0.46591404088023514 0.8756422028377167\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3072 - custom_metric: 1.3416\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2812val_aucs: 0.4820603834759091 0.8786427931177717\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2812 - custom_metric: 1.3607\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2531val_aucs: 0.49368649620882615 0.8700485966013611\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2531 - custom_metric: 1.3637\n",
      "Epoch 10/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.2306val_aucs: 0.4600639353727716 0.8626099747959786\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.2315 - custom_metric: 1.3227\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2078val_aucs: 0.47243073228644994 0.8738284190176366\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2078 - custom_metric: 1.3463\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1793val_aucs: 0.4431574637425264 0.8633380649432989\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1793 - custom_metric: 1.3065\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1559val_aucs: 0.45256235616265317 0.8609991973440521\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1559 - custom_metric: 1.3136\n",
      "Epoch 14/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1457val_aucs: 0.44180883736908827 0.8612670095912701\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1457 - custom_metric: 1.3031\n",
      "Test res 0.8818737097594544 0.468436460995955 0.4921875\n",
      "Repeat 2 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_40ld.h5\n",
      "Epoch 1/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4858val_aucs: 0.5138759606076246 0.8857778038939162\n",
      "457/457 [==============================] - 30s 52ms/step - loss: 0.4858 - custom_metric: 1.3997\n",
      "Epoch 2/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4230val_aucs: 0.5331592703862567 0.8931455683424186\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.4230 - custom_metric: 1.4263\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3906val_aucs: 0.5439451921551324 0.892190320354617\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.3906 - custom_metric: 1.4361\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3627val_aucs: 0.5350051608776755 0.8922490816661693\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.3627 - custom_metric: 1.4273\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3343val_aucs: 0.5374066508722461 0.891677288903755\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3343 - custom_metric: 1.4291\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3099val_aucs: 0.5152124434329411 0.8826498940789692\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3099 - custom_metric: 1.3979\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2846val_aucs: 0.49241424178089405 0.8758720027964357\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2846 - custom_metric: 1.3683\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2513val_aucs: 0.5105148585088624 0.8799242732431121\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2513 - custom_metric: 1.3904\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2195val_aucs: 0.5157125157447998 0.8776009413863451\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2195 - custom_metric: 1.3933\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2110val_aucs: 0.5132854733605551 0.8809450626937992\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2110 - custom_metric: 1.3942\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1781val_aucs: 0.4841130900487432 0.8726996453227502\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1781 - custom_metric: 1.3568\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1604val_aucs: 0.4731953842719256 0.8633980310440529\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1604 - custom_metric: 1.3366\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1483val_aucs: 0.4801494273469903 0.870208316382955\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1483 - custom_metric: 1.3504\n",
      "Test res 0.8815749910244136 0.47441881271535696 0.47609756097560973\n",
      "Repeat 3 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_40ld.h5\n",
      "Epoch 1/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4874val_aucs: 0.5122726263027279 0.8890720731982487\n",
      "457/457 [==============================] - 31s 53ms/step - loss: 0.4893 - custom_metric: 1.4013\n",
      "Epoch 2/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4227val_aucs: 0.5364785255187894 0.8957885691418561\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.4225 - custom_metric: 1.4323\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3964val_aucs: 0.5432455114748822 0.8984427761306915\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3964 - custom_metric: 1.4417\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3678val_aucs: 0.5314245548832849 0.8987505729423411\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3678 - custom_metric: 1.4302\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3407val_aucs: 0.5352747194587884 0.8885634026053054\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3407 - custom_metric: 1.4238\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3129val_aucs: 0.5066880462291564 0.8877842796414619\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3129 - custom_metric: 1.3945\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2866val_aucs: 0.48850602724053327 0.8770023499480716\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2866 - custom_metric: 1.3655\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2535val_aucs: 0.4735532396777725 0.8750832387929331\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2535 - custom_metric: 1.3486\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2226val_aucs: 0.4902672655568516 0.8704753279038447\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2226 - custom_metric: 1.3607\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1952val_aucs: 0.49792614588063727 0.8752860780711085\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1952 - custom_metric: 1.3732\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1829val_aucs: 0.4860756706653709 0.8648398552450856\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1829 - custom_metric: 1.3509\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1603val_aucs: 0.45831414073288645 0.8617681767186108\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1603 - custom_metric: 1.3201\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1393val_aucs: 0.4536759700953691 0.8658159209964598\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1393 - custom_metric: 1.3195\n",
      "Test res 0.8849454639630205 0.4794583650773382 0.4833984375\n",
      "Repeat 4 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_40ld.h5\n",
      "Epoch 1/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4784val_aucs: 0.5152592529132812 0.8849542246317512\n",
      "457/457 [==============================] - 31s 52ms/step - loss: 0.4784 - custom_metric: 1.4002\n",
      "Epoch 2/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4129val_aucs: 0.5259634118507991 0.8939918230871522\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.4133 - custom_metric: 1.4200\n",
      "Epoch 3/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3861val_aucs: 0.5283894905425505 0.8979260433715222\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.3865 - custom_metric: 1.4263\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3624val_aucs: 0.5208118179337953 0.8919731677066285\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3624 - custom_metric: 1.4128\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3348val_aucs: 0.52184698056695 0.889891379398527\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3348 - custom_metric: 1.4117\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3034val_aucs: 0.5299918960084132 0.8899561106280687\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3034 - custom_metric: 1.4199\n",
      "Epoch 7/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.2680val_aucs: 0.4711376193332377 0.8810247992532735\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.2681 - custom_metric: 1.3522\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2483val_aucs: 0.500565620857337 0.8911028922872339\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2483 - custom_metric: 1.3917\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2202val_aucs: 0.4798527810361911 0.8853266289893617\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2202 - custom_metric: 1.3652\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1890val_aucs: 0.4616084444029461 0.8700692384410801\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1890 - custom_metric: 1.3317\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1694val_aucs: 0.45852387065708783 0.8671443458469723\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1694 - custom_metric: 1.3257\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1599val_aucs: 0.4779753163614132 0.8767237686681669\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1599 - custom_metric: 1.3547\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1490val_aucs: 0.45508368583295605 0.8736622212561375\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1490 - custom_metric: 1.3287\n",
      "Test res 0.8856321430708473 0.4762156163971888 0.4868292682926829\n",
      "Repeat 5 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_40ld.h5\n",
      "Epoch 1/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4874val_aucs: 0.46378748779442214 0.8790703290572133\n",
      "457/457 [==============================] - 30s 52ms/step - loss: 0.4877 - custom_metric: 1.3429\n",
      "Epoch 2/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4253val_aucs: 0.48913291547440496 0.8936399482035717\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.4253 - custom_metric: 1.3828\n",
      "Epoch 3/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3989val_aucs: 0.5051402256544362 0.8929471085185138\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3992 - custom_metric: 1.3981\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3655val_aucs: 0.4709576448575216 0.8825162072023682\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3655 - custom_metric: 1.3535\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3407val_aucs: 0.5230672449924598 0.8894237855527936\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.3407 - custom_metric: 1.4125\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3131val_aucs: 0.48862496209000944 0.8837603207714504\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3131 - custom_metric: 1.3724\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2842val_aucs: 0.4710596231200307 0.8816451611560098\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2842 - custom_metric: 1.3527\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2587val_aucs: 0.44503553670160195 0.8698710502100587\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2587 - custom_metric: 1.3149\n",
      "Epoch 9/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.2209val_aucs: 0.44525138295462113 0.874406985023171\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2207 - custom_metric: 1.3197\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2024val_aucs: 0.4286014255473457 0.8674044743120526\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2024 - custom_metric: 1.2960\n",
      "Epoch 11/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.1748val_aucs: 0.40318844107830404 0.8636413222245818\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1751 - custom_metric: 1.2668\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1765val_aucs: 0.45589669048632653 0.8712533986201498\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1765 - custom_metric: 1.3272\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1302val_aucs: 0.43228796096100364 0.870137527012004\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1302 - custom_metric: 1.3024\n",
      "Epoch 14/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1315val_aucs: 0.41820755625894446 0.8643374928696637\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1315 - custom_metric: 1.2825\n",
      "Epoch 15/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1249val_aucs: 0.430137481361804 0.8641376352682049\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1249 - custom_metric: 1.2943\n",
      "Test res 0.8796698293142651 0.4733348679005625 0.49221789883268485\n",
      "Repeat 6 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_40ld.h5\n",
      "Epoch 1/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4892val_aucs: 0.4153727634057725 0.8607660147300569\n",
      "457/457 [==============================] - 31s 52ms/step - loss: 0.4890 - custom_metric: 1.2761\n",
      "Epoch 2/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4313val_aucs: 0.4511329080887936 0.8764728978172418\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.4313 - custom_metric: 1.3276\n",
      "Epoch 3/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3998val_aucs: 0.45739052954466464 0.8693296400326185\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.3994 - custom_metric: 1.3267\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3764val_aucs: 0.4516625830385193 0.8745658430088322\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.3764 - custom_metric: 1.3262\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3445val_aucs: 0.41153008578053324 0.8594336651263531\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3445 - custom_metric: 1.2710\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3283val_aucs: 0.43418977422462307 0.8594310763641698\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3283 - custom_metric: 1.2936\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2936val_aucs: 0.4259312205240904 0.8616099512018329\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2936 - custom_metric: 1.2875\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2604val_aucs: 0.4461510025992118 0.8592584922186124\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2604 - custom_metric: 1.3054\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2417val_aucs: 0.3997972584157454 0.8511677474748783\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2417 - custom_metric: 1.2510\n",
      "Epoch 10/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.2122val_aucs: 0.4153207924652404 0.858164308735778\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2121 - custom_metric: 1.2735\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1855val_aucs: 0.3938681231144851 0.8497335732252957\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1855 - custom_metric: 1.2436\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1721val_aucs: 0.3893084697866381 0.8414965634182016\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1721 - custom_metric: 1.2308\n",
      "Test res 0.8841140535244136 0.4840345455427791 0.48530805687203793\n",
      "Repeat 7 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_40ld.h5\n",
      "Epoch 1/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4877val_aucs: 0.4563241918833617 0.8736517093399607\n",
      "457/457 [==============================] - 30s 52ms/step - loss: 0.4879 - custom_metric: 1.3300\n",
      "Epoch 2/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4291val_aucs: 0.4743780497173192 0.8797122442492198\n",
      "457/457 [==============================] - 22s 49ms/step - loss: 0.4296 - custom_metric: 1.3541\n",
      "Epoch 3/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4085val_aucs: 0.46629396700399767 0.8786944789619698\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.4084 - custom_metric: 1.3450\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3722val_aucs: 0.4659379088584729 0.8761830279736448\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3722 - custom_metric: 1.3421\n",
      "Epoch 5/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3510val_aucs: 0.4543239364048755 0.8744166136862791\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3514 - custom_metric: 1.3287\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3278val_aucs: 0.4233265059208686 0.8670800341001041\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3278 - custom_metric: 1.2904\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3002val_aucs: 0.43768673169953665 0.8687714931799793\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3002 - custom_metric: 1.3065\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2786val_aucs: 0.42864080125982945 0.8648702100913188\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2786 - custom_metric: 1.2935\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2528val_aucs: 0.4116657938437537 0.8558006299849727\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2528 - custom_metric: 1.2675\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2209val_aucs: 0.43827049679830415 0.863269961565137\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2209 - custom_metric: 1.3015\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1976val_aucs: 0.4042847339534599 0.8530029042885218\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1976 - custom_metric: 1.2573\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1805val_aucs: 0.40444902157413853 0.8577241792856317\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1805 - custom_metric: 1.2622\n",
      "Test res 0.8849258298677597 0.47809526240818 0.482421875\n",
      "Repeat 8 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_40ld.h5\n",
      "Epoch 1/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4941val_aucs: 0.43264355197683996 0.8639084354409895\n",
      "457/457 [==============================] - 30s 53ms/step - loss: 0.4941 - custom_metric: 1.2966\n",
      "Epoch 2/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.4284val_aucs: 0.46786778617645786 0.8834320093052827\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.4284 - custom_metric: 1.3513\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3962val_aucs: 0.4664215940044975 0.8846863801872618\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3962 - custom_metric: 1.3511\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3750val_aucs: 0.48308076614389683 0.8829669257889263\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3750 - custom_metric: 1.3660\n",
      "Epoch 5/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3510val_aucs: 0.4704020615799402 0.8862929502369669\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3510 - custom_metric: 1.3567\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3263val_aucs: 0.458052878746486 0.8792742529765345\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3263 - custom_metric: 1.3373\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2963val_aucs: 0.4563292862026383 0.8670466203329094\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2963 - custom_metric: 1.3234\n",
      "Epoch 8/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2789val_aucs: 0.39883742128625915 0.8563740824760142\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2789 - custom_metric: 1.2552\n",
      "Epoch 9/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.2466val_aucs: 0.41963935586679363 0.8615207056987632\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2462 - custom_metric: 1.2812\n",
      "Epoch 10/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.2180val_aucs: 0.42004675684243514 0.852233484568258\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2182 - custom_metric: 1.2723\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1952val_aucs: 0.4225265666140638 0.8632573185180905\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1952 - custom_metric: 1.2858\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1675val_aucs: 0.3747099334839409 0.836999515951913\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1675 - custom_metric: 1.2117\n",
      "Epoch 13/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1428val_aucs: 0.3920037277581788 0.8530959209917928\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1428 - custom_metric: 1.2451\n",
      "Epoch 14/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1241val_aucs: 0.3964909078191144 0.8475907770777946\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1241 - custom_metric: 1.2441\n",
      "Test res 0.8869055543920535 0.469589899947052 0.48588120740019475\n",
      "Repeat 9 ld 40\n",
      "Num train: 14620 Num valid: 3704\n",
      "Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_40ld.h5\n",
      "Epoch 1/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4932val_aucs: 0.48320880430533625 0.8768581327068853\n",
      "457/457 [==============================] - 31s 52ms/step - loss: 0.4929 - custom_metric: 1.3601\n",
      "Epoch 2/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.4205val_aucs: 0.5383918381470324 0.8893469819486336\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.4205 - custom_metric: 1.4277\n",
      "Epoch 3/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3966val_aucs: 0.500814476853735 0.8871050582368243\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3966 - custom_metric: 1.3879\n",
      "Epoch 4/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3733val_aucs: 0.5154592896402742 0.8846896056184999\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3733 - custom_metric: 1.4001\n",
      "Epoch 5/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.3492val_aucs: 0.48601847291729317 0.8897934482809738\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3508 - custom_metric: 1.3758\n",
      "Epoch 6/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.3180val_aucs: 0.45425235378193 0.8706825964458836\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.3180 - custom_metric: 1.3249\n",
      "Epoch 7/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2931val_aucs: 0.44545301103118046 0.8733413383351514\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.2931 - custom_metric: 1.3188\n",
      "Epoch 8/1000\n",
      "456/457 [============================>.] - ETA: 0s - loss: 0.2594val_aucs: 0.45647423247507457 0.8706529482910018\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2595 - custom_metric: 1.3271\n",
      "Epoch 9/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2386val_aucs: 0.4540079832263762 0.8645340923260983\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.2386 - custom_metric: 1.3185\n",
      "Epoch 10/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.2137val_aucs: 0.43620234368745336 0.8639838574516714\n",
      "457/457 [==============================] - 22s 48ms/step - loss: 0.2137 - custom_metric: 1.3002\n",
      "Epoch 11/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1776val_aucs: 0.4094957850886446 0.8585687091629366\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1776 - custom_metric: 1.2681\n",
      "Epoch 12/1000\n",
      "457/457 [==============================] - ETA: 0s - loss: 0.1697val_aucs: 0.38991373818324665 0.8493777811495287\n",
      "457/457 [==============================] - 22s 47ms/step - loss: 0.1697 - custom_metric: 1.2393\n",
      "Test res 0.8883042499401628 0.4915209100348656 0.5082926829268293\n",
      "gen_res {40: [(0.883832121940821, 0.002709333262001629), (0.4758611161515708, 0.0076527492004964184), (0.4868268016065146, 0.008907013242096244)]}\n"
     ]
    }
   ],
   "source": [
    "repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "lds = [40]\n",
    "batch_size, lr, patience = 32, 0.0005, 10\n",
    "d, N, he, dropout = 50,2,4,0.2\n",
    "fore_savepath = 'Exp0/models/forecasting/forecasting_50_epochs.h5'\n",
    "f = open('Exp0/logs/log_text_90.csv', 'a+')\n",
    "f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "train_inds = np.arange(len(train_op))\n",
    "valid_inds = np.arange(len(valid_op))\n",
    "gen_res = {}\n",
    "\n",
    "np.random.seed(2021)\n",
    "for ld in lds:\n",
    "    np.random.shuffle(train_inds)\n",
    "    np.random.shuffle(valid_inds)\n",
    "    train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "    valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "    f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "    all_test_res = []\n",
    "    for i in range(repeats[ld]):\n",
    "        print ('Repeat', i, 'ld', ld)\n",
    "        # Get train and validation data.\n",
    "        curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "        curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "        curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "        curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "        curr_train_op = train_op[curr_train_ind]\n",
    "        curr_valid_op = valid_op[curr_valid_ind]\n",
    "        print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "        # Construct save_path.\n",
    "        savepath = 'Exp0/models/classification/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "        print (savepath)\n",
    "        # Build and compile model.\n",
    "        model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "        model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "        fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "        # Load pretrained weights here.\n",
    "        fore_model.load_weights(fore_savepath)\n",
    "        # Train model.\n",
    "        es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "                           restore_best_weights=True)\n",
    "        cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "        his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "                        verbose=1, callbacks=[cus, es]).history\n",
    "        model.save_weights(savepath)\n",
    "        # Test and write to log.\n",
    "        rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "        f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "        print ('Test res', rocauc, prauc, minrp)\n",
    "        all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "    gen_res[ld] = []\n",
    "    for i in range(len(all_test_res[0])):\n",
    "        nums = [test_res[i] for test_res in all_test_res]\n",
    "        gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "    print ('gen_res', gen_res)\n",
    "f.close()\n",
    "\n",
    "# # save to local\n",
    "# log_path = '/content/log.csv'\n",
    "# files.download(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "# lds = [40]\n",
    "# batch_size, lr, patience = 32, 0.0005, 10\n",
    "# d, N, he, dropout = 50,2,4,0.2\n",
    "# fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_text_5.h5'\n",
    "# f = open('log_text_40.csv', 'a+')\n",
    "# f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "# train_inds = np.arange(len(train_op))\n",
    "# valid_inds = np.arange(len(valid_op))\n",
    "# gen_res = {}\n",
    "\n",
    "# np.random.seed(2021)\n",
    "# for ld in lds:\n",
    "#     np.random.shuffle(train_inds)\n",
    "#     np.random.shuffle(valid_inds)\n",
    "#     train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "#     valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "#     f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "#     all_test_res = []\n",
    "#     for i in range(repeats[ld]):\n",
    "#         print ('Repeat', i, 'ld', ld)\n",
    "#         # Get train and validation data.\n",
    "#         curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "#         curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "#         curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "#         curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "#         curr_train_op = train_op[curr_train_ind]\n",
    "#         curr_valid_op = valid_op[curr_valid_ind]\n",
    "#         print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "#         # Construct save_path.\n",
    "#         savepath = 'new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "#         print (savepath)\n",
    "#         # Build and compile model.\n",
    "#         model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "#         model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "#         fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "#         # Load pretrained weights here.\n",
    "#         fore_model.load_weights(fore_savepath)\n",
    "#         # Train model.\n",
    "#         es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "#                            restore_best_weights=True)\n",
    "#         cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "#         his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "#                         verbose=1, callbacks=[cus, es]).history\n",
    "#         model.save_weights(savepath)\n",
    "#         # Test and write to log.\n",
    "#         rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "#         f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "#         print ('Test res', rocauc, prauc, minrp)\n",
    "#         all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "#     gen_res[ld] = []\n",
    "#     for i in range(len(all_test_res[0])):\n",
    "#         nums = [test_res[i] for test_res in all_test_res]\n",
    "#         gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "#     print ('gen_res', gen_res)\n",
    "# f.close()\n",
    "\n",
    "# # # save to local\n",
    "# # log_path = '/content/log.csv'\n",
    "# # files.download(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "# lds = [50]\n",
    "# batch_size, lr, patience = 32, 0.0005, 10\n",
    "# d, N, he, dropout = 50,2,4,0.2\n",
    "# fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_text_5.h5'\n",
    "# f = open('log_text_50.csv', 'a+')\n",
    "# f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "# train_inds = np.arange(len(train_op))\n",
    "# valid_inds = np.arange(len(valid_op))\n",
    "# gen_res = {}\n",
    "\n",
    "# np.random.seed(2021)\n",
    "# for ld in lds:\n",
    "#     np.random.shuffle(train_inds)\n",
    "#     np.random.shuffle(valid_inds)\n",
    "#     train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "#     valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "#     f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "#     all_test_res = []\n",
    "#     for i in range(repeats[ld]):\n",
    "#         print ('Repeat', i, 'ld', ld)\n",
    "#         # Get train and validation data.\n",
    "#         curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "#         curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "#         curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "#         curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "#         curr_train_op = train_op[curr_train_ind]\n",
    "#         curr_valid_op = valid_op[curr_valid_ind]\n",
    "#         print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "#         # Construct save_path.\n",
    "#         savepath = 'new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "#         print (savepath)\n",
    "#         # Build and compile model.\n",
    "#         model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "#         model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "#         fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "#         # Load pretrained weights here.\n",
    "#         fore_model.load_weights(fore_savepath)\n",
    "#         # Train model.\n",
    "#         es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "#                            restore_best_weights=True)\n",
    "#         cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "#         his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "#                         verbose=1, callbacks=[cus, es]).history\n",
    "#         model.save_weights(savepath)\n",
    "#         # Test and write to log.\n",
    "#         rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "#         f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "#         print ('Test res', rocauc, prauc, minrp)\n",
    "#         all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "#     gen_res[ld] = []\n",
    "#     for i in range(len(all_test_res[0])):\n",
    "#         nums = [test_res[i] for test_res in all_test_res]\n",
    "#         gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "#     print ('gen_res', gen_res)\n",
    "# f.close()\n",
    "\n",
    "# # # save to local\n",
    "# # log_path = '/content/log.csv'\n",
    "# # files.download(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "# lds = [90]\n",
    "# batch_size, lr, patience = 32, 0.0005, 10\n",
    "# d, N, he, dropout = 50,2,4,0.2\n",
    "# fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_text_5.h5'\n",
    "# f = open('log_text_90.csv', 'a+')\n",
    "# f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "# train_inds = np.arange(len(train_op))\n",
    "# valid_inds = np.arange(len(valid_op))\n",
    "# gen_res = {}\n",
    "\n",
    "# np.random.seed(2021)\n",
    "# for ld in lds:\n",
    "#     np.random.shuffle(train_inds)\n",
    "#     np.random.shuffle(valid_inds)\n",
    "#     train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "#     valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "#     f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "#     all_test_res = []\n",
    "#     for i in range(repeats[ld]):\n",
    "#         print ('Repeat', i, 'ld', ld)\n",
    "#         # Get train and validation data.\n",
    "#         curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "#         curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "#         curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "#         curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "#         curr_train_op = train_op[curr_train_ind]\n",
    "#         curr_valid_op = valid_op[curr_valid_ind]\n",
    "#         print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "#         # Construct save_path.\n",
    "#         savepath = 'new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "#         print (savepath)\n",
    "#         # Build and compile model.\n",
    "#         model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "#         model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "#         fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "#         # Load pretrained weights here.\n",
    "#         fore_model.load_weights(fore_savepath)\n",
    "#         # Train model.\n",
    "#         es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "#                            restore_best_weights=True)\n",
    "#         cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "#         his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "#                         verbose=1, callbacks=[cus, es]).history\n",
    "#         model.save_weights(savepath)\n",
    "#         # Test and write to log.\n",
    "#         rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "#         f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "#         print ('Test res', rocauc, prauc, minrp)\n",
    "#         all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "#     gen_res[ld] = []\n",
    "#     for i in range(len(all_test_res[0])):\n",
    "#         nums = [test_res[i] for test_res in all_test_res]\n",
    "#         gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "#     print ('gen_res', gen_res)\n",
    "# f.close()\n",
    "\n",
    "# # # save to local\n",
    "# # log_path = '/content/log.csv'\n",
    "# # files.download(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "# lds = [100]\n",
    "# batch_size, lr, patience = 32, 0.0005, 10\n",
    "# d, N, he, dropout = 50,2,4,0.2\n",
    "# fore_savepath = 'mimic_iii_24h_strats_no_interp_with_ss_fore_text_5.h5'\n",
    "# f = open('log_text_100.csv', 'a+')\n",
    "# f.write('\\nTraining on different % of labeled data\\n')\n",
    "\n",
    "# train_inds = np.arange(len(train_op))\n",
    "# valid_inds = np.arange(len(valid_op))\n",
    "# gen_res = {}\n",
    "\n",
    "# np.random.seed(2021)\n",
    "# for ld in lds:\n",
    "#     np.random.shuffle(train_inds)\n",
    "#     np.random.shuffle(valid_inds)\n",
    "#     train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "#     valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "#     f.write('Training on '+str(ld)+' % of labaled data+\\n'+'val_metric,roc_auc,pr_auc,min_rp,savepath\\n')\n",
    "#     all_test_res = []\n",
    "#     for i in range(repeats[ld]):\n",
    "#         print ('Repeat', i, 'ld', ld)\n",
    "#         # Get train and validation data.\n",
    "#         curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "#         curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "#         curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "#         curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "#         curr_train_op = train_op[curr_train_ind]\n",
    "#         curr_valid_op = valid_op[curr_valid_ind]\n",
    "#         print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "#         # Construct save_path.\n",
    "#         savepath = 'new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "#         print (savepath)\n",
    "#         # Build and compile model.\n",
    "#         model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "#         model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "#         fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "#         # Load pretrained weights here.\n",
    "#         fore_model.load_weights(fore_savepath)\n",
    "#         # Train model.\n",
    "#         es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "#                            restore_best_weights=True)\n",
    "#         cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "#         his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "#                         verbose=1, callbacks=[cus, es]).history\n",
    "#         model.save_weights(savepath)\n",
    "#         # Test and write to log.\n",
    "#         rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "#         f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "#         print ('Test res', rocauc, prauc, minrp)\n",
    "#         all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "#     gen_res[ld] = []\n",
    "#     for i in range(len(all_test_res[0])):\n",
    "#         nums = [test_res[i] for test_res in all_test_res]\n",
    "#         gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "#     print ('gen_res', gen_res)\n",
    "# f.close()\n",
    "\n",
    "# # # save to local\n",
    "# # log_path = '/content/log.csv'\n",
    "# # files.download(log_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
