{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6a8d57-4fbb-4783-8a15-991e090bee29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 27 15:28:32 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   36C    P0              39W / 300W |      9MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           On  | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              39W / 300W |      9MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2520      G   /usr/libexec/Xorg                             8MiB |\n",
      "|    1   N/A  N/A      2520      G   /usr/libexec/Xorg                             8MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# gpu check\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "910e34b6-f416-4a46-ab35-a047cfe17c27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis\n"
     ]
    }
   ],
   "source": [
    "cd /pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76443227-3cf6-4379-b21d-6aec2443738c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-27 15:28:35.213633: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-27 15:28:35.805326: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-27 15:29:00.757146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Lambda\n",
    "# from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, pipeline, AutoModel\n",
    "import resources.smart_cond as sc\n",
    "# from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "174f33fd-69b1-4d29-ad43-b9ae694b4f29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = 'Data/sepsis_removed_0.pkl'\n",
    "pkl = pickle.load(open(data_path, 'rb'))\n",
    "data = pkl[0]\n",
    "oc = pkl[1]\n",
    "train_ind = pkl[2]\n",
    "valid_ind = pkl[3]\n",
    "test_ind = pkl[4]\n",
    "del pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9de00119-48bb-4706-ab2f-d501afc56380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.loc[data['variable'] == 'Text', 'value'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f927205b-b7a0-45c1-b292-7b5a39ea4e93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22938it [00:00, 789614.05it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_window = 2 # hours\n",
    "obs_windows = range(20, 124, 4)\n",
    "\n",
    "# Remove train, val patients\n",
    "data = data.merge(oc[['ts_ind', 'SUBJECT_ID']], on='ts_ind', how='left')\n",
    "train_sub = oc.loc[oc.ts_ind.isin(train_ind)].SUBJECT_ID.unique()\n",
    "valid_sub = oc.loc[oc.ts_ind.isin(valid_ind)].SUBJECT_ID.unique()\n",
    "data = data.loc[~data.SUBJECT_ID.isin(train_sub)]\n",
    "data = data.loc[~data.SUBJECT_ID.isin(valid_sub)]\n",
    "oc = oc.loc[~oc.SUBJECT_ID.isin(train_sub)]\n",
    "oc = oc.loc[~oc.SUBJECT_ID.isin(valid_sub)]\n",
    "\n",
    "data.drop(columns=['SUBJECT_ID', 'TABLE'], inplace=True)\n",
    "# Fix age.\n",
    "data.loc[(data.variable=='Age')&(data.value>200), 'value'] = 91.4\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "N = data.ts_ind.max()+1\n",
    "demo = np.zeros((N, D))\n",
    "for row in tqdm(static_data.itertuples()):\n",
    "    demo[row.ts_ind, static_var_to_ind[row.variable]] = row.value\n",
    "# Normalize static data.\n",
    "means = demo.mean(axis=0, keepdims=True)\n",
    "stds = demo.std(axis=0, keepdims=True)\n",
    "stds = (stds==0)*1 + (stds!=0)*stds\n",
    "demo = (demo-means)/stds\n",
    "\n",
    "# Get variable indices.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Find max_len.\n",
    "fore_max_len = 880\n",
    "# Get forecast inputs and outputs.\n",
    "fore_times_ip = []\n",
    "fore_values_ip = []\n",
    "fore_varis_ip = []\n",
    "fore_op = []\n",
    "fore_inds = []\n",
    "def f(x):\n",
    "    mask = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    for vv in x:\n",
    "        v = int(vv[0])-1\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]\n",
    "    return values+mask\n",
    "def pad(x):\n",
    "    return x+[0]*(fore_max_len-len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c575424-240a-46a2-9d93-284b5c3d7045",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [01:10<00:00,  2.71s/it]\n"
     ]
    }
   ],
   "source": [
    "for w in tqdm(obs_windows):\n",
    "    pred_data = data.loc[(data.hour>=w)&(data.hour<=w+pred_window)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data['vind_value'].apply(f)    \n",
    "    obs_data = data.loc[(data.hour<w)&(data.hour>=w-24)]\n",
    "    obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "    obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "    obs_data = obs_data.groupby('ts_ind').agg({'vind':list, 'hour':list, 'value':list}).reset_index()\n",
    "    obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "    for col in ['vind', 'hour', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "    fore_op.append(np.array(list(obs_data.vind_value)))\n",
    "    fore_inds.append(np.array(list(obs_data.ts_ind)))\n",
    "    fore_times_ip.append(np.array(list(obs_data.hour)))\n",
    "    fore_values_ip.append(np.array(list(obs_data.value)))\n",
    "    fore_varis_ip.append(np.array(list(obs_data.vind)))\n",
    "\n",
    "del data\n",
    "fore_times_ip = np.concatenate(fore_times_ip, axis=0)\n",
    "fore_values_ip = np.concatenate(fore_values_ip, axis=0)\n",
    "fore_varis_ip = np.concatenate(fore_varis_ip, axis=0)\n",
    "fore_op = np.concatenate(fore_op, axis=0)\n",
    "fore_inds = np.concatenate(fore_inds, axis=0)\n",
    "fore_demo = demo[fore_inds]\n",
    "\n",
    "fore_test_ip = [fore_demo, fore_times_ip, fore_values_ip, fore_varis_ip]\n",
    "fore_test_op = fore_op\n",
    "# release RAM\n",
    "del fore_times_ip, fore_values_ip, fore_varis_ip, demo, fore_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab28b29b-5f88-465d-9ddd-59b2467a5ea8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# laod text features\n",
    "path = 'Data/text_emb_input_test_1.pkl'\n",
    "text_ip = pickle.load(open(path, 'rb'))\n",
    "text_features = text_ip[0]\n",
    "fore_test_ip.append(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bd5ef50-3670-4d1b-a449-6255e00e4401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131920/131920 [00:38<00:00, 3443.15it/s]\n",
      "/scratch/slurm_tmpdir/job_23194023/ipykernel_31096/41647890.py:18: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_text_times = np.array(test_text_times)\n",
      "/scratch/slurm_tmpdir/job_23194023/ipykernel_31096/41647890.py:19: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  test_text_varis = np.array(test_text_varis)\n"
     ]
    }
   ],
   "source": [
    "# valid text times\n",
    "test_text_times = []\n",
    "test_text_varis = []\n",
    "\n",
    "test_times = fore_test_ip[1]\n",
    "test_varis = fore_test_ip[3]\n",
    "\n",
    "for i in tqdm(range(len(fore_test_ip[0]))):\n",
    "    times = []\n",
    "    varis = []\n",
    "    for j in range(880):\n",
    "        if test_varis[i][j] == 124:\n",
    "            times.append(test_times[i][j])\n",
    "            varis.append(135)\n",
    "    test_text_times.append(np.array(times))\n",
    "    test_text_varis.append(np.array(varis))\n",
    "\n",
    "test_text_times = np.array(test_text_times)\n",
    "test_text_varis = np.array(test_text_varis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edd717ec-6e10-4cb5-b654-ffed39b0fb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "# padding\n",
    "padded_test_text_times = pad_sequences(test_text_times, maxlen=50, padding='post', dtype='float32')\n",
    "padded_test_text_varis = pad_sequences(test_text_varis, maxlen=50, padding='post')\n",
    "\n",
    "del test_text_times, test_text_varis, test_times, test_varis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f68f78c3-1022-4344-a291-ea140558dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "fore_test_ip.append(padded_test_text_times)\n",
    "fore_test_ip.append(padded_test_text_varis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9f1749b-2d99-49bd-ae5b-d3ca0732e29c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_res(y_true, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    minrp = np.minimum(precision, recall).max()\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return [roc_auc, pr_auc, minrp]\n",
    "\n",
    "# ######################################################################################################## \n",
    "# ######################################################################################################## \n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=train_op)\n",
    "# def mortality_loss(y_true, y_pred):\n",
    "#     sample_weights = (1-y_true)*class_weights[0] + y_true*class_weights[1]\n",
    "#     bce = K.binary_crossentropy(y_true, y_pred)\n",
    "#     return K.mean(sample_weights*bce, axis=-1)\n",
    "# ######################################################################################################## \n",
    "# ######################################################################################################## \n",
    "\n",
    "# var_weights = np.sum(fore_train_op[:, V:], axis=0)\n",
    "# var_weights[var_weights==0] = var_weights.max()\n",
    "# var_weights = var_weights.max()/var_weights\n",
    "# var_weights = var_weights.reshape((1, V))\n",
    "def forecast_loss(y_true, y_pred):\n",
    "    return K.sum(y_true[:,V:]*(y_true[:,:V]-y_pred)**2, axis=-1)\n",
    "\n",
    "def get_min_loss(weight):\n",
    "    def min_loss(y_true, y_pred):\n",
    "        return weight*y_pred\n",
    "    return min_loss\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, validation_data, batch_size):\n",
    "        self.val_x, self.val_y = validation_data\n",
    "        self.batch_size = batch_size\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.val_x, verbose=0, batch_size=self.batch_size)\n",
    "        if type(y_pred)==type([]):\n",
    "            y_pred = y_pred[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(self.val_y, y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(self.val_y, y_pred)\n",
    "        logs['custom_metric'] = pr_auc + roc_auc\n",
    "        print ('val_aucs:', pr_auc, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cde35606-16f3-4042-9f9e-5af8aa4730a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Activation, Dropout, Softmax, Layer, InputSpec, Input, Dense, Lambda, TimeDistributed, Concatenate, Add\n",
    "from tensorflow.keras import initializers, regularizers, constraints, Model\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow import nn\n",
    "\n",
    "    \n",
    "class CVE(Layer):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        super(CVE, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        self.W1 = self.add_weight(name='CVE_W1',\n",
    "                            shape=(1, self.hid_units),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        self.b1 = self.add_weight(name='CVE_b1',\n",
    "                            shape=(self.hid_units,),\n",
    "                            initializer='zeros',\n",
    "                            trainable=True)\n",
    "        self.W2 = self.add_weight(name='CVE_W2',\n",
    "                            shape=(self.hid_units, self.output_dim),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        super(CVE, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = K.expand_dims(x, axis=-1)\n",
    "        x = K.dot(K.tanh(K.bias_add(K.dot(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.output_dim,)\n",
    "    \n",
    "    \n",
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, hid_dim):\n",
    "        self.hid_dim = hid_dim\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        self.W = self.add_weight(shape=(d, self.hid_dim), name='Att_W',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.hid_dim,), name='Att_b',\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(shape=(self.hid_dim,1), name='Att_u',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = K.dot(K.tanh(K.bias_add(K.dot(x,self.W), self.b)), self.u)\n",
    "        mask = K.expand_dims(mask, axis=-1)\n",
    "        attn_weights = mask*attn_weights + (1-mask)*mask_value\n",
    "        attn_weights = K.softmax(attn_weights, axis=-2)\n",
    "        return attn_weights\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (1,)\n",
    "    \n",
    "    \n",
    "class Transformer(Layer):\n",
    "    \n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0):\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = K.epsilon() * K.epsilon()\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        if self.dk==None:\n",
    "            self.dk = d//self.h\n",
    "        if self.dv==None:\n",
    "            self.dv = d//self.h\n",
    "        if self.dff==None:\n",
    "            self.dff = 2*d\n",
    "        self.Wq = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wq',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wk = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wk',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wv = self.add_weight(shape=(self.N, self.h, d, self.dv), name='Wv',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wo = self.add_weight(shape=(self.N, self.dv*self.h, d), name='Wo',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.W1 = self.add_weight(shape=(self.N, d, self.dff), name='W1',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b1 = self.add_weight(shape=(self.N, self.dff), name='b1',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.W2 = self.add_weight(shape=(self.N, self.dff, d), name='W2',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b2 = self.add_weight(shape=(self.N, d), name='b2',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.gamma = self.add_weight(shape=(2*self.N,), name='gamma',\n",
    "                                 initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(shape=(2*self.N,), name='beta',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Transformer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e-30):\n",
    "        mask = K.expand_dims(mask, axis=-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = K.dot(x, self.Wq[i,j,:,:])\n",
    "                k = K.permute_dimensions(K.dot(x, self.Wk[i,j,:,:]), (0,2,1))\n",
    "                v = K.dot(x, self.Wv[i,j,:,:])\n",
    "                A = K.batch_dot(q,k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask*A + (1-mask)*mask_value\n",
    "                # Mask for attention dropout.\n",
    "                def dropped_A():\n",
    "                    dp_mask = K.cast((K.random_uniform(shape=array_ops.shape(A))>=self.dropout), K.floatx())\n",
    "                    return A*dp_mask + (1-dp_mask)*mask_value\n",
    "                A = sc.smart_cond(K.learning_phase(), dropped_A, lambda: array_ops.identity(A))\n",
    "                A = K.softmax(A, axis=-1)\n",
    "                mha_ops.append(K.batch_dot(A,v))\n",
    "            conc = K.concatenate(mha_ops, axis=-1)\n",
    "            proj = K.dot(conc, self.Wo[i,:,:])\n",
    "            # Dropout.\n",
    "            proj = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(proj, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(proj))\n",
    "            # Add & LN\n",
    "            x = x+proj\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i] + self.beta[2*i]\n",
    "            # FFN\n",
    "            ffn_op = K.bias_add(K.dot(K.relu(K.bias_add(K.dot(x, self.W1[i,:,:]), self.b1[i,:])), \n",
    "                           self.W2[i,:,:]), self.b2[i,:,])\n",
    "            # Dropout.\n",
    "            ffn_op = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(ffn_op, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(ffn_op))\n",
    "            # Add & LN\n",
    "            x = x+ffn_op\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i+1] + self.beta[2*i+1]            \n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def build_strats(D, max_len, V, d, N, he, dropout, forecast=False):\n",
    "    # demo\n",
    "    demo = Input(shape=(D,))\n",
    "    demo_enc = Dense(2*d, activation='tanh')(demo)\n",
    "    demo_enc = Dense(d, activation='tanh')(demo_enc)\n",
    "    \n",
    "    ## text\n",
    "    # text\n",
    "    texts = Input(shape=(33792,))\n",
    "    text_enc = Dense(1000, activation='relu')(texts)\n",
    "    text_enc = Dense(d, activation='relu')(text_enc)\n",
    "    \n",
    "    # text time\n",
    "    text_times = Input(shape=(50,))\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    text_times_emb = CVE(cve_units, d)(text_times)\n",
    "    \n",
    "    # text varis\n",
    "    text_varis = Input(shape=(50,))\n",
    "    text_varis_emb = Embedding(V+1, d)(text_varis)\n",
    "    \n",
    "    text_comb_emb = Add()([text_varis_emb, text_enc, text_times_emb])\n",
    "    text_mask = Lambda(lambda x:K.clip(x,0,1))(text_varis)\n",
    "    text_cont_emb = Transformer(N, he, dk=None, dv=None, dff=None, dropout=dropout)(text_comb_emb, mask=text_mask)\n",
    "    text_attn_weights = Attention(2*d)(text_cont_emb, mask=text_mask)\n",
    "    text_fused_emb = Lambda(lambda x:K.sum(x[0]*x[1], axis=-2))([text_cont_emb, text_attn_weights])\n",
    "    \n",
    "    \n",
    "    ## physio\n",
    "    # triplet\n",
    "    varis = Input(shape=(max_len,))\n",
    "    values = Input(shape=(max_len,))\n",
    "    times = Input(shape=(max_len,))\n",
    "    \n",
    "    \n",
    "    varis_emb = Embedding(V+1, d)(varis)\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    values_emb = CVE(cve_units, d)(values)\n",
    "    times_emb = CVE(cve_units, d)(times)\n",
    "    \n",
    "    \n",
    "    # comb_emb = Add()([varis_emb, values_emb, times_emb]) # b, L, d\n",
    "    comb_emb = Add()([varis_emb, values_emb, times_emb]) # b, L, d\n",
    "#     demo_enc = Lambda(lambda x:K.expand_dims(x, axis=-2))(demo_enc) # b, 1, d\n",
    "#     comb_emb = Concatenate(axis=-2)([demo_enc, comb_emb]) # b, L+1, d\n",
    "    mask = Lambda(lambda x:K.clip(x,0,1))(varis) # b, L\n",
    "#     mask = Lambda(lambda x:K.concatenate((K.ones_like(x)[:,0:1], x), axis=-1))(mask) # b, L+1\n",
    "    cont_emb = Transformer(N, he, dk=None, dv=None, dff=None, dropout=dropout)(comb_emb, mask=mask)\n",
    "    attn_weights = Attention(2*d)(cont_emb, mask=mask)\n",
    "    fused_emb = Lambda(lambda x:K.sum(x[0]*x[1], axis=-2))([cont_emb, attn_weights])\n",
    "    \n",
    "#     # embed text input\n",
    "#     texts = Input(shape=(33792,))\n",
    "#     text_enc = Dense(1000, activation='relu')(texts)\n",
    "#     text_enc = Dense(d, activation='relu')(text_enc)\n",
    "\n",
    "\n",
    "    conc = Concatenate(axis=-1)([fused_emb, text_fused_emb, demo_enc])\n",
    "    \n",
    "    \n",
    "    fore_op = Dense(V)(conc)\n",
    "    op = Dense(1, activation='sigmoid')(fore_op)\n",
    "    model = Model([demo, times, values, varis, texts, text_times, text_varis], op)\n",
    "    if forecast:\n",
    "        fore_model = Model([demo, times, values, varis, texts, text_times, text_varis], fore_op)\n",
    "        return [model, fore_model]\n",
    "    return model\n",
    "\n",
    "# To tune:\n",
    "# 1. Transformer parameters. (N, h, dropout)\n",
    "# 2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ed0d61-eaf4-4b3e-a848-e29189d12a7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # quick eva\n",
    "# lr, batch_size, samples_per_epoch, patience = 0.0005, 1, 102400, 5\n",
    "# # lr, batch_size, samples_per_epoch, patience = 0.0005, 1, 1024, 5\n",
    "# d, N, he, dropout = 50, 2, 4, 0.2\n",
    "# model, fore_model =  build_strats(D, fore_max_len, V, d, N, he, dropout, forecast=True)\n",
    "# # print (fore_model.summary())\n",
    "\n",
    "# fore_path = 'Exp_post_koll/exp_0/models/forecasting/forecasting_104_epochs.h5'\n",
    "\n",
    "# fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "# fore_model.load_weights(fore_path)\n",
    "\n",
    "# val_loss = fore_model.evaluate(fore_test_ip, fore_op, batch_size=batch_size, verbose=1)\n",
    "# print(f'Test loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "937fc818-c7bd-4c6d-b810-a5daaeabd936",
   "metadata": {},
   "source": [
    "131920/131920 [==============================] - 564s 4ms/step - loss: 5.3401\n",
    "Test loss: 5.340090751647949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d279a246-7ccf-4262-b1a6-dd5da9cb0b7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # quick eva\n",
    "# lr, batch_size, samples_per_epoch, patience = 0.0005, 1, 102400, 5\n",
    "# # lr, batch_size, samples_per_epoch, patience = 0.0005, 1, 1024, 5\n",
    "# d, N, he, dropout = 50, 2, 4, 0.2\n",
    "# model, fore_model =  build_strats(D, fore_max_len, V, d, N, he, dropout, forecast=True)\n",
    "# # print (fore_model.summary())\n",
    "\n",
    "# fore_path = 'Exp_post_koll/exp_0/models/forecasting/forecasting_66_epochs.h5'\n",
    "\n",
    "# fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "# fore_model.load_weights(fore_path)\n",
    "\n",
    "# val_loss = fore_model.evaluate(fore_test_ip, fore_op, batch_size=batch_size, verbose=1)\n",
    "# print(f'Test loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1aaaf830-a7d1-4c25-86ab-96f73d46f5a2",
   "metadata": {},
   "source": [
    "131920/131920 [==============================] - 567s 4ms/step - loss: 5.3262\n",
    "Test loss: 5.326155185699463"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7873113f-4005-4c55-a45e-03d64b6a15d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # quick eva\n",
    "# lr, batch_size, samples_per_epoch, patience = 0.0005, 1, 102400, 5\n",
    "# # lr, batch_size, samples_per_epoch, patience = 0.0005, 1, 1024, 5\n",
    "# d, N, he, dropout = 50, 2, 4, 0.2\n",
    "# model, fore_model =  build_strats(D, fore_max_len, V, d, N, he, dropout, forecast=True)\n",
    "# # print (fore_model.summary())\n",
    "\n",
    "# fore_path = 'Exp_post_koll/exp_0/models/forecasting/forecasting_137_epochs.h5'\n",
    "\n",
    "# fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "# fore_model.load_weights(fore_path)\n",
    "\n",
    "# val_loss = fore_model.evaluate(fore_test_ip, fore_op, batch_size=batch_size, verbose=1)\n",
    "# print(f'Test loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c39ef38-5d6a-465b-b99f-738a2e1e149c",
   "metadata": {},
   "source": [
    "131920/131920 [==============================] - 550s 4ms/step - loss: 5.3197\n",
    "Test loss: 5.3197197914123535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a3c8212-673f-4559-b4bb-67a960638bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr, batch_size, samples_per_epoch, patience = 0.0005, 1, 102400, 5\n",
    "# # lr, batch_size, samples_per_epoch, patience = 0.0005, 1, 1024, 5\n",
    "# d, N, he, dropout = 50, 2, 4, 0.2\n",
    "# model, fore_model =  build_strats(D, fore_max_len, V, d, N, he, dropout, forecast=True)\n",
    "# # print (fore_model.summary())\n",
    "\n",
    "# fore_path = 'Exp_post_koll/exp_0/models/forecasting/forecasting_126_epochs.h5'\n",
    "\n",
    "# fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "# fore_model.load_weights(fore_path)\n",
    "\n",
    "# val_loss = fore_model.evaluate(fore_test_ip, fore_op, batch_size=batch_size, verbose=1)\n",
    "# print(f'Test loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3110756e-8cd3-4775-8ea3-e1a108c00d90",
   "metadata": {},
   "source": [
    "131920/131920 [==============================] - 541s 4ms/step - loss: 5.3223\n",
    "Test loss: 5.322259902954102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951132b-dafe-43a9-91e1-474c2e23eda9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hours = []\n",
    "# max_hours = []\n",
    "# # get hours\n",
    "# for time in fore_test_ip[1]:\n",
    "#   hour = max(time)\n",
    "#   max_hours.append(hour)\n",
    "  \n",
    "#   for obs_window in obs_windows:\n",
    "#     if hour < obs_window:\n",
    "#       hour = obs_window\n",
    "#       break\n",
    "  \n",
    "#   hours.append(hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108eecd-f5e4-47b6-ae7d-9178e20e9019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get patient ids\n",
    "# test_patient_ids = []\n",
    "# test_sepsis_labels = []\n",
    "\n",
    "# # sub_ids = oc['SUBJECT_ID'].tolist()\n",
    "# # ts_ind = oc['ts_ind'].tolist()\n",
    "# # sepsis = oc['in_hospital_sepsis'].tolist()\n",
    "\n",
    "# # for ind in val_inds:\n",
    "# #    for i in range(len(sub_ids)):\n",
    "# #      if ts_ind[i] == ind:\n",
    "# #        val_sepsis_labels.append(sepsis[i])\n",
    "# #        val_patient_ids.append(sub_ids[i])\n",
    "# #        break\n",
    "\n",
    "# for ind in fore_inds:\n",
    "#   test_sepsis_labels.append(oc[oc['ts_ind']==ind]['in_hospital_sepsis'].item())\n",
    "#   test_patient_ids.append(oc[oc['ts_ind']==ind]['SUBJECT_ID'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2891737-88ee-49db-baed-121c671c797e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_data = pd.DataFrame(\n",
    "#     {'ts_ind': fore_inds,\n",
    "#      'obs_window': hours,\n",
    "#      'SUBJECT_ID': test_patient_ids,\n",
    "#      'sepsis_label': test_sepsis_labels,\n",
    "#      'forecasting_pred': pd.Series(test_y_preds.tolist()),\n",
    "#      'forecasting_test_op': pd.Series(fore_op.tolist())\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0eaed0-cdc8-4c43-add4-e46c21b0b97a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9631e8-a354-4720-ade7-c0a54ca4aee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # dump to pkl\n",
    "# pickle.dump([test_data, var_to_ind], open('randomization_test/data/post_koll_exp_0.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b72b64-ff2d-4118-9163-7a77f0f901a0",
   "metadata": {},
   "source": [
    "## Eva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0e0ba-4b26-47e7-beb9-78d7a8923f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow.keras.backend as K\n",
    "# def forecast_loss(y_true, y_pred):\n",
    "#     V=134\n",
    "#     return K.sum(y_true[:,V:]*(y_true[:,:V]-y_pred)**2, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d07b8-6de4-4470-a645-a2b180595093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = []\n",
    "# for y in test_data['forecasting_test_op']:\n",
    "#   y_true.append(y)\n",
    "# y_true = np.array(y_true)\n",
    "# # y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec11ee-a451-4965-af1a-ee681a2a3dde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# y_pred = []\n",
    "# for y in test_data['forecasting_pred']:\n",
    "#   y_pred.append(y)\n",
    "# y_pred = np.array(y_pred)\n",
    "# # y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ead362-2bbd-474a-90d2-57f19351a008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mmse = forecast_loss(y_true, y_pred)\n",
    "# s = 0\n",
    "# for i in mmse:\n",
    "#   s += i\n",
    "# s/len(mmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6abdc2-b603-4779-a894-4072ce2fcadc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mmse = forecast_loss(y_true, y_pred)\n",
    "# s = 0\n",
    "# for i in mmse:\n",
    "#   s += i\n",
    "# s/len(mmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
