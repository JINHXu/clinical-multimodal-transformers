{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECKAJiLXiTlN"
   },
   "source": [
    "## Hardware check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e7odgm5VsOsb",
    "outputId": "cd4abc44-f4d3-43d2-fffb-8d38304e09f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct  3 23:52:15 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    39W / 300W |      9MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    39W / 300W |      9MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1772      G   /usr/libexec/Xorg                   8MiB |\n",
      "|    1   N/A  N/A      1772      G   /usr/libexec/Xorg                   8MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# gpu check\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oEEUeIW3sOsc",
    "outputId": "2a9ba788-fe62-482d-fab1-6490901982af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of cores\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() \n",
    "cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.7\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY9I5eXWeOom"
   },
   "source": [
    "## Environment Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis\n"
     ]
    }
   ],
   "source": [
    "cd /pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RpTg0QzPqqKv",
    "outputId": "59a45a87-7972-422f-9a3b-f6d40ae0fabb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 22:32:31.973951: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-20 22:32:32.746202: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-20 22:32:52.043042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylFHWoydK5S9"
   },
   "source": [
    "### Version Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AeG1PfTVsOsd",
    "outputId": "5bea12e5-90f1-463b-89f0-ab5e02d42208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf. __version__)\n",
    "\n",
    "pickle.format_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ax1p4f8cUI-s",
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/sepsis_removed_0.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/sepsis_removed_0.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m pkl \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m pkl[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m oc \u001b[38;5;241m=\u001b[39m pkl[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/bwhpc/common/jupyter/tensorflow/2023-03-24/lib/python3.9/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/sepsis_removed_0.pkl'"
     ]
    }
   ],
   "source": [
    "data_path = 'Data/sepsis_removed_0.pkl'\n",
    "pkl = pickle.load(open(data_path, 'rb'))\n",
    "data = pkl[0]\n",
    "oc = pkl[1]\n",
    "train_ind = pkl[2]\n",
    "valid_ind = pkl[3]\n",
    "test_ind = pkl[4]\n",
    "# embs = pkl[5]\n",
    "del pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = 'Data/CLS_emb_keywords_reserved.pkl'\n",
    "pkl = pickle.load(open(data_path, 'rb'))\n",
    "embs = pkl[0]\n",
    "del pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_data = data[data['variable'] == 'Text']\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embs_list = []\n",
    "\n",
    "for emb in embs:\n",
    "    embs_list.append(emb)\n",
    "\n",
    "text_data['value'] = embs_list\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "physio_data = data[data['variable'] != 'Text']\n",
    "physio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del data\n",
    "data = text_data.append(physio_data, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts_ind</th>\n",
       "      <th>hour</th>\n",
       "      <th>variable</th>\n",
       "      <th>value</th>\n",
       "      <th>TABLE</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10223</td>\n",
       "      <td>467.816667</td>\n",
       "      <td>Text</td>\n",
       "      <td>[0.14401615, 0.15158324, -0.3973308, 0.26409, ...</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18407</td>\n",
       "      <td>28.016667</td>\n",
       "      <td>Text</td>\n",
       "      <td>[0.10852944, 0.2838851, -0.3647646, 0.27892292...</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40300</td>\n",
       "      <td>155.166667</td>\n",
       "      <td>Text</td>\n",
       "      <td>[0.064612664, 0.20329593, -0.4460111, 0.203105...</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23747</td>\n",
       "      <td>52.383333</td>\n",
       "      <td>Text</td>\n",
       "      <td>[0.070009716, 0.33203533, -0.21765643, 0.15761...</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2357</td>\n",
       "      <td>73.133333</td>\n",
       "      <td>Text</td>\n",
       "      <td>[0.1063, 0.10509527, -0.35790482, 0.10100723, ...</td>\n",
       "      <td>noteevents</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886223</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>MBP</td>\n",
       "      <td>0.195381</td>\n",
       "      <td>chart</td>\n",
       "      <td>78.552377</td>\n",
       "      <td>17.645628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886224</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>O2 Saturation</td>\n",
       "      <td>-0.678068</td>\n",
       "      <td>chart</td>\n",
       "      <td>96.820961</td>\n",
       "      <td>4.160290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886225</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>RR</td>\n",
       "      <td>0.179866</td>\n",
       "      <td>chart</td>\n",
       "      <td>26.278501</td>\n",
       "      <td>15.130729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886226</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>SBP</td>\n",
       "      <td>-0.404061</td>\n",
       "      <td>chart</td>\n",
       "      <td>120.239648</td>\n",
       "      <td>25.341836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82886227</th>\n",
       "      <td>57281</td>\n",
       "      <td>20.400000</td>\n",
       "      <td>Urine</td>\n",
       "      <td>-0.24296</td>\n",
       "      <td>output</td>\n",
       "      <td>123.393012</td>\n",
       "      <td>137.442433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82447326 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ts_ind        hour       variable  \\\n",
       "0          10223  467.816667           Text   \n",
       "1          18407   28.016667           Text   \n",
       "2          40300  155.166667           Text   \n",
       "3          23747   52.383333           Text   \n",
       "4           2357   73.133333           Text   \n",
       "...          ...         ...            ...   \n",
       "82886223   57281   20.400000            MBP   \n",
       "82886224   57281   20.400000  O2 Saturation   \n",
       "82886225   57281   20.400000             RR   \n",
       "82886226   57281   20.400000            SBP   \n",
       "82886227   57281   20.400000          Urine   \n",
       "\n",
       "                                                      value       TABLE  \\\n",
       "0         [0.14401615, 0.15158324, -0.3973308, 0.26409, ...  noteevents   \n",
       "1         [0.10852944, 0.2838851, -0.3647646, 0.27892292...  noteevents   \n",
       "2         [0.064612664, 0.20329593, -0.4460111, 0.203105...  noteevents   \n",
       "3         [0.070009716, 0.33203533, -0.21765643, 0.15761...  noteevents   \n",
       "4         [0.1063, 0.10509527, -0.35790482, 0.10100723, ...  noteevents   \n",
       "...                                                     ...         ...   \n",
       "82886223                                           0.195381       chart   \n",
       "82886224                                          -0.678068       chart   \n",
       "82886225                                           0.179866       chart   \n",
       "82886226                                          -0.404061       chart   \n",
       "82886227                                           -0.24296      output   \n",
       "\n",
       "                mean         std  \n",
       "0           1.000000    1.000000  \n",
       "1           1.000000    1.000000  \n",
       "2           1.000000    1.000000  \n",
       "3           1.000000    1.000000  \n",
       "4           1.000000    1.000000  \n",
       "...              ...         ...  \n",
       "82886223   78.552377   17.645628  \n",
       "82886224   96.820961    4.160290  \n",
       "82886225   26.278501   15.130729  \n",
       "82886226  120.239648   25.341836  \n",
       "82886227  123.393012  137.442433  \n",
       "\n",
       "[82447326 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "M3Hpfnb2UI-s",
    "outputId": "c0f96396-fab4-40f9-9d39-0b0c85844d37",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      "  4%|▍         | 1/26 [00:21<09:05, 21.80s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      "  8%|▊         | 2/26 [00:44<08:57, 22.39s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 12%|█▏        | 3/26 [01:04<08:11, 21.35s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 15%|█▌        | 4/26 [01:23<07:24, 20.22s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 19%|█▉        | 5/26 [01:41<06:47, 19.40s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 23%|██▎       | 6/26 [01:58<06:11, 18.56s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 27%|██▋       | 7/26 [02:13<05:31, 17.43s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 31%|███       | 8/26 [02:27<04:54, 16.35s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 35%|███▍      | 9/26 [02:40<04:20, 15.34s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 38%|███▊      | 10/26 [02:52<03:48, 14.28s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 42%|████▏     | 11/26 [03:04<03:23, 13.54s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 46%|████▌     | 12/26 [03:15<02:59, 12.83s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 50%|█████     | 13/26 [03:25<02:35, 12.00s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 54%|█████▍    | 14/26 [03:35<02:16, 11.40s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 58%|█████▊    | 15/26 [03:44<01:57, 10.64s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 62%|██████▏   | 16/26 [03:53<01:41, 10.13s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 65%|██████▌   | 17/26 [04:01<01:25,  9.54s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 69%|██████▉   | 18/26 [04:09<01:12,  9.07s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 73%|███████▎  | 19/26 [04:17<01:00,  8.65s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 77%|███████▋  | 20/26 [04:24<00:49,  8.26s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 81%|████████  | 21/26 [04:31<00:40,  8.04s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 85%|████████▍ | 22/26 [04:38<00:30,  7.61s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 88%|████████▊ | 23/26 [04:44<00:21,  7.25s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 92%|█████████▏| 24/26 [04:51<00:14,  7.03s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      " 96%|█████████▌| 25/26 [04:57<00:06,  6.80s/it]/scratch/slurm_tmpdir/job_22677741/ipykernel_83883/2820743741.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  fore_texts_ip.append(np.array(obs_strings))\n",
      "100%|██████████| 26/26 [05:03<00:00, 11.66s/it]\n"
     ]
    }
   ],
   "source": [
    "pred_window = 2 # hours\n",
    "obs_windows = range(20, 124, 4)\n",
    "\n",
    "# Remove test patients.\n",
    "data = data.merge(oc[['ts_ind', 'SUBJECT_ID']], on='ts_ind', how='left')\n",
    "test_sub = oc.loc[oc.ts_ind.isin(test_ind)].SUBJECT_ID.unique()\n",
    "data = data.loc[~data.SUBJECT_ID.isin(test_sub)]\n",
    "oc = oc.loc[~oc.SUBJECT_ID.isin(test_sub)]\n",
    "data.drop(columns=['SUBJECT_ID', 'TABLE'], inplace=True)\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "N = data.ts_ind.max()+1\n",
    "\n",
    "# Get variable indices.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Find max_len.\n",
    "fore_max_len = 880\n",
    "# Get forecast inputs and outputs.\n",
    "fore_texts_ip = []\n",
    "fore_inds = []\n",
    "def f(x):\n",
    "    mask = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    for vv in x:\n",
    "        v = int(vv[0])-1\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]\n",
    "    return values+mask\n",
    "def pad(x):\n",
    "    return x+[0]*(fore_max_len-len(x))\n",
    "for w in tqdm(obs_windows):\n",
    "    pred_data = data.loc[(data.hour>=w)&(data.hour<=w+pred_window)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg({'value':'first'}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg({'vind_value':list}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data['vind_value'].apply(f)    \n",
    "    obs_data = data.loc[(data.hour<w)&(data.hour>=w-24)]\n",
    "    obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "    obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "    obs_data = obs_data.groupby('ts_ind').agg({'vind':list, 'hour':list, 'value':list}).reset_index()\n",
    "    obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "    for col in ['vind', 'hour', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "    fore_inds.append(np.array(list(obs_data.ts_ind)))\n",
    "    \n",
    "    matrix = list(obs_data.value)\n",
    "    obs_strings = []\n",
    "    for l in matrix:\n",
    "        string_list = []\n",
    "        for value in l:\n",
    "            if not np.isscalar(value):\n",
    "            # if isinstance(value, str):\n",
    "                string_list.append(value)\n",
    "        obs_strings.append(string_list)\n",
    "    del matrix\n",
    "    fore_texts_ip.append(np.array(obs_strings)) \n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fore_texts_ip = np.concatenate(fore_texts_ip, axis=0)\n",
    "fore_inds = np.concatenate(fore_inds, axis=0)\n",
    "# Get train and valid ts_ind for forecast task.\n",
    "train_sub = oc.loc[oc.ts_ind.isin(train_ind)].SUBJECT_ID.unique()\n",
    "valid_sub = oc.loc[oc.ts_ind.isin(valid_ind)].SUBJECT_ID.unique()\n",
    "rem_sub = oc.loc[~oc.SUBJECT_ID.isin(np.concatenate((train_ind, valid_ind)))].SUBJECT_ID.unique()\n",
    "bp = int(0.8*len(rem_sub))\n",
    "train_sub = np.concatenate((train_sub, rem_sub[:bp]))\n",
    "valid_sub = np.concatenate((valid_sub, rem_sub[bp:]))\n",
    "train_ind = oc.loc[oc.SUBJECT_ID.isin(train_sub)].ts_ind.unique() # Add remaining ts_ind s of train subjects.\n",
    "valid_ind = oc.loc[oc.SUBJECT_ID.isin(valid_sub)].ts_ind.unique() # Add remaining ts_ind s of train subjects.\n",
    "# Generate 3 sets of inputs and outputs.\n",
    "train_ind = np.argwhere(np.in1d(fore_inds, train_ind)).flatten()\n",
    "valid_ind = np.argwhere(np.in1d(fore_inds, valid_ind)).flatten()\n",
    "fore_train_ip = [ip[train_ind] for ip in [fore_texts_ip]]\n",
    "fore_valid_ip = [ip[valid_ind] for ip in [fore_texts_ip]]\n",
    "del fore_texts_ip\n",
    "\n",
    "fore_train_text_ip = fore_train_ip[0]\n",
    "fore_valid_text_ip = fore_valid_ip[0]\n",
    "del fore_train_ip, fore_valid_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Ctd5oKMQUI-t",
    "outputId": "002f0508-07bd-4606-b628-54787bbe37be",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449482/449482 [00:04<00:00, 98917.84it/s] \n"
     ]
    }
   ],
   "source": [
    "fore_train_concat_text_ip = []\n",
    "# concat train texts per instance\n",
    "for text in tqdm(fore_train_text_ip):\n",
    "    if text == []:\n",
    "        fore_train_concat_text_ip.append(np.array([0.0]))\n",
    "    else: \n",
    "        concat_text = np.concatenate(text)\n",
    "        fore_train_concat_text_ip.append(concat_text)\n",
    "# train_embs = np.array(fore_train_concat_text_ip)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136823/136823 [00:01<00:00, 93632.88it/s]\n"
     ]
    }
   ],
   "source": [
    "fore_valid_concat_text_ip = []\n",
    "# concat train texts per instance\n",
    "for text in tqdm(fore_valid_text_ip):\n",
    "    if text == []:\n",
    "        fore_valid_concat_text_ip.append(np.array([0.0]))\n",
    "    else: \n",
    "        concat_text = np.concatenate(text)\n",
    "        fore_valid_concat_text_ip.append(concat_text)\n",
    "# train_embs = np.array(fore_train_concat_text_ip)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449482, 33792)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding\n",
    "max_len = 33792\n",
    "# padding\n",
    "padded_train = pad_sequences(fore_train_concat_text_ip, maxlen=max_len, dtype='float32', padding='post')\n",
    "padded_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4608"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "768*50 - 33792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136823, 33792)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding\n",
    "padded_valid = pad_sequences(fore_valid_concat_text_ip, maxlen=max_len, dtype='float32', padding='post')\n",
    "padded_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dump to pkl, update sepsis_removed_0.pkl\n",
    "pickle.dump([padded_train, padded_valid], open('Data/text_emb_input_train_val_1.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Data/sepsis_removed_0.pkl'\n",
    "pkl = pickle.load(open(data_path, 'rb'))\n",
    "data = pkl[0]\n",
    "oc = pkl[1]\n",
    "train_ind = pkl[2]\n",
    "valid_ind = pkl[3]\n",
    "test_ind = pkl[4]\n",
    "# embs = pkl[5]\n",
    "del pkl\n",
    "\n",
    "# embs\n",
    "data_path = 'Data/CLS_emb_keywords_reserved.pkl'\n",
    "pkl = pickle.load(open(data_path, 'rb'))\n",
    "embs = pkl[0]\n",
    "del pkl\n",
    "\n",
    "text_data = data[data['variable'] == 'Text']\n",
    "embs_list = []\n",
    "for emb in embs:\n",
    "    embs_list.append(emb)\n",
    "text_data['value'] = embs_list\n",
    "physio_data = data[data['variable'] != 'Text']\n",
    "del data\n",
    "data = text_data.append(physio_data, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_window = 2  # hours\n",
    "obs_windows = range(20, 124, 4)\n",
    "\n",
    "# Remove train, val patients\n",
    "data = data.merge(oc[['ts_ind', 'SUBJECT_ID']], on='ts_ind', how='left')\n",
    "train_sub = oc.loc[oc.ts_ind.isin(train_ind)].SUBJECT_ID.unique()\n",
    "valid_sub = oc.loc[oc.ts_ind.isin(valid_ind)].SUBJECT_ID.unique()\n",
    "data = data.loc[~data.SUBJECT_ID.isin(train_sub)]\n",
    "data = data.loc[~data.SUBJECT_ID.isin(valid_sub)]\n",
    "oc = oc.loc[~oc.SUBJECT_ID.isin(train_sub)]\n",
    "oc = oc.loc[~oc.SUBJECT_ID.isin(valid_sub)]\n",
    "\n",
    "data.drop(columns=['SUBJECT_ID', 'TABLE'], inplace=True)\n",
    "# Get static data with mean fill and missingness indicator.\n",
    "static_varis = ['Age', 'Gender']\n",
    "ii = data.variable.isin(static_varis)\n",
    "static_data = data.loc[ii]\n",
    "data = data.loc[~ii]\n",
    "\n",
    "\n",
    "def inv_list(l, start=0):\n",
    "    d = {}\n",
    "    for i in range(len(l)):\n",
    "        d[l[i]] = i+start\n",
    "    return d\n",
    "\n",
    "\n",
    "static_var_to_ind = inv_list(static_varis)\n",
    "D = len(static_varis)\n",
    "N = data.ts_ind.max()+1\n",
    "\n",
    "# Get variable indices.\n",
    "varis = sorted(list(set(data.variable)))\n",
    "V = len(varis)\n",
    "var_to_ind = inv_list(varis, start=1)\n",
    "data['vind'] = data.variable.map(var_to_ind)\n",
    "data = data[['ts_ind', 'vind', 'hour', 'value']\n",
    "            ].sort_values(by=['ts_ind', 'vind', 'hour'])\n",
    "# Find max_len.\n",
    "fore_max_len = 880\n",
    "# Get forecast inputs and outputs.\n",
    "fore_texts_ip = []\n",
    "fore_inds = []\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    mask = [0 for i in range(V)]\n",
    "    values = [0 for i in range(V)]\n",
    "    for vv in x:\n",
    "        v = int(vv[0])-1\n",
    "        mask[v] = 1\n",
    "        values[v] = vv[1]\n",
    "    return values+mask\n",
    "\n",
    "\n",
    "def pad(x):\n",
    "    return x+[0]*(fore_max_len-len(x))\n",
    "\n",
    "\n",
    "for w in tqdm(obs_windows):\n",
    "    pred_data = data.loc[(data.hour >= w) & (data.hour <= w+pred_window)]\n",
    "    pred_data = pred_data.groupby(['ts_ind', 'vind']).agg(\n",
    "        {'value': 'first'}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data[['vind', 'value']].values.tolist()\n",
    "    pred_data = pred_data.groupby('ts_ind').agg(\n",
    "        {'vind_value': list}).reset_index()\n",
    "    pred_data['vind_value'] = pred_data['vind_value'].apply(f)\n",
    "    obs_data = data.loc[(data.hour < w) & (data.hour >= w-24)]\n",
    "    obs_data = obs_data.loc[obs_data.ts_ind.isin(pred_data.ts_ind)]\n",
    "    obs_data = obs_data.groupby('ts_ind').head(fore_max_len)\n",
    "    obs_data = obs_data.groupby('ts_ind').agg(\n",
    "        {'vind': list, 'hour': list, 'value': list}).reset_index()\n",
    "    obs_data = obs_data.merge(pred_data, on='ts_ind')\n",
    "    for col in ['vind', 'hour', 'value']:\n",
    "        obs_data[col] = obs_data[col].apply(pad)\n",
    "    fore_inds.append(np.array(list(obs_data.ts_ind)))\n",
    "\n",
    "    matrix = list(obs_data.value)\n",
    "    obs_strings = []\n",
    "    for l in matrix:\n",
    "        string_list = []\n",
    "        for value in l:\n",
    "            if not np.isscalar(value):\n",
    "            # if isinstance(value, str):\n",
    "                string_list.append(value)\n",
    "        obs_strings.append(string_list)\n",
    "    del matrix\n",
    "    fore_texts_ip.append(np.array(obs_strings))\n",
    "del data\n",
    "\n",
    "fore_texts_ip = np.concatenate(fore_texts_ip, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fore_test_concat_text_ip = []\n",
    "# concat train texts per instance\n",
    "for text in tqdm(fore_texts_ip):\n",
    "    if text == []:\n",
    "        fore_test_concat_text_ip.append(np.array([0.0]))\n",
    "    else: \n",
    "        concat_text = np.concatenate(text)\n",
    "        fore_test_concat_text_ip.append(concat_text)\n",
    "# train_embs = np.array(fore_train_concat_text_ip)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33792"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # obtain max_len, max_len = 33792\n",
    "# max_length = max([len(max(fore_test_concat_text_ip, key=len)), len(max(fore_train_concat_text_ip, key=len)), len(max(fore_valid_concat_text_ip, key=len))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449482, 33792)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # padding\n",
    "# padded_train = pad_sequences(fore_train_concat_text_ip, maxlen=max_length, dtype='float32', padding='post')\n",
    "# padded_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136823, 33792)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # padding\n",
    "# padded_valid = pad_sequences(fore_valid_concat_text_ip, maxlen=max_length, dtype='float32', padding='post')\n",
    "# padded_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# padding\n",
    "max_len = 33792\n",
    "padded_test = pad_sequences(fore_test_concat_text_ip, maxlen=max_len, dtype='float32', padding='post')\n",
    "padded_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_path = 'Data/sepsis_removed_0.pkl'\n",
    "# pkl = pickle.load(open(data_path, 'rb'))\n",
    "# data = pkl[0]\n",
    "# oc = pkl[1]\n",
    "# train_ind = pkl[2]\n",
    "# valid_ind = pkl[3]\n",
    "# test_ind = pkl[4]\n",
    "# embs = pkl[5]\n",
    "# del pkl\n",
    "# text_data = data[data['variable'] == 'Text']\n",
    "# embs_list = []\n",
    "# for emb in embs:\n",
    "#     embs_list.append(emb)\n",
    "# text_data['value'] = embs_list\n",
    "# physio_data = data[data['variable'] != 'Text']\n",
    "# data_more = text_data.append(physio_data, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump to pkl, update sepsis_removed_0.pkl\n",
    "pickle.dump([padded_test], open('Data/text_emb_input_test_1.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
