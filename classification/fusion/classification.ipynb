{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e7odgm5VsOsb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 24 17:50:54 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   31C    P0              41W / 300W |      9MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2552      G   /usr/libexec/Xorg                             8MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# gpu check\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oEEUeIW3sOsc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of cores\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() \n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY9I5eXWeOom"
   },
   "source": [
    "## Environment Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis\n"
     ]
    }
   ],
   "source": [
    "cd /pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 17:50:57.918644: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-24 17:50:58.793180: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-24 17:51:16.673168: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Lambda\n",
    "# from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, pipeline, AutoModel\n",
    "import resources.smart_cond as sc\n",
    "# from google.colab import files\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Data/CLS/cls_data.pkl'\n",
    "train_ip, valid_ip, test_ip, train_op, valid_op, test_op = pickle.load(open(data_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path ='Data/CLS/cls_text_times.pkl'\n",
    "train_times, valid_times, test_times, train_varis, valid_varis, test_varis = pickle.load(open(data_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ip.append(train_times)\n",
    "valid_ip.append(valid_times)\n",
    "test_ip.append(test_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ip.append(train_varis)\n",
    "valid_ip.append(valid_varis)\n",
    "test_ip.append(test_varis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_res(y_true, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    minrp = np.minimum(precision, recall).max()\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return [roc_auc, pr_auc, minrp]\n",
    "\n",
    "######################################################################################################## \n",
    "######################################################################################################## \n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=train_op)\n",
    "def mortality_loss(y_true, y_pred):\n",
    "    sample_weights = (1-y_true)*class_weights[0] + y_true*class_weights[1]\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    return K.mean(sample_weights*bce, axis=-1)\n",
    "######################################################################################################## \n",
    "######################################################################################################## \n",
    "\n",
    "# var_weights = np.sum(fore_train_op[:, V:], axis=0)\n",
    "# var_weights[var_weights==0] = var_weights.max()\n",
    "# var_weights = var_weights.max()/var_weights\n",
    "# var_weights = var_weights.reshape((1, V))\n",
    "def forecast_loss(y_true, y_pred):\n",
    "    return K.sum(y_true[:,V:]*(y_true[:,:V]-y_pred)**2, axis=-1)\n",
    "\n",
    "def get_min_loss(weight):\n",
    "    def min_loss(y_true, y_pred):\n",
    "        return weight*y_pred\n",
    "    return min_loss\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, validation_data, batch_size):\n",
    "        self.val_x, self.val_y = validation_data\n",
    "        self.batch_size = batch_size\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.val_x, verbose=0, batch_size=self.batch_size)\n",
    "        if type(y_pred)==type([]):\n",
    "            y_pred = y_pred[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(self.val_y, y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(self.val_y, y_pred)\n",
    "        logs['custom_metric'] = pr_auc + roc_auc\n",
    "        print ('val_aucs:', pr_auc, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Activation, Dropout, Softmax, Layer, InputSpec, Input, Dense, Lambda, TimeDistributed, Concatenate, Add\n",
    "from tensorflow.keras import initializers, regularizers, constraints, Model\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # class Multimodal_Fusion(tf.keras.Model):\n",
    "# class Multimodal_Fusion(Layer):\n",
    "#     def __init__(self, num_latents=880+50):\n",
    "#         super(Multimodal_Fusion, self).__init__()\n",
    "\n",
    "#         # Latents\n",
    "#         self.num_latents = num_latents\n",
    "#         self.latents = tf.Variable(tf.random.normal(shape=(1, num_latents, 50), stddev=0.02), trainable=True)\n",
    "#         self.scale_a = tf.Variable(tf.zeros(shape=(1,)), trainable=True)\n",
    "#         self.scale_v = tf.Variable(tf.zeros(shape=(1,)), trainable=True)\n",
    "\n",
    "class Multimodal_Fusion(Layer):\n",
    "    def __init__(self, num_latents=880+50, name=\"multimodal_fusion\"):\n",
    "        super(Multimodal_Fusion, self).__init__(name=name)\n",
    "\n",
    "        # Latents\n",
    "        self.num_latents = num_latents\n",
    "        self.latents = self.add_weight(name=\"latents\", shape=(1, num_latents, 50), initializer='random_normal', trainable=True)\n",
    "        self.scale_a = self.add_weight(name=\"scale_a\", shape=(1,), initializer='zeros', trainable=True)\n",
    "        self.scale_v = self.add_weight(name=\"scale_v\", shape=(1,), initializer='zeros', trainable=True)\n",
    "\n",
    "\n",
    "    def multimodal_fusion_attention(self, q, k, v): # requires q,k,v to have same dim\n",
    "        B, N, C = q.shape\n",
    "        B, _, _ = k.shape\n",
    "        attn = tf.matmul(q, k, transpose_b=True) * (C ** -0.5) # scaling\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        x = tf.matmul(attn, v)\n",
    "        # x = tf.reshape(x, (B, N, C))\n",
    "        return x\n",
    "        \n",
    "    # Latent Fusion\n",
    "    def multimodal_fusion(self, pysio_embs, text_embs):\n",
    "        # shapes\n",
    "        B, N, C = pysio_embs.shape\n",
    "        # concat all the tokens\n",
    "        concat_ = tf.concat([pysio_embs, text_embs], axis=1)\n",
    "        # cross attention (AV -->> latents)\n",
    "        # X = tf.broadcast_to(self.latents, [B, N, C])\n",
    "        fused_latents = self.multimodal_fusion_attention(self.latents, concat_, concat_)\n",
    "        # cross attention (latents -->> AV)\n",
    "        pysio_embs = pysio_embs + self.scale_a * self.multimodal_fusion_attention(pysio_embs, fused_latents, fused_latents)\n",
    "        text_embs = text_embs + self.scale_v * self.multimodal_fusion_attention(text_embs, fused_latents, fused_latents)\n",
    "        return pysio_embs, text_embs\n",
    "    \n",
    "    def call(self, x, y):\n",
    "\n",
    "        # Bottleneck Fusion\n",
    "        x,y = self.multimodal_fusion(x,y)\n",
    "\n",
    "        return tf.concat([x, y], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVE(Layer):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        super(CVE, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        self.W1 = self.add_weight(name='CVE_W1',\n",
    "                            shape=(1, self.hid_units),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        self.b1 = self.add_weight(name='CVE_b1',\n",
    "                            shape=(self.hid_units,),\n",
    "                            initializer='zeros',\n",
    "                            trainable=True)\n",
    "        self.W2 = self.add_weight(name='CVE_W2',\n",
    "                            shape=(self.hid_units, self.output_dim),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        super(CVE, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = K.expand_dims(x, axis=-1)\n",
    "        x = K.dot(K.tanh(K.bias_add(K.dot(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.output_dim,)\n",
    "    \n",
    "    \n",
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, hid_dim):\n",
    "        self.hid_dim = hid_dim\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        self.W = self.add_weight(shape=(d, self.hid_dim), name='Att_W',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.hid_dim,), name='Att_b',\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(shape=(self.hid_dim,1), name='Att_u',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = K.dot(K.tanh(K.bias_add(K.dot(x,self.W), self.b)), self.u)\n",
    "        mask = K.expand_dims(mask, axis=-1)\n",
    "        attn_weights = mask*attn_weights + (1-mask)*mask_value\n",
    "        attn_weights = K.softmax(attn_weights, axis=-2)\n",
    "        return attn_weights\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (1,)\n",
    "    \n",
    "    \n",
    "class Transformer(Layer):\n",
    "    \n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0):\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = K.epsilon() * K.epsilon()\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        if self.dk==None:\n",
    "            self.dk = d//self.h\n",
    "        if self.dv==None:\n",
    "            self.dv = d//self.h\n",
    "        if self.dff==None:\n",
    "            self.dff = 2*d\n",
    "        self.Wq = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wq',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wk = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wk',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wv = self.add_weight(shape=(self.N, self.h, d, self.dv), name='Wv',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wo = self.add_weight(shape=(self.N, self.dv*self.h, d), name='Wo',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.W1 = self.add_weight(shape=(self.N, d, self.dff), name='W1',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b1 = self.add_weight(shape=(self.N, self.dff), name='b1',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.W2 = self.add_weight(shape=(self.N, self.dff, d), name='W2',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b2 = self.add_weight(shape=(self.N, d), name='b2',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.gamma = self.add_weight(shape=(2*self.N,), name='gamma',\n",
    "                                 initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(shape=(2*self.N,), name='beta',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Transformer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e-30):\n",
    "        mask = K.expand_dims(mask, axis=-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = K.dot(x, self.Wq[i,j,:,:])\n",
    "                k = K.permute_dimensions(K.dot(x, self.Wk[i,j,:,:]), (0,2,1))\n",
    "                v = K.dot(x, self.Wv[i,j,:,:])\n",
    "                A = K.batch_dot(q,k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask*A + (1-mask)*mask_value\n",
    "                # Mask for attention dropout.\n",
    "                def dropped_A():\n",
    "                    dp_mask = K.cast((K.random_uniform(shape=array_ops.shape(A))>=self.dropout), K.floatx())\n",
    "                    return A*dp_mask + (1-dp_mask)*mask_value\n",
    "                A = sc.smart_cond(K.learning_phase(), dropped_A, lambda: array_ops.identity(A))\n",
    "                A = K.softmax(A, axis=-1)\n",
    "                mha_ops.append(K.batch_dot(A,v))\n",
    "            conc = K.concatenate(mha_ops, axis=-1)\n",
    "            proj = K.dot(conc, self.Wo[i,:,:])\n",
    "            # Dropout.\n",
    "            proj = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(proj, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(proj))\n",
    "            # Add & LN\n",
    "            x = x+proj\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i] + self.beta[2*i]\n",
    "            # FFN\n",
    "            ffn_op = K.bias_add(K.dot(K.relu(K.bias_add(K.dot(x, self.W1[i,:,:]), self.b1[i,:])), \n",
    "                           self.W2[i,:,:]), self.b2[i,:,])\n",
    "            # Dropout.\n",
    "            ffn_op = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(ffn_op, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(ffn_op))\n",
    "            # Add & LN\n",
    "            x = x+ffn_op\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i+1] + self.beta[2*i+1]            \n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def build_strats(D, max_len, V, d, N, he, dropout, forecast=False):\n",
    "    # demo\n",
    "    demo = Input(shape=(D,))\n",
    "    demo_enc = Dense(2*d, activation='tanh')(demo)\n",
    "    demo_enc = Dense(d, activation='tanh')(demo_enc)\n",
    "    \n",
    "    ## text\n",
    "    # text\n",
    "    texts = Input(shape=(33792,))\n",
    "    text_enc = Dense(1000, activation='relu')(texts)\n",
    "    text_enc = Dense(d, activation='relu')(text_enc)\n",
    "    \n",
    "    # text time\n",
    "    text_times = Input(shape=(50,))\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    text_times_emb = CVE(cve_units, d)(text_times)\n",
    "    \n",
    "    # text varis\n",
    "    text_varis = Input(shape=(50,))\n",
    "    text_varis_emb = Embedding(V+1, d)(text_varis)\n",
    "    \n",
    "    \n",
    "    \n",
    "    text_comb_emb = Add()([text_varis_emb, text_enc, text_times_emb])\n",
    "\n",
    "    \n",
    "    ## physio\n",
    "    # triplet\n",
    "    varis = Input(shape=(max_len,))\n",
    "    values = Input(shape=(max_len,))\n",
    "    times = Input(shape=(max_len,))\n",
    "    \n",
    "    \n",
    "    varis_emb = Embedding(V+1, d)(varis)\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    values_emb = CVE(cve_units, d)(values)\n",
    "    times_emb = CVE(cve_units, d)(times)\n",
    "    \n",
    "    \n",
    "    comb_emb = Add()([varis_emb, values_emb, times_emb]) # b, L, d\n",
    "    \n",
    "    # conc_0 = Concatenate(axis=1)([comb_emb, text_comb_emb])\n",
    "    \n",
    "    fused_pre_transformer = Multimodal_Fusion()(comb_emb, text_comb_emb)\n",
    "    \n",
    "#     demo_enc = Lambda(lambda x:K.expand_dims(x, axis=-2))(demo_enc) # b, 1, d\n",
    "#     comb_emb = Concatenate(axis=-2)([demo_enc, comb_emb]) # b, L+1, d\n",
    "\n",
    "\n",
    "    conc_varis = Concatenate(axis=-1)([varis, text_varis])\n",
    "    mask = Lambda(lambda x:K.clip(x,0,1))(conc_varis) # b, L\n",
    "    \n",
    "\n",
    "    cont_emb = Transformer(N, he, dk=None, dv=None, dff=None, dropout=dropout)(fused_pre_transformer, mask=mask)\n",
    "    attn_weights = Attention(2*d)(cont_emb, mask=mask)\n",
    "    fused_emb = Lambda(lambda x:K.sum(x[0]*x[1], axis=-2))([cont_emb, attn_weights])\n",
    "    \n",
    "#     # embed text input\n",
    "#     texts = Input(shape=(33792,))\n",
    "#     text_enc = Dense(1000, activation='relu')(texts)\n",
    "#     text_enc = Dense(d, activation='relu')(text_enc)\n",
    "\n",
    "\n",
    "    conc = Concatenate(axis=-1)([fused_emb, demo_enc])\n",
    "    \n",
    "    \n",
    "    fore_op = Dense(V)(conc)\n",
    "    op = Dense(1, activation='sigmoid')(fore_op)\n",
    "    model = Model([demo, times, values, varis, texts, text_times, text_varis], op)\n",
    "    if forecast:\n",
    "        fore_model = Model([demo, times, values, varis, texts, text_times, text_varis], fore_op)\n",
    "        return [model, fore_model]\n",
    "    return model\n",
    "\n",
    "# To tune:\n",
    "# 1. Transformer parameters. (N, h, dropout)\n",
    "# 2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "D = 2\n",
    "max_len = 880\n",
    "V = 134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 0 ld 100\n",
      "Num train: 36551 Num valid: 9262\n",
      "CLS/post_koll_exp1_fusion/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_100ld.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 17:53:24.713198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31133 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b2:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 17:53:43.161225: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x14e24497d060 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-24 17:53:43.161282: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2024-03-24 17:53:43.170856: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-24 17:53:43.554586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
      "2024-03-24 17:53:43.876957: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1143/1143 [==============================] - ETA: 0s - loss: 0.4499val_aucs: 0.5090490980498839 0.8974345891912152\n",
      "1143/1143 [==============================] - 93s 70ms/step - loss: 0.4499 - custom_metric: 1.4065\n",
      "Epoch 2/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.3952val_aucs: 0.5430705044892083 0.9024163696696715\n",
      "1143/1143 [==============================] - 76s 67ms/step - loss: 0.3952 - custom_metric: 1.4455\n",
      "Epoch 3/1000\n",
      "1142/1143 [============================>.] - ETA: 0s - loss: 0.3609val_aucs: 0.5281636133267025 0.896495488398588\n",
      "1143/1143 [==============================] - 76s 66ms/step - loss: 0.3609 - custom_metric: 1.4247\n",
      "Epoch 4/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.3207val_aucs: 0.49170524734530646 0.8840595995146402\n",
      "1143/1143 [==============================] - 76s 66ms/step - loss: 0.3207 - custom_metric: 1.3758\n",
      "Epoch 5/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.2859val_aucs: 0.43744879596880293 0.859182792385865\n",
      "1143/1143 [==============================] - 76s 66ms/step - loss: 0.2859 - custom_metric: 1.2966\n",
      "Epoch 6/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.2480val_aucs: 0.4715359853693528 0.873519647330627\n",
      "1143/1143 [==============================] - 76s 67ms/step - loss: 0.2480 - custom_metric: 1.3451\n",
      "Epoch 7/1000\n",
      "1142/1143 [============================>.] - ETA: 0s - loss: 0.2144val_aucs: 0.4756914033593703 0.8678990718446847\n",
      "1143/1143 [==============================] - 76s 67ms/step - loss: 0.2145 - custom_metric: 1.3436\n",
      "Epoch 8/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.1893val_aucs: 0.4389032797277517 0.8570445199025045\n",
      "1143/1143 [==============================] - 76s 66ms/step - loss: 0.1893 - custom_metric: 1.2959\n",
      "Epoch 9/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.1728val_aucs: 0.42634668144635535 0.8491684084137502\n",
      "1143/1143 [==============================] - 76s 67ms/step - loss: 0.1728 - custom_metric: 1.2755\n",
      "Epoch 10/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.1572val_aucs: 0.40837277888980206 0.8400063687590926\n",
      "1143/1143 [==============================] - 75s 66ms/step - loss: 0.1572 - custom_metric: 1.2484\n",
      "Epoch 11/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.1429val_aucs: 0.4053377677710232 0.8443388322776969\n",
      "1143/1143 [==============================] - 76s 66ms/step - loss: 0.1429 - custom_metric: 1.2497\n",
      "Epoch 12/1000\n",
      "1142/1143 [============================>.] - ETA: 0s - loss: 0.1291val_aucs: 0.40736238969364236 0.8508380900499124\n",
      "1143/1143 [==============================] - 76s 66ms/step - loss: 0.1291 - custom_metric: 1.2582\n",
      "Test res 0.8934982161022019 0.5211927680126884 0.51171875\n",
      "[[0.8934982161022019, 0.5211927680126884, 0.51171875]]\n",
      "Repeat 1 ld 100\n",
      "Num train: 36551 Num valid: 9262\n",
      "CLS/post_koll_exp1_fusion/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_100ld.h5\n",
      "Epoch 1/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.4513val_aucs: 0.5136109423632206 0.8971948342640298\n",
      "1143/1143 [==============================] - 87s 69ms/step - loss: 0.4513 - custom_metric: 1.4108\n",
      "Epoch 2/1000\n",
      " 465/1143 [===========>..................] - ETA: 40s - loss: 0.3993"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1143/1143 [==============================] - ETA: 0s - loss: 0.2579val_aucs: 0.48540143210439957 0.8732724807137244\n",
      "1143/1143 [==============================] - 79s 69ms/step - loss: 0.2579 - custom_metric: 1.3587\n",
      "Epoch 7/1000\n",
      "1142/1143 [============================>.] - ETA: 0s - loss: 0.2285val_aucs: 0.44787473844869496 0.8635672039082898\n",
      "1143/1143 [==============================] - 76s 66ms/step - loss: 0.2285 - custom_metric: 1.3114\n",
      "Epoch 8/1000\n",
      "1142/1143 [============================>.] - ETA: 0s - loss: 0.2027val_aucs: 0.43091653486210646 0.84645824465776\n",
      "1143/1143 [==============================] - 75s 66ms/step - loss: 0.2027 - custom_metric: 1.2774\n",
      "Epoch 9/1000\n",
      "  41/1143 [>.............................] - ETA: 1:05 - loss: 0.1654"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1142/1143 [============================>.] - ETA: 0s - loss: 0.1189val_aucs: 0.4132599408604636 0.8456613556615782\n",
      "1143/1143 [==============================] - 76s 66ms/step - loss: 0.1189 - custom_metric: 1.2589\n",
      "Test res 0.8912100492909287 0.47625315559891357 0.49416342412451364\n",
      "[[0.8934982161022019, 0.5211927680126884, 0.51171875], [0.8912100492909287, 0.47625315559891357, 0.49416342412451364]]\n",
      "Repeat 2 ld 100\n",
      "Num train: 36551 Num valid: 9262\n",
      "CLS/post_koll_exp1_fusion/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_100ld.h5\n",
      "Epoch 1/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.4510val_aucs: 0.5105353012155777 0.8946212573613962\n",
      "1143/1143 [==============================] - 88s 69ms/step - loss: 0.4510 - custom_metric: 1.4052\n",
      "Epoch 2/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.3964val_aucs: 0.5292718930172383 0.9015920177160559\n",
      "1143/1143 [==============================] - 77s 67ms/step - loss: 0.3964 - custom_metric: 1.4309\n",
      "Epoch 3/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.3603val_aucs: 0.5372443753650062 0.8940591601073212\n",
      "1143/1143 [==============================] - 76s 67ms/step - loss: 0.3603 - custom_metric: 1.4313\n",
      "Epoch 4/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.2716val_aucs: 0.4772279416178553 0.8779016765242139\n",
      "1143/1143 [==============================] - 76s 66ms/step - loss: 0.2716 - custom_metric: 1.3551\n",
      "Epoch 7/1000\n",
      "1142/1143 [============================>.] - ETA: 0s - loss: 0.2424val_aucs: 0.4456974355325899 0.8623393119199028\n",
      "1143/1143 [==============================] - 76s 67ms/step - loss: 0.2424 - custom_metric: 1.3080\n",
      "Epoch 8/1000\n",
      "1142/1143 [============================>.] - ETA: 0s - loss: 0.2211val_aucs: 0.4309563394914474 0.8654362732330002\n",
      "1143/1143 [==============================] - 76s 67ms/step - loss: 0.2210 - custom_metric: 1.2964\n",
      "Epoch 9/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.1967val_aucs: 0.42282052567009476 0.8473162139183062\n",
      "1143/1143 [==============================] - 76s 67ms/step - loss: 0.1967 - custom_metric: 1.2701\n",
      "Epoch 10/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.1796val_aucs: 0.43245791920583465 0.8551876269251864\n",
      "1143/1143 [==============================] - 76s 67ms/step - loss: 0.1796 - custom_metric: 1.2876\n",
      "Epoch 11/1000\n",
      "1142/1143 [============================>.] - ETA: 0s - loss: 0.1719val_aucs: 0.42648999415596617 0.8608659077096397\n",
      "1143/1143 [==============================] - 76s 67ms/step - loss: 0.1719 - custom_metric: 1.2874\n",
      "Epoch 12/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.1524val_aucs: 0.4066419089493134 0.8459259265141553\n",
      "1143/1143 [==============================] - 76s 66ms/step - loss: 0.1524 - custom_metric: 1.2526\n",
      "Epoch 13/1000\n",
      "1142/1143 [============================>.] - ETA: 0s - loss: 0.1474val_aucs: 0.37984905411840997 0.8414639569529061\n",
      "1143/1143 [==============================] - 75s 66ms/step - loss: 0.1474 - custom_metric: 1.2213\n",
      "Test res 0.8920629637386309 0.4993526677653265 0.5029296875\n",
      "[[0.8934982161022019, 0.5211927680126884, 0.51171875], [0.8912100492909287, 0.47625315559891357, 0.49416342412451364], [0.8920629637386309, 0.4993526677653265, 0.5029296875]]\n",
      "Repeat 3 ld 100\n",
      "Num train: 36551 Num valid: 9262\n",
      "CLS/post_koll_exp1_fusion/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat3_100ld.h5\n",
      "Epoch 1/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.4517val_aucs: 0.5310216312523672 0.9023353704891928\n",
      "1143/1143 [==============================] - 88s 69ms/step - loss: 0.4517 - custom_metric: 1.4334\n",
      "Epoch 2/1000\n",
      "1142/1143 [============================>.] - ETA: 0s - loss: 0.3950val_aucs: 0.5400314953273331 0.9049951082847868\n",
      "1143/1143 [==============================] - 76s 66ms/step - loss: 0.3949 - custom_metric: 1.4450\n",
      "Epoch 3/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.3705val_aucs: 0.53846786869019 0.9009258259269376\n",
      "1143/1143 [==============================] - 76s 67ms/step - loss: 0.3705 - custom_metric: 1.4394\n",
      "Epoch 4/1000\n",
      "1142/1143 [============================>.] - ETA: 0s - loss: 0.3322val_aucs: 0.5007257985489819 0.8922980896899161\n",
      "1143/1143 [==============================] - 76s 67ms/step - loss: 0.3321 - custom_metric: 1.3930\n",
      "Epoch 5/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.2969val_aucs: 0.4834476010435049 0.8731941947410885\n",
      "1143/1143 [==============================] - 76s 66ms/step - loss: 0.2969 - custom_metric: 1.3566\n",
      "Epoch 6/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.2730val_aucs: 0.4698647227087558 0.8771421106798214\n",
      "1143/1143 [==============================] - 76s 66ms/step - loss: 0.2730 - custom_metric: 1.3470\n",
      "Epoch 7/1000\n",
      "1143/1143 [==============================] - ETA: 0s - loss: 0.2452"
     ]
    }
   ],
   "source": [
    "repeats = {k:5 for k in [10,20,30,40,50,60,70,80,90,100]}\n",
    "lds = [100]\n",
    "batch_size, lr, patience = 32, 0.0005, 10\n",
    "d, N, he, dropout = 50,2,4,0.2\n",
    "\n",
    "# best val model\n",
    "fore_savepath = 'Exp_post_koll/exp_1/fusion/models/forecasting/forecasting_161_epochs.h5'\n",
    "\n",
    "train_inds = np.arange(len(train_op))\n",
    "valid_inds = np.arange(len(valid_op))\n",
    "gen_res = {}\n",
    "\n",
    "np.random.seed(2023)\n",
    "\n",
    "for ld in lds:\n",
    "    np.random.shuffle(train_inds)\n",
    "    np.random.shuffle(valid_inds)\n",
    "    train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "    valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "    all_test_res = []\n",
    "    for i in range(repeats[ld]):\n",
    "        print ('Repeat', i, 'ld', ld)\n",
    "        # Get train and validation data.\n",
    "        curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "        curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "        curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "        curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "        curr_train_op = train_op[curr_train_ind]\n",
    "        curr_valid_op = valid_op[curr_valid_ind]\n",
    "        print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "        # Construct save_path.\n",
    "        savepath = 'CLS/post_koll_exp1_fusion/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "        print (savepath)\n",
    "        # Build and compile model.\n",
    "        model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "        model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "        fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "        # Load pretrained weights here.\n",
    "        fore_model.load_weights(fore_savepath)\n",
    "        # Train model.\n",
    "        es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "                           restore_best_weights=True)\n",
    "        cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "        his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "                        verbose=1, callbacks=[cus, es]).history\n",
    "        model.save_weights(savepath)\n",
    "        # Test and write to log.\n",
    "        rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "\n",
    "        print ('Test res', rocauc, prauc, minrp)\n",
    "        all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "        # in case of unexpected disconnection\n",
    "        print(all_test_res)\n",
    "        \n",
    "    gen_res[ld] = []\n",
    "    for i in range(len(all_test_res[0])):\n",
    "        nums = [test_res[i] for test_res in all_test_res]\n",
    "        gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "    print ('gen_res', gen_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "repeats = {k:10 for k in [10,20,30,40,50,60,70,80,90,100]}\n",
    "lds = [60, 70]\n",
    "batch_size, lr, patience = 32, 0.0005, 10\n",
    "d, N, he, dropout = 50,2,4,0.2\n",
    "\n",
    "# best val model\n",
    "fore_savepath = 'Exp_post_koll/exp_1/fusion/models/forecasting/forecasting_161_epochs.h5'\n",
    "\n",
    "train_inds = np.arange(len(train_op))\n",
    "valid_inds = np.arange(len(valid_op))\n",
    "gen_res = {}\n",
    "\n",
    "np.random.seed(2023)\n",
    "\n",
    "for ld in lds:\n",
    "    np.random.shuffle(train_inds)\n",
    "    np.random.shuffle(valid_inds)\n",
    "    train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "    valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "    all_test_res = []\n",
    "    for i in range(repeats[ld]):\n",
    "        print ('Repeat', i, 'ld', ld)\n",
    "        # Get train and validation data.\n",
    "        curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "        curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "        curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "        curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "        curr_train_op = train_op[curr_train_ind]\n",
    "        curr_valid_op = valid_op[curr_valid_ind]\n",
    "        print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "        # Construct save_path.\n",
    "        savepath = 'CLS/post_koll_exp1_fusion/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "        print (savepath)\n",
    "        # Build and compile model.\n",
    "        model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "        model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "        fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "        # Load pretrained weights here.\n",
    "        fore_model.load_weights(fore_savepath)\n",
    "        # Train model.\n",
    "        es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "                           restore_best_weights=True)\n",
    "        cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "        his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "                        verbose=1, callbacks=[cus, es]).history\n",
    "        model.save_weights(savepath)\n",
    "        # Test and write to log.\n",
    "        rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "\n",
    "        print ('Test res', rocauc, prauc, minrp)\n",
    "        all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "        # in case of unexpected disconnection\n",
    "        print(all_test_res)\n",
    "        \n",
    "    gen_res[ld] = []\n",
    "    for i in range(len(all_test_res[0])):\n",
    "        nums = [test_res[i] for test_res in all_test_res]\n",
    "        gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "    print ('gen_res', gen_res)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
