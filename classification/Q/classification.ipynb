{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e7odgm5VsOsb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun  5 10:05:11 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   36C    P0             41W /  300W |      17MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           On  |   00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             41W /  300W |      17MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1918      G   /usr/libexec/Xorg                              17MiB |\n",
      "|    1   N/A  N/A      1918      G   /usr/libexec/Xorg                              17MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# gpu check\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oEEUeIW3sOsc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of cores\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() \n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY9I5eXWeOom"
   },
   "source": [
    "## Environment Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis\n"
     ]
    }
   ],
   "source": [
    "cd /pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 10:05:16.270485: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-05 10:05:16.908476: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-05 10:05:46.484222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Lambda\n",
    "# from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, pipeline, AutoModel\n",
    "import resources.smart_cond as sc\n",
    "# from google.colab import files\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Data/CLS/cls_data.pkl'\n",
    "train_ip, valid_ip, test_ip, train_op, valid_op, test_op = pickle.load(open(data_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_res(y_true, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    minrp = np.minimum(precision, recall).max()\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return [roc_auc, pr_auc, minrp]\n",
    "\n",
    "######################################################################################################## \n",
    "######################################################################################################## \n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=train_op)\n",
    "def mortality_loss(y_true, y_pred):\n",
    "    sample_weights = (1-y_true)*class_weights[0] + y_true*class_weights[1]\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    return K.mean(sample_weights*bce, axis=-1)\n",
    "######################################################################################################## \n",
    "######################################################################################################## \n",
    "\n",
    "# var_weights = np.sum(fore_train_op[:, V:], axis=0)\n",
    "# var_weights[var_weights==0] = var_weights.max()\n",
    "# var_weights = var_weights.max()/var_weights\n",
    "# var_weights = var_weights.reshape((1, V))\n",
    "def forecast_loss(y_true, y_pred):\n",
    "    return K.sum(y_true[:,V:]*(y_true[:,:V]-y_pred)**2, axis=-1)\n",
    "\n",
    "def get_min_loss(weight):\n",
    "    def min_loss(y_true, y_pred):\n",
    "        return weight*y_pred\n",
    "    return min_loss\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, validation_data, batch_size):\n",
    "        self.val_x, self.val_y = validation_data\n",
    "        self.batch_size = batch_size\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.val_x, verbose=0, batch_size=self.batch_size)\n",
    "        if type(y_pred)==type([]):\n",
    "            y_pred = y_pred[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(self.val_y, y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(self.val_y, y_pred)\n",
    "        logs['custom_metric'] = pr_auc + roc_auc\n",
    "        print ('val_aucs:', pr_auc, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Activation, Dropout, Softmax, Layer, InputSpec, Input, Dense, Lambda, TimeDistributed, Concatenate, Add\n",
    "from tensorflow.keras import initializers, regularizers, constraints, Model\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow import nn\n",
    "\n",
    "    \n",
    "class CVE(Layer):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        super(CVE, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        self.W1 = self.add_weight(name='CVE_W1',\n",
    "                            shape=(1, self.hid_units),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        self.b1 = self.add_weight(name='CVE_b1',\n",
    "                            shape=(self.hid_units,),\n",
    "                            initializer='zeros',\n",
    "                            trainable=True)\n",
    "        self.W2 = self.add_weight(name='CVE_W2',\n",
    "                            shape=(self.hid_units, self.output_dim),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        super(CVE, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = K.expand_dims(x, axis=-1)\n",
    "        x = K.dot(K.tanh(K.bias_add(K.dot(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.output_dim,)\n",
    "    \n",
    "    \n",
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, hid_dim):\n",
    "        self.hid_dim = hid_dim\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        self.W = self.add_weight(shape=(d, self.hid_dim), name='Att_W',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.hid_dim,), name='Att_b',\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(shape=(self.hid_dim,1), name='Att_u',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = K.dot(K.tanh(K.bias_add(K.dot(x,self.W), self.b)), self.u)\n",
    "        mask = K.expand_dims(mask, axis=-1)\n",
    "        attn_weights = mask*attn_weights + (1-mask)*mask_value\n",
    "        attn_weights = K.softmax(attn_weights, axis=-2)\n",
    "        return attn_weights\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (1,)\n",
    "    \n",
    "    \n",
    "class Transformer(Layer):\n",
    "    \n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0):\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = K.epsilon() * K.epsilon()\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        if self.dk==None:\n",
    "            self.dk = d//self.h\n",
    "        if self.dv==None:\n",
    "            self.dv = d//self.h\n",
    "        if self.dff==None:\n",
    "            self.dff = 2*d\n",
    "        self.Wq = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wq',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wk = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wk',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wv = self.add_weight(shape=(self.N, self.h, d, self.dv), name='Wv',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wo = self.add_weight(shape=(self.N, self.dv*self.h, d), name='Wo',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.W1 = self.add_weight(shape=(self.N, d, self.dff), name='W1',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b1 = self.add_weight(shape=(self.N, self.dff), name='b1',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.W2 = self.add_weight(shape=(self.N, self.dff, d), name='W2',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b2 = self.add_weight(shape=(self.N, d), name='b2',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.gamma = self.add_weight(shape=(2*self.N,), name='gamma',\n",
    "                                 initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(shape=(2*self.N,), name='beta',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Transformer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e-30):\n",
    "        mask = K.expand_dims(mask, axis=-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = K.dot(x, self.Wq[i,j,:,:])\n",
    "                k = K.permute_dimensions(K.dot(x, self.Wk[i,j,:,:]), (0,2,1))\n",
    "                v = K.dot(x, self.Wv[i,j,:,:])\n",
    "                A = K.batch_dot(q,k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask*A + (1-mask)*mask_value\n",
    "                # Mask for attention dropout.\n",
    "                def dropped_A():\n",
    "                    dp_mask = K.cast((K.random_uniform(shape=array_ops.shape(A))>=self.dropout), K.floatx())\n",
    "                    return A*dp_mask + (1-dp_mask)*mask_value\n",
    "                A = sc.smart_cond(K.learning_phase(), dropped_A, lambda: array_ops.identity(A))\n",
    "                A = K.softmax(A, axis=-1)\n",
    "                mha_ops.append(K.batch_dot(A,v))\n",
    "            conc = K.concatenate(mha_ops, axis=-1)\n",
    "            proj = K.dot(conc, self.Wo[i,:,:])\n",
    "            # Dropout.\n",
    "            proj = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(proj, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(proj))\n",
    "            # Add & LN\n",
    "            x = x+proj\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i] + self.beta[2*i]\n",
    "            # FFN\n",
    "            ffn_op = K.bias_add(K.dot(K.relu(K.bias_add(K.dot(x, self.W1[i,:,:]), self.b1[i,:])), \n",
    "                           self.W2[i,:,:]), self.b2[i,:,])\n",
    "            # Dropout.\n",
    "            ffn_op = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(ffn_op, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(ffn_op))\n",
    "            # Add & LN\n",
    "            x = x+ffn_op\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i+1] + self.beta[2*i+1]            \n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def build_strats(D, max_len, V, d, N, he, dropout, forecast=False):\n",
    "    \n",
    "    # demo\n",
    "    demo = Input(shape=(D,))\n",
    "    demo_enc = Dense(2*d, activation='tanh')(demo)\n",
    "    demo_enc = Dense(d, activation='tanh')(demo_enc)\n",
    "    \n",
    "    # text\n",
    "    texts = Input(shape=(33792,))\n",
    "    text_enc = Dense(880*2, activation='relu')(texts)\n",
    "    text_enc = Dense(880, activation='relu')(text_enc)\n",
    "    text_enc = Dense(d, activation='relu')(text_enc)\n",
    "    \n",
    "    # triplet\n",
    "    varis = Input(shape=(max_len,))\n",
    "    values = Input(shape=(max_len,))\n",
    "    times = Input(shape=(max_len,))\n",
    "    \n",
    "    varis_emb = Embedding(V+1, d)(varis)\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    values_emb = CVE(cve_units, d)(values)\n",
    "    times_emb = CVE(cve_units, d)(times)\n",
    "    \n",
    "    # comb_emb = Add()([varis_emb, values_emb, times_emb]) # b, L, d\n",
    "    comb_emb = Add()([varis_emb, values_emb, times_emb, text_enc]) # b, L, d\n",
    "#     demo_enc = Lambda(lambda x:K.expand_dims(x, axis=-2))(demo_enc) # b, 1, d\n",
    "#     comb_emb = Concatenate(axis=-2)([demo_enc, comb_emb]) # b, L+1, d\n",
    "    mask = Lambda(lambda x:K.clip(x,0,1))(varis) # b, L\n",
    "#     mask = Lambda(lambda x:K.concatenate((K.ones_like(x)[:,0:1], x), axis=-1))(mask) # b, L+1\n",
    "    cont_emb = Transformer(N, he, dk=None, dv=None, dff=None, dropout=dropout)(comb_emb, mask=mask)\n",
    "    attn_weights = Attention(2*d)(cont_emb, mask=mask)\n",
    "    fused_emb = Lambda(lambda x:K.sum(x[0]*x[1], axis=-2))([cont_emb, attn_weights])\n",
    "    # conc = Concatenate(axis=-1)([fused_emb, text_enc, demo_enc])\n",
    "    conc = Concatenate(axis=-1)([fused_emb, demo_enc])\n",
    "    fore_op = Dense(V)(conc)\n",
    "    op = Dense(1, activation='sigmoid')(fore_op)\n",
    "    model = Model([demo, times, values, varis, texts], op)\n",
    "    if forecast:\n",
    "        fore_model = Model([demo, times, values, varis, texts], fore_op)\n",
    "        return [model, fore_model]\n",
    "    return model\n",
    "\n",
    "# To tune:\n",
    "# 1. Transformer parameters. (N, h, dropout)\n",
    "# 2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "D = 2\n",
    "max_len = 880\n",
    "V = 134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 0 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/Q/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_50ld.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 10:08:15.562009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31117 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2024-06-05 10:08:15.562708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31117 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b2:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-05 10:08:35.252610: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x1491582267e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-05 10:08:35.252662: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2024-06-05 10:08:35.252669: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2024-06-05 10:08:35.663360: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-06-05 10:08:35.902195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
      "2024-06-05 10:08:36.232084: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - ETA: 0s - loss: 0.4709val_aucs: 0.5021378710661757 0.8875363291965459\n",
      "572/572 [==============================] - 49s 58ms/step - loss: 0.4709 - custom_metric: 1.3897\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4184val_aucs: 0.5081546642166908 0.8911855115415487\n",
      "572/572 [==============================] - 33s 59ms/step - loss: 0.4184 - custom_metric: 1.3993\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3908val_aucs: 0.529586888312372 0.8886810572182089\n",
      "572/572 [==============================] - 34s 59ms/step - loss: 0.3908 - custom_metric: 1.4183\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3778val_aucs: 0.53470498294191 0.8986139777548446\n",
      "572/572 [==============================] - 30s 52ms/step - loss: 0.3778 - custom_metric: 1.4333\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3597val_aucs: 0.5224924135066356 0.8953934137757666\n",
      "572/572 [==============================] - 29s 51ms/step - loss: 0.3597 - custom_metric: 1.4179\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3450val_aucs: 0.5316822997530801 0.8951822744392404\n",
      "572/572 [==============================] - 29s 51ms/step - loss: 0.3450 - custom_metric: 1.4269\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3272val_aucs: 0.5243557540281593 0.8942252564543586\n",
      "572/572 [==============================] - 29s 51ms/step - loss: 0.3272 - custom_metric: 1.4186\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3111val_aucs: 0.5023377272259462 0.8882381056336166\n",
      "572/572 [==============================] - 29s 51ms/step - loss: 0.3111 - custom_metric: 1.3906\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3011val_aucs: 0.5104578573410211 0.8912003960378574\n",
      "572/572 [==============================] - 29s 51ms/step - loss: 0.3011 - custom_metric: 1.4017\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2811val_aucs: 0.4972546009462522 0.8880928439751969\n",
      "572/572 [==============================] - 30s 52ms/step - loss: 0.2811 - custom_metric: 1.3853\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2664val_aucs: 0.49267187290354764 0.8859417586197772\n",
      "572/572 [==============================] - 30s 52ms/step - loss: 0.2664 - custom_metric: 1.3786\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2454val_aucs: 0.4928707880743073 0.8816473057959126\n",
      "572/572 [==============================] - 29s 51ms/step - loss: 0.2454 - custom_metric: 1.3745\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2373val_aucs: 0.4768089302923364 0.8805045072459933\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.2373 - custom_metric: 1.3573\n",
      "Epoch 14/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2452val_aucs: 0.4704990039974274 0.8793650163619203\n",
      "572/572 [==============================] - 30s 52ms/step - loss: 0.2452 - custom_metric: 1.3499\n",
      "Repeat 1 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/Q/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4854val_aucs: 0.47734980780892355 0.8902663721555806\n",
      "572/572 [==============================] - 39s 56ms/step - loss: 0.4854 - custom_metric: 1.3676\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4148val_aucs: 0.526654085942299 0.9032820854884035\n",
      "572/572 [==============================] - 30s 52ms/step - loss: 0.4148 - custom_metric: 1.4299\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3894val_aucs: 0.5199308461086216 0.9007859400115908\n",
      "572/572 [==============================] - 29s 51ms/step - loss: 0.3894 - custom_metric: 1.4207\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3748val_aucs: 0.53670178790784 0.9049053591540824\n",
      "572/572 [==============================] - 30s 52ms/step - loss: 0.3748 - custom_metric: 1.4416\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3584val_aucs: 0.5272417933976993 0.9020530119954948\n",
      "572/572 [==============================] - 29s 52ms/step - loss: 0.3584 - custom_metric: 1.4293\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3427val_aucs: 0.5172236677624563 0.9000781839455009\n",
      "572/572 [==============================] - 29s 51ms/step - loss: 0.3427 - custom_metric: 1.4173\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3222val_aucs: 0.5256959260328185 0.8961017375425092\n",
      "572/572 [==============================] - 29s 51ms/step - loss: 0.3222 - custom_metric: 1.4218\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3119val_aucs: 0.5070612649116216 0.8941072268209205\n",
      "572/572 [==============================] - 29s 51ms/step - loss: 0.3119 - custom_metric: 1.4012\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3904val_aucs: 0.520961548675641 0.9053143230240109\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3904 - custom_metric: 1.4263\n",
      "Epoch 4/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3719val_aucs: 0.5231794763987002 0.9007289699518245\n",
      "572/572 [==============================] - 28s 49ms/step - loss: 0.3719 - custom_metric: 1.4239\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3588val_aucs: 0.5269677762030994 0.9013227758374793\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3588 - custom_metric: 1.4283\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3489val_aucs: 0.4943734764831224 0.8985587825221891\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3489 - custom_metric: 1.3929\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3281val_aucs: 0.5248142642989673 0.9026344743992443\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3281 - custom_metric: 1.4274\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3129val_aucs: 0.5139724809121398 0.8961113969190263\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3129 - custom_metric: 1.4101\n",
      "Epoch 9/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2955val_aucs: 0.5328276370046096 0.9014053228440053\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2955 - custom_metric: 1.4342\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2771val_aucs: 0.5178791045051839 0.8996601193576458\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2771 - custom_metric: 1.4175\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2628val_aucs: 0.48158020988306 0.8824866140709094\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.2628 - custom_metric: 1.3641\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2524val_aucs: 0.4649941060693277 0.8770118835735717\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2524 - custom_metric: 1.3420\n",
      "Epoch 13/1000\n",
      "337/572 [================>.............] - ETA: 10s - loss: 0.2269"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - ETA: 0s - loss: 0.2157val_aucs: 0.4588193187517643 0.8848229606362618\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2157 - custom_metric: 1.3436\n",
      "Epoch 15/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2023val_aucs: 0.4643366980506371 0.8788628004844179\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2023 - custom_metric: 1.3432\n",
      "Epoch 16/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1937val_aucs: 0.4484184660118376 0.8743773026620612\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.1937 - custom_metric: 1.3228\n",
      "Epoch 17/1000\n",
      "513/572 [=========================>....] - ETA: 2s - loss: 0.1811val_aucs: 0.4369048559550278 0.8734068649954794\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2432 - custom_metric: 1.3103\n",
      "Repeat 4 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/Q/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4873val_aucs: 0.46993664555018927 0.8836925653730734\n",
      "572/572 [==============================] - 38s 54ms/step - loss: 0.4873 - custom_metric: 1.3536\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4172val_aucs: 0.4986800703338115 0.8945946091036467\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4172 - custom_metric: 1.3933\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3849val_aucs: 0.5097004403679402 0.8963350279422581\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3849 - custom_metric: 1.4060\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3695val_aucs: 0.49269217370579527 0.8965352673706362\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3695 - custom_metric: 1.3892\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3576val_aucs: 0.49893379197918153 0.8886867407140979\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3576 - custom_metric: 1.3876\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3411val_aucs: 0.5265084497907632 0.895253305561073\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3411 - custom_metric: 1.4218\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3188val_aucs: 0.5013127045643513 0.8911690654240727\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3188 - custom_metric: 1.3925\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3070val_aucs: 0.4780603798325824 0.8807265523924047\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3070 - custom_metric: 1.3588\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2933val_aucs: 0.46585502483819163 0.8820495820740081\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.2933 - custom_metric: 1.3479\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2652val_aucs: 0.4619874511923126 0.8720373422376352\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2652 - custom_metric: 1.3340\n",
      "Epoch 11/1000\n",
      "195/572 [=========>....................] - ETA: 16s - loss: 0.2561"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - ETA: 0s - loss: 0.1984val_aucs: 0.4493125126880121 0.8589946692291588\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.1984 - custom_metric: 1.3083\n",
      "Repeat 5 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/Q/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4715val_aucs: 0.5019153815349209 0.8933609624376601\n",
      "572/572 [==============================] - 38s 54ms/step - loss: 0.4715 - custom_metric: 1.3953\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4141val_aucs: 0.5004726239849543 0.8917042898479145\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4141 - custom_metric: 1.3922\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3877val_aucs: 0.5040596730052667 0.8903117400430542\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3877 - custom_metric: 1.3944\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3740val_aucs: 0.5134920469478557 0.8917713942139932\n",
      "572/572 [==============================] - 29s 51ms/step - loss: 0.3740 - custom_metric: 1.4053\n",
      "Epoch 5/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3547val_aucs: 0.515208272697782 0.8959436752792883\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3547 - custom_metric: 1.4112\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3388val_aucs: 0.5187091023934745 0.8951099706351294\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3388 - custom_metric: 1.4138\n",
      "Epoch 7/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.3204val_aucs: 0.49712585077139465 0.8837328279927205\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3204 - custom_metric: 1.3809\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3082val_aucs: 0.4969100409361492 0.8873086854523102\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3082 - custom_metric: 1.3842\n",
      "Epoch 9/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2969val_aucs: 0.5218429813865659 0.891794478115924\n",
      "572/572 [==============================] - 28s 49ms/step - loss: 0.2969 - custom_metric: 1.4136\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2775val_aucs: 0.4909251103723171 0.8836979337223597\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2775 - custom_metric: 1.3746\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2618val_aucs: 0.49186185847113173 0.8849546642902772\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2618 - custom_metric: 1.3768\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2451val_aucs: 0.5125663664262602 0.882247942580136\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2451 - custom_metric: 1.3948\n",
      "Epoch 13/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2257val_aucs: 0.4815749060604515 0.881756201785513\n",
      "572/572 [==============================] - 28s 49ms/step - loss: 0.2257 - custom_metric: 1.3633\n",
      "Epoch 14/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2200val_aucs: 0.4529750581177352 0.8781535025794918\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2200 - custom_metric: 1.3311\n",
      "Epoch 15/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1999val_aucs: 0.46647698578397745 0.8735874530940481\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.1999 - custom_metric: 1.3401\n",
      "Epoch 16/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1893val_aucs: 0.4757577131685314 0.872930635558872\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.1893 - custom_metric: 1.3487\n",
      "Repeat 6 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/Q/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4622val_aucs: 0.44563090417834994 0.8808856448721076\n",
      "572/572 [==============================] - 38s 55ms/step - loss: 0.4622 - custom_metric: 1.3265\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4117val_aucs: 0.4816288168910057 0.8939385098505184\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4117 - custom_metric: 1.3756\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3836val_aucs: 0.5011488213036241 0.8913835814086719\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3836 - custom_metric: 1.3925\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3714val_aucs: 0.5155952526095462 0.8984696421561867\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3714 - custom_metric: 1.4141\n",
      "Epoch 5/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2084val_aucs: 0.46053585096078675 0.8795767512916539\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2084 - custom_metric: 1.3401\n",
      "Repeat 7 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/Q/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4685val_aucs: 0.499782462766541 0.8948932399036904\n",
      "572/572 [==============================] - 38s 54ms/step - loss: 0.4685 - custom_metric: 1.3947\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4093val_aucs: 0.514809841388837 0.8997584847965068\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4093 - custom_metric: 1.4146\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3923val_aucs: 0.5140184201664206 0.8955348804706211\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3923 - custom_metric: 1.4096\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3742val_aucs: 0.5346272868413483 0.9004430072825145\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3742 - custom_metric: 1.4351\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3545val_aucs: 0.5185019494278091 0.8958292304336686\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3545 - custom_metric: 1.4143\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3414val_aucs: 0.5042405859456739 0.8866665290210005\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3414 - custom_metric: 1.3909\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3280val_aucs: 0.49757077452736803 0.8874360212355497\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3280 - custom_metric: 1.3850\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3086val_aucs: 0.5272846618118118 0.8886240092159068\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3086 - custom_metric: 1.4159\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2916val_aucs: 0.5172936244266046 0.8849636933085149\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2916 - custom_metric: 1.4023\n",
      "Epoch 10/1000\n",
      "365/572 [==================>...........] - ETA: 9s - loss: 0.2762"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - ETA: 0s - loss: 0.2186val_aucs: 0.4849878253942704 0.8820112837354217\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.2186 - custom_metric: 1.3670\n",
      "Epoch 15/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1974val_aucs: 0.48435520391312736 0.8732032335480611\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.1974 - custom_metric: 1.3576\n",
      "Repeat 9 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/Q/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4708val_aucs: 0.499667016635922 0.8982077870739789\n",
      "572/572 [==============================] - 38s 54ms/step - loss: 0.4708 - custom_metric: 1.3979\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4255val_aucs: 0.49399267971135313 0.9011185666371887\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4255 - custom_metric: 1.3951\n",
      "Epoch 3/1000\n",
      "383/572 [===================>..........] - ETA: 8s - loss: 0.3921"
     ]
    }
   ],
   "source": [
    "repeats = {k:10 for k in [10,20,30,40,50,60,70,80,90,100]}\n",
    "lds = [50,60]\n",
    "batch_size, lr, patience = 32, 0.0005, 10\n",
    "d, N, he, dropout = 50,2,4,0.2\n",
    "\n",
    "# best val model\n",
    "fore_savepath = 'Exp_Q/models/forecasting/forecasting_124_epochs.h5'\n",
    "\n",
    "train_inds = np.arange(len(train_op))\n",
    "valid_inds = np.arange(len(valid_op))\n",
    "gen_res = {}\n",
    "\n",
    "np.random.seed(2023)\n",
    "\n",
    "for ld in lds:\n",
    "    np.random.shuffle(train_inds)\n",
    "    np.random.shuffle(valid_inds)\n",
    "    train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "    valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "    all_test_res = []\n",
    "    for i in range(repeats[ld]):\n",
    "        print ('Repeat', i, 'ld', ld)\n",
    "        # Get train and validation data.\n",
    "        curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "        curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "        curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "        curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "        curr_train_op = train_op[curr_train_ind]\n",
    "        curr_valid_op = valid_op[curr_valid_ind]\n",
    "        print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "        # Construct save_path.\n",
    "        savepath = 'CLS/Q/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "        print (savepath)\n",
    "        # Build and compile model.\n",
    "        model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "        model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "        fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "        # Load pretrained weights here.\n",
    "        fore_model.load_weights(fore_savepath)\n",
    "        # Train model.\n",
    "        es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "                           restore_best_weights=True)\n",
    "        cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "        his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "                        verbose=1, callbacks=[cus, es]).history\n",
    "        model.save_weights(savepath)\n",
    "#         # Test and write to log.\n",
    "#         rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "\n",
    "#         print ('Test res', rocauc, prauc, minrp)\n",
    "#         all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "#         # in case of unexpected disconnection\n",
    "#         print(all_test_res)\n",
    "        \n",
    "#     gen_res[ld] = []\n",
    "#     for i in range(len(all_test_res[0])):\n",
    "#         nums = [test_res[i] for test_res in all_test_res]\n",
    "#         gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "#     print ('gen_res', gen_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeats = {k:10 for k in [10,20,30,40,50,60,70,80,90,100]}\n",
    "# lds = [100]\n",
    "# batch_size, lr, patience = 32, 0.0005, 10\n",
    "# d, N, he, dropout = 50,2,4,0.2\n",
    "\n",
    "# # best val model\n",
    "# fore_savepath = 'Exp1/Exp_M_Q/models/forecasting/forecasting_105_epochs.h5'\n",
    "\n",
    "# train_inds = np.arange(len(train_op))\n",
    "# valid_inds = np.arange(len(valid_op))\n",
    "# gen_res = {}\n",
    "\n",
    "# np.random.seed(2023)\n",
    "\n",
    "# for ld in lds:\n",
    "#     np.random.shuffle(train_inds)\n",
    "#     np.random.shuffle(valid_inds)\n",
    "#     train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "#     valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "#     all_test_res = []\n",
    "#     for i in range(repeats[ld]):\n",
    "#         print ('Repeat', i, 'ld', ld)\n",
    "#         # Get train and validation data.\n",
    "#         curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "#         curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "#         curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "#         curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "#         curr_train_op = train_op[curr_train_ind]\n",
    "#         curr_valid_op = valid_op[curr_valid_ind]\n",
    "#         print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "#         # Construct save_path.\n",
    "#         savepath = 'CLS/Q_M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "#         print (savepath)\n",
    "#         # Build and compile model.\n",
    "#         model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "#         model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "#         fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "#         # Load pretrained weights here.\n",
    "#         fore_model.load_weights(fore_savepath)\n",
    "#         # Train model.\n",
    "#         es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "#                            restore_best_weights=True)\n",
    "#         cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "#         his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "#                         verbose=1, callbacks=[cus, es]).history\n",
    "#         model.save_weights(savepath)\n",
    "#         # Test and write to log.\n",
    "#         rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "\n",
    "#         print ('Test res', rocauc, prauc, minrp)\n",
    "#         all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "#         # in case of unexpected disconnection\n",
    "#         print(all_test_res)\n",
    "        \n",
    "#     gen_res[ld] = []\n",
    "#     for i in range(len(all_test_res[0])):\n",
    "#         nums = [test_res[i] for test_res in all_test_res]\n",
    "#         gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "#     print ('gen_res', gen_res)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
