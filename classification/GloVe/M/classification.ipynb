{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECKAJiLXiTlN"
   },
   "source": [
    "## Hardware check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "e7odgm5VsOsb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 14 08:39:00 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  |   00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   30C    P0             41W /  300W |      17MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           On  |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             41W /  300W |      17MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-32GB           On  |   00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             41W /  300W |      17MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-32GB           On  |   00000000:B3:00.0 Off |                    0 |\n",
      "| N/A   35C    P0             42W /  300W |      17MiB /  32768MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1918      G   /usr/libexec/Xorg                              17MiB |\n",
      "|    1   N/A  N/A      1918      G   /usr/libexec/Xorg                              17MiB |\n",
      "|    2   N/A  N/A      1918      G   /usr/libexec/Xorg                              17MiB |\n",
      "|    3   N/A  N/A      1918      G   /usr/libexec/Xorg                              17MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# gpu check\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oEEUeIW3sOsc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check number of cores\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count() \n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY9I5eXWeOom"
   },
   "source": [
    "## Environment Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis\n"
     ]
    }
   ],
   "source": [
    "cd /pfs/data5/home/hd/hd_hd/hd_nf283/MA_Thesis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 08:39:00.868235: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-14 08:39:00.917605: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-14 08:39:02.048696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Lambda, GlobalAveragePooling1D\n",
    "# from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "import pandas as pd\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, pipeline, AutoModel\n",
    "import resources.smart_cond as sc\n",
    "# from google.colab import files\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36551it [00:00, 63485.61it/s]\n"
     ]
    }
   ],
   "source": [
    "train_path = 'CLS/GloVe/Data/classification_train.txt'\n",
    "train_text = []\n",
    "\n",
    "with open(train_path) as file:\n",
    "    for line in tqdm(file):\n",
    "        train_text.append(line.rstrip())\n",
    "\n",
    "# train_text = np.array(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9262it [00:00, 53674.09it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_path = 'CLS/GloVe/Data/classification_valid.txt'\n",
    "valid_text = []\n",
    "\n",
    "with open(valid_path) as file:\n",
    "    for line in tqdm(file):\n",
    "        valid_text.append(line.rstrip())\n",
    "\n",
    "# train_text = np.array(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11469it [00:00, 69753.32it/s]\n"
     ]
    }
   ],
   "source": [
    "test_path = 'CLS/GloVe/Data/classification_test.txt'\n",
    "test_text = []\n",
    "\n",
    "with open(test_path) as file:\n",
    "    for line in tqdm(file):\n",
    "        test_text.append(line.rstrip())\n",
    "\n",
    "# train_text = np.array(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 297321\n"
     ]
    }
   ],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "train_val_ip = train_text+valid_text\n",
    "t.fit_on_texts(train_val_ip)\n",
    "# vocab_size = len(t.word_index) + 1\n",
    "vocab_size = 297321\n",
    "print(f'vocabulary size: {vocab_size}')\n",
    "del train_val_ip\n",
    "\n",
    "# encode\n",
    "encoded_train = t.texts_to_sequences(train_text)\n",
    "encoded_valid = t.texts_to_sequences(valid_text)\n",
    "encoded_test = t.texts_to_sequences(test_text)\n",
    "\n",
    "\n",
    "# pad to max_len\n",
    "max_length = 24135\n",
    "padded_train = pad_sequences(encoded_train, maxlen=max_length, padding='post')\n",
    "padded_valid = pad_sequences(encoded_valid, maxlen=max_length, padding='post')\n",
    "padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Data/CLS/cls_data.pkl'\n",
    "train_ip, valid_ip, test_ip, train_op, valid_op, test_op = pickle.load(open(data_path, 'rb'))\n",
    "train_ip = train_ip[:4]\n",
    "valid_ip = valid_ip[:4]\n",
    "test_ip = test_ip[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ip.append(padded_train)\n",
    "valid_ip.append(padded_valid)\n",
    "test_ip.append(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196017it [01:29, 24490.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array, asarray, zeros\n",
    "\n",
    "# GloVe\n",
    "glove_path = 'resources/glove.840B.300d.txt'\n",
    "\n",
    "embedding_model = {}\n",
    "f = open('resources/glove.840B.300d.txt', 'r', encoding=\"utf8\")\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = ''.join(values[:-300])\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embedding_model[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289274/289274 [00:00<00:00, 551466.81it/s]\n"
     ]
    }
   ],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 300))\n",
    "for word, i in tqdm(t.word_index.items()):\n",
    "    embedding_vector = embedding_model.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_res(y_true, y_pred):\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    minrp = np.minimum(precision, recall).max()\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    return [roc_auc, pr_auc, minrp]\n",
    "\n",
    "######################################################################################################## \n",
    "######################################################################################################## \n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=[0,1], y=train_op)\n",
    "def mortality_loss(y_true, y_pred):\n",
    "    sample_weights = (1-y_true)*class_weights[0] + y_true*class_weights[1]\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    return K.mean(sample_weights*bce, axis=-1)\n",
    "######################################################################################################## \n",
    "######################################################################################################## \n",
    "\n",
    "# var_weights = np.sum(fore_train_op[:, V:], axis=0)\n",
    "# var_weights[var_weights==0] = var_weights.max()\n",
    "# var_weights = var_weights.max()/var_weights\n",
    "# var_weights = var_weights.reshape((1, V))\n",
    "def forecast_loss(y_true, y_pred):\n",
    "    return K.sum(y_true[:,V:]*(y_true[:,:V]-y_pred)**2, axis=-1)\n",
    "\n",
    "def get_min_loss(weight):\n",
    "    def min_loss(y_true, y_pred):\n",
    "        return weight*y_pred\n",
    "    return min_loss\n",
    "\n",
    "class CustomCallback(Callback):\n",
    "    def __init__(self, validation_data, batch_size):\n",
    "        self.val_x, self.val_y = validation_data\n",
    "        self.batch_size = batch_size\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.val_x, verbose=0, batch_size=self.batch_size)\n",
    "        if type(y_pred)==type([]):\n",
    "            y_pred = y_pred[0]\n",
    "        precision, recall, thresholds = precision_recall_curve(self.val_y, y_pred)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(self.val_y, y_pred)\n",
    "        logs['custom_metric'] = pr_auc + roc_auc\n",
    "        print ('val_aucs:', pr_auc, roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding, Activation, Dropout, Softmax, Layer, InputSpec, Input, Dense, Lambda, TimeDistributed, Concatenate, Add\n",
    "from tensorflow.keras import initializers, regularizers, constraints, Model\n",
    "from tensorflow.python.keras.utils import tf_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow import nn\n",
    "\n",
    "    \n",
    "class CVE(Layer):\n",
    "    def __init__(self, hid_units, output_dim):\n",
    "        self.hid_units = hid_units\n",
    "        self.output_dim = output_dim\n",
    "        super(CVE, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape): \n",
    "        self.W1 = self.add_weight(name='CVE_W1',\n",
    "                            shape=(1, self.hid_units),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        self.b1 = self.add_weight(name='CVE_b1',\n",
    "                            shape=(self.hid_units,),\n",
    "                            initializer='zeros',\n",
    "                            trainable=True)\n",
    "        self.W2 = self.add_weight(name='CVE_W2',\n",
    "                            shape=(self.hid_units, self.output_dim),\n",
    "                            initializer='glorot_uniform',\n",
    "                            trainable=True)\n",
    "        super(CVE, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = K.expand_dims(x, axis=-1)\n",
    "        x = K.dot(K.tanh(K.bias_add(K.dot(x, self.W1), self.b1)), self.W2)\n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape + (self.output_dim,)\n",
    "    \n",
    "    \n",
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, hid_dim):\n",
    "        self.hid_dim = hid_dim\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        self.W = self.add_weight(shape=(d, self.hid_dim), name='Att_W',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.hid_dim,), name='Att_b',\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(shape=(self.hid_dim,1), name='Att_u',\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e30):\n",
    "        attn_weights = K.dot(K.tanh(K.bias_add(K.dot(x,self.W), self.b)), self.u)\n",
    "        mask = K.expand_dims(mask, axis=-1)\n",
    "        attn_weights = mask*attn_weights + (1-mask)*mask_value\n",
    "        attn_weights = K.softmax(attn_weights, axis=-2)\n",
    "        return attn_weights\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1] + (1,)\n",
    "    \n",
    "    \n",
    "class Transformer(Layer):\n",
    "    \n",
    "    def __init__(self, N=2, h=8, dk=None, dv=None, dff=None, dropout=0):\n",
    "        self.N, self.h, self.dk, self.dv, self.dff, self.dropout = N, h, dk, dv, dff, dropout\n",
    "        self.epsilon = K.epsilon() * K.epsilon()\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = input_shape.as_list()[-1]\n",
    "        if self.dk==None:\n",
    "            self.dk = d//self.h\n",
    "        if self.dv==None:\n",
    "            self.dv = d//self.h\n",
    "        if self.dff==None:\n",
    "            self.dff = 2*d\n",
    "        self.Wq = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wq',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wk = self.add_weight(shape=(self.N, self.h, d, self.dk), name='Wk',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wv = self.add_weight(shape=(self.N, self.h, d, self.dv), name='Wv',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.Wo = self.add_weight(shape=(self.N, self.dv*self.h, d), name='Wo',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.W1 = self.add_weight(shape=(self.N, d, self.dff), name='W1',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b1 = self.add_weight(shape=(self.N, self.dff), name='b1',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.W2 = self.add_weight(shape=(self.N, self.dff, d), name='W2',\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b2 = self.add_weight(shape=(self.N, d), name='b2',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        self.gamma = self.add_weight(shape=(2*self.N,), name='gamma',\n",
    "                                 initializer='ones', trainable=True)\n",
    "        self.beta = self.add_weight(shape=(2*self.N,), name='beta',\n",
    "                                 initializer='zeros', trainable=True)\n",
    "        super(Transformer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x, mask, mask_value=-1e-30):\n",
    "        mask = K.expand_dims(mask, axis=-2)\n",
    "        for i in range(self.N):\n",
    "            # MHA\n",
    "            mha_ops = []\n",
    "            for j in range(self.h):\n",
    "                q = K.dot(x, self.Wq[i,j,:,:])\n",
    "                k = K.permute_dimensions(K.dot(x, self.Wk[i,j,:,:]), (0,2,1))\n",
    "                v = K.dot(x, self.Wv[i,j,:,:])\n",
    "                A = K.batch_dot(q,k)\n",
    "                # Mask unobserved steps.\n",
    "                A = mask*A + (1-mask)*mask_value\n",
    "                # Mask for attention dropout.\n",
    "                def dropped_A():\n",
    "                    dp_mask = K.cast((K.random_uniform(shape=array_ops.shape(A))>=self.dropout), K.floatx())\n",
    "                    return A*dp_mask + (1-dp_mask)*mask_value\n",
    "                A = sc.smart_cond(K.learning_phase(), dropped_A, lambda: array_ops.identity(A))\n",
    "                A = K.softmax(A, axis=-1)\n",
    "                mha_ops.append(K.batch_dot(A,v))\n",
    "            conc = K.concatenate(mha_ops, axis=-1)\n",
    "            proj = K.dot(conc, self.Wo[i,:,:])\n",
    "            # Dropout.\n",
    "            proj = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(proj, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(proj))\n",
    "            # Add & LN\n",
    "            x = x+proj\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i] + self.beta[2*i]\n",
    "            # FFN\n",
    "            ffn_op = K.bias_add(K.dot(K.relu(K.bias_add(K.dot(x, self.W1[i,:,:]), self.b1[i,:])), \n",
    "                           self.W2[i,:,:]), self.b2[i,:,])\n",
    "            # Dropout.\n",
    "            ffn_op = sc.smart_cond(K.learning_phase(), lambda: array_ops.identity(nn.dropout(ffn_op, rate=self.dropout)),\\\n",
    "                                       lambda: array_ops.identity(ffn_op))\n",
    "            # Add & LN\n",
    "            x = x+ffn_op\n",
    "            mean = K.mean(x, axis=-1, keepdims=True)\n",
    "            variance = K.mean(K.square(x - mean), axis=-1, keepdims=True)\n",
    "            std = K.sqrt(variance + self.epsilon)\n",
    "            x = (x - mean) / std\n",
    "            x = x*self.gamma[2*i+1] + self.beta[2*i+1]            \n",
    "        return x\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "def build_strats(D, max_len, V, d, N, he, dropout, forecast=False):\n",
    "    demo = Input(shape=(D,))\n",
    "    demo_enc = Dense(2*d, activation='tanh')(demo)\n",
    "    demo_enc = Dense(d, activation='tanh')(demo_enc)\n",
    "    varis = Input(shape=(max_len,))\n",
    "    values = Input(shape=(max_len,))\n",
    "    times = Input(shape=(max_len,))\n",
    "    varis_emb = Embedding(V+1, d)(varis)\n",
    "    cve_units = int(np.sqrt(d))\n",
    "    values_emb = CVE(cve_units, d)(values)\n",
    "    times_emb = CVE(cve_units, d)(times)\n",
    "    comb_emb = Add()([varis_emb, values_emb, times_emb]) # b, L, d\n",
    "#     demo_enc = Lambda(lambda x:K.expand_dims(x, axis=-2))(demo_enc) # b, 1, d\n",
    "#     comb_emb = Concatenate(axis=-2)([demo_enc, comb_emb]) # b, L+1, d\n",
    "    mask = Lambda(lambda x:K.clip(x,0,1))(varis) # b, L\n",
    "#     mask = Lambda(lambda x:K.concatenate((K.ones_like(x)[:,0:1], x), axis=-1))(mask) # b, L+1\n",
    "    cont_emb = Transformer(N, he, dk=None, dv=None, dff=None, dropout=dropout)(comb_emb, mask=mask)\n",
    "    attn_weights = Attention(2*d)(cont_emb, mask=mask)\n",
    "    fused_emb = Lambda(lambda x:K.sum(x[0]*x[1], axis=-2))([cont_emb, attn_weights])\n",
    "    \n",
    "    \n",
    "    ## GloVe text embedding starts here\n",
    "    max_length = 24135\n",
    "    # embed text input\n",
    "    texts = Input(shape=(max_length,))\n",
    "    embedded_text = Embedding(vocab_size, 300, weights=[\n",
    "    embedding_matrix], input_length=max_length, trainable=False)(texts)\n",
    "    pooled = GlobalAveragePooling1D()(embedded_text)\n",
    "    # flattened = Flatten()(embedded_text)\n",
    "    \n",
    "    text_0 = Dense(100, activation='relu')(pooled)\n",
    "    \n",
    "    \n",
    "    # hidden_states = LSTM(64, return_sequences=True, name='lstm_layer')(embedded_text)\n",
    "    # pooled = AveragePooling1D()(hidden_states)\n",
    "    # text_0 = Dense(100, activation='relu')(hidden_states)\n",
    "    \n",
    "    # text_1 = Flatten()(hidden_states)\n",
    "    \n",
    "    # text_enc = Dense(22528, activation='relu')(texts)\n",
    "    # text_enc = Dense(10000, activation='relu')(texts)\n",
    "    # text_enc = Dense(5000, activation='relu')(texts)\n",
    "    # text_enc = Dense(1000, activation='relu')(texts)\n",
    "    \n",
    "    # text_enc = Dense(d, activation='relu')(text_1)\n",
    "    \n",
    "    text_enc = Dense(d, activation='relu')(text_0)\n",
    "    \n",
    "    # text_enc = Dense(d * max_length, activation='relu')(text_0)\n",
    "    # reshaped_text_enc = Reshape((max_length, d))(text_enc)\n",
    "    \n",
    "    # reshaped_text_enc = Reshape((d,))(text_enc)\n",
    "    \n",
    "    # conc = Concatenate(axis=-1)([fused_emb, text_enc, demo_enc])\n",
    "    conc = Concatenate(axis=-1)([fused_emb, text_enc, demo_enc])\n",
    "    fore_op = Dense(V)(conc)\n",
    "    op = Dense(1, activation='sigmoid')(fore_op)\n",
    "    model = Model([demo, times, values, varis, texts], op)\n",
    "    if forecast:\n",
    "        fore_model = Model([demo, times, values, varis, texts], fore_op)\n",
    "        return [model, fore_model]\n",
    "    return model\n",
    "\n",
    "# To tune:\n",
    "# 1. Transformer parameters. (N, h, dropout)\n",
    "# 2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "D = 2\n",
    "max_len = 880\n",
    "V = 134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 0 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/GloVe/M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_50ld.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 08:41:39.714129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31117 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3a:00.0, compute capability: 7.0\n",
      "2024-06-14 08:41:39.714807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 31117 MB memory:  -> device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0\n",
      "2024-06-14 08:41:39.715336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 31117 MB memory:  -> device: 2, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b2:00.0, compute capability: 7.0\n",
      "2024-06-14 08:41:39.715884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 31117 MB memory:  -> device: 3, name: Tesla V100-SXM2-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-14 08:41:56.541139: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x14c6b1bae330 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-14 08:41:56.541180: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2024-06-14 08:41:56.541187: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (1): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2024-06-14 08:41:56.541192: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (2): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2024-06-14 08:41:56.541197: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (3): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2024-06-14 08:41:56.552598: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-06-14 08:41:56.642692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8700\n",
      "2024-06-14 08:41:57.001671: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "572/572 [==============================] - ETA: 0s - loss: 0.4760val_aucs: 0.47093636228993097 0.8786996242491599\n",
      "572/572 [==============================] - 46s 57ms/step - loss: 0.4760 - custom_metric: 1.3496\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4179val_aucs: 0.5056711801038498 0.8877433339507644\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4179 - custom_metric: 1.3934\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3876val_aucs: 0.5313564975735605 0.8952467739232446\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3876 - custom_metric: 1.4266\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3709val_aucs: 0.5165124090030826 0.8883618674640346\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3709 - custom_metric: 1.4049\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3491val_aucs: 0.49996804116759397 0.8834648681784905\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3491 - custom_metric: 1.3834\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3352val_aucs: 0.4916365249785385 0.884854639111605\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3352 - custom_metric: 1.3765\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4702val_aucs: 0.5072227250960302 0.9000612349782944\n",
      "572/572 [==============================] - 39s 54ms/step - loss: 0.4702 - custom_metric: 1.4073\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4189val_aucs: 0.5268235932753766 0.9024843905479436\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4189 - custom_metric: 1.4293\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3952val_aucs: 0.5547371580620825 0.9089137898984155\n",
      "572/572 [==============================] - 29s 51ms/step - loss: 0.3952 - custom_metric: 1.4637\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3758val_aucs: 0.5514030576609206 0.9027550272823698\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3758 - custom_metric: 1.4542\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3633val_aucs: 0.5472510219573086 0.9051951318192256\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3633 - custom_metric: 1.4524\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3430val_aucs: 0.5598808467764327 0.9016735738264208\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3430 - custom_metric: 1.4616\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3258val_aucs: 0.5381337618624837 0.8971033668303244\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3258 - custom_metric: 1.4352\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3085val_aucs: 0.5283640512265924 0.8972613749439591\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3085 - custom_metric: 1.4256\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2925val_aucs: 0.4965661462905636 0.8896627702266787\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2925 - custom_metric: 1.3862\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2752val_aucs: 0.49517318681877814 0.8848700396933877\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2752 - custom_metric: 1.3800\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2552val_aucs: 0.5078685847590605 0.884828487386688\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2552 - custom_metric: 1.3927\n",
      "Epoch 12/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2287val_aucs: 0.4754789674711775 0.8792440760625909\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2287 - custom_metric: 1.3547\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2188val_aucs: 0.4918136795776773 0.8839608096138916\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.2188 - custom_metric: 1.3758\n",
      "Repeat 2 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/GloVe/M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat2_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4636val_aucs: 0.5091024306193085 0.8922415399969537\n",
      "572/572 [==============================] - 38s 55ms/step - loss: 0.4636 - custom_metric: 1.4013\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4051val_aucs: 0.527839400875353 0.9019932705550034\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4051 - custom_metric: 1.4298\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3792val_aucs: 0.5230998996722451 0.9000882986689163\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3792 - custom_metric: 1.4232\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3589val_aucs: 0.5276248923262221 0.9045453044599882\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3589 - custom_metric: 1.4322\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3433val_aucs: 0.5291947923020082 0.899410348092738\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3433 - custom_metric: 1.4286\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3293val_aucs: 0.5069299384505104 0.8932944137511595\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3293 - custom_metric: 1.4002\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3152val_aucs: 0.4964020301813439 0.888827821856104\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3152 - custom_metric: 1.3852\n",
      "Epoch 8/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2924val_aucs: 0.4829851945855039 0.8866432552704933\n",
      "572/572 [==============================] - 28s 49ms/step - loss: 0.2924 - custom_metric: 1.3696\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2783val_aucs: 0.4905590904480559 0.8874751160717423\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.2783 - custom_metric: 1.3780\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2725val_aucs: 0.46948010012167846 0.8856982681424848\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2725 - custom_metric: 1.3552\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2587val_aucs: 0.4637943699039103 0.8803483914453821\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2587 - custom_metric: 1.3441\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2442val_aucs: 0.42985843306038346 0.8769423041817127\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2442 - custom_metric: 1.3068\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2229val_aucs: 0.40597928943756745 0.8690126802690521\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2229 - custom_metric: 1.2750\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2163val_aucs: 0.4523165182010545 0.8786246893123446\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.2163 - custom_metric: 1.3309\n",
      "Repeat 4 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/GloVe/M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat4_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4667val_aucs: 0.4872347171058636 0.8890533989703507\n",
      "572/572 [==============================] - 39s 54ms/step - loss: 0.4667 - custom_metric: 1.3763\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4066val_aucs: 0.5032927926251142 0.8998993434508823\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4066 - custom_metric: 1.4032\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3894val_aucs: 0.5082716816709569 0.8968954836077455\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3894 - custom_metric: 1.4052\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3707val_aucs: 0.5137656058259973 0.8953429569941539\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3707 - custom_metric: 1.4091\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3505val_aucs: 0.5175218683390497 0.8933561309233023\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3505 - custom_metric: 1.4109\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3329val_aucs: 0.4933508583336179 0.89248860567864\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3329 - custom_metric: 1.3858\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3024val_aucs: 0.4779321021232891 0.8844189030315068\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3024 - custom_metric: 1.3624\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2957val_aucs: 0.4956726768288095 0.8915861861636166\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2957 - custom_metric: 1.3873\n",
      "Epoch 9/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2668val_aucs: 0.4808256085850039 0.8840286240383944\n",
      "572/572 [==============================] - 28s 49ms/step - loss: 0.2668 - custom_metric: 1.3649\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2542val_aucs: 0.4456500478388528 0.879931499863107\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2542 - custom_metric: 1.3256\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2444val_aucs: 0.4846135110207168 0.8817057393022221\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.2444 - custom_metric: 1.3663\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2206val_aucs: 0.47903854148307945 0.8819983143383241\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2206 - custom_metric: 1.3610\n",
      "Epoch 13/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2159val_aucs: 0.4604565757640467 0.8773342924784058\n",
      "572/572 [==============================] - 28s 49ms/step - loss: 0.2159 - custom_metric: 1.3378\n",
      "Epoch 14/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2003val_aucs: 0.45924315709001035 0.877336976653049\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2003 - custom_metric: 1.3366\n",
      "Epoch 15/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1865val_aucs: 0.44054956498760145 0.8709260939353758\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.1865 - custom_metric: 1.3115\n",
      "Repeat 5 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/GloVe/M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4714val_aucs: 0.5458293091522506 0.8965460040692087\n",
      "572/572 [==============================] - 38s 55ms/step - loss: 0.4714 - custom_metric: 1.4424\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4035val_aucs: 0.5249986133685289 0.8993337878535729\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4035 - custom_metric: 1.4243\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3827val_aucs: 0.5265802722352252 0.9020823826881472\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3827 - custom_metric: 1.4287\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3607val_aucs: 0.5214219213248549 0.897983648008074\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3607 - custom_metric: 1.4194\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3507val_aucs: 0.5092515732448291 0.8963709958824762\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3507 - custom_metric: 1.4056\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3295val_aucs: 0.49156040190831335 0.8879340981441618\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3295 - custom_metric: 1.3795\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3089val_aucs: 0.5014370514893002 0.885112493759294\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3089 - custom_metric: 1.3865\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2999val_aucs: 0.4944947318935435 0.8857846110899359\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2999 - custom_metric: 1.3803\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2816val_aucs: 0.4229288469216041 0.8678349984163372\n",
      "572/572 [==============================] - 28s 49ms/step - loss: 0.2816 - custom_metric: 1.2908\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2652val_aucs: 0.48460172718450273 0.881681850147898\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2652 - custom_metric: 1.3663\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2366val_aucs: 0.44339330524545495 0.8710764077153916\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2366 - custom_metric: 1.3145\n",
      "Repeat 6 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/GloVe/M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4631val_aucs: 0.502883122127548 0.8910292385872967\n",
      "572/572 [==============================] - 39s 55ms/step - loss: 0.4631 - custom_metric: 1.3939\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4084val_aucs: 0.4977020361792529 0.8985895246077298\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4084 - custom_metric: 1.3963\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3805val_aucs: 0.5217905678275543 0.902901579690003\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3805 - custom_metric: 1.4247\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3736val_aucs: 0.5178971634154432 0.8957383379836408\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3736 - custom_metric: 1.4136\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3526val_aucs: 0.5289308794607783 0.9061346927083886\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3526 - custom_metric: 1.4351\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3301val_aucs: 0.5224389625218104 0.8958682990483668\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3301 - custom_metric: 1.4183\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3119val_aucs: 0.4907913379660381 0.8960932112583414\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3119 - custom_metric: 1.3869\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2945val_aucs: 0.5041002289695821 0.8906560646728696\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.2945 - custom_metric: 1.3948\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2724val_aucs: 0.4759756014325612 0.8806373927158149\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2724 - custom_metric: 1.3566\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2586val_aucs: 0.4914216987384398 0.8907112318187123\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2586 - custom_metric: 1.3821\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2354val_aucs: 0.5010092868584509 0.8830536076130662\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2354 - custom_metric: 1.3841\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2297val_aucs: 0.47685252214087315 0.8796008869179601\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2297 - custom_metric: 1.3565\n",
      "Epoch 13/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2120val_aucs: 0.4491424125202963 0.8735277267953192\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2120 - custom_metric: 1.3227\n",
      "Epoch 14/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1808val_aucs: 0.44603093735569244 0.8734858209826117\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.1808 - custom_metric: 1.3195\n",
      "Epoch 15/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.1703val_aucs: 0.42203887312522204 0.8691626263805048\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.1703 - custom_metric: 1.2912\n",
      "Repeat 7 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/GloVe/M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat7_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4792val_aucs: 0.5058938880176601 0.8864396783750187\n",
      "572/572 [==============================] - 38s 55ms/step - loss: 0.4792 - custom_metric: 1.3923\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4221val_aucs: 0.5435063902181138 0.9007574746890796\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4221 - custom_metric: 1.4443\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3967val_aucs: 0.5574603052611767 0.9014557617417049\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3967 - custom_metric: 1.4589\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3772val_aucs: 0.5493915246624321 0.9004678893837074\n",
      "572/572 [==============================] - 37s 64ms/step - loss: 0.3772 - custom_metric: 1.4499\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3660val_aucs: 0.5480207065440422 0.9033131841254313\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3660 - custom_metric: 1.4513\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3415val_aucs: 0.5563846527564951 0.9014007034752355\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3415 - custom_metric: 1.4578\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3260val_aucs: 0.5437612780779109 0.8996044275316744\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3260 - custom_metric: 1.4434\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3169val_aucs: 0.5188990347236858 0.8944517149591193\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3169 - custom_metric: 1.4134\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2919val_aucs: 0.5320158074122935 0.8894440597424544\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2919 - custom_metric: 1.4215\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2689val_aucs: 0.5004215383675699 0.8814209056237785\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2689 - custom_metric: 1.3818\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2627val_aucs: 0.4985016773131262 0.8823463080255894\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2627 - custom_metric: 1.3808\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2374val_aucs: 0.48838839252479344 0.8748552073473138\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.2374 - custom_metric: 1.3632\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2319val_aucs: 0.45892429749859864 0.8804965620347841\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.2319 - custom_metric: 1.3394\n",
      "Repeat 8 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/GloVe/M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat8_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4728val_aucs: 0.52103193479295 0.8966654035619553\n",
      "572/572 [==============================] - 39s 55ms/step - loss: 0.4728 - custom_metric: 1.4177\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4192val_aucs: 0.526190159386316 0.9022541156161846\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4192 - custom_metric: 1.4284\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3955val_aucs: 0.5456628000534021 0.9053329333501747\n",
      "572/572 [==============================] - 29s 51ms/step - loss: 0.3955 - custom_metric: 1.4510\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3736val_aucs: 0.5300262477237998 0.9003347227485158\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3736 - custom_metric: 1.4304\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3595val_aucs: 0.5322234776821193 0.8971306471306469\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3595 - custom_metric: 1.4294\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3481val_aucs: 0.5298364842440271 0.8967175066312997\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3481 - custom_metric: 1.4266\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3267val_aucs: 0.46122516599292196 0.8857211275314725\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3267 - custom_metric: 1.3469\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3135val_aucs: 0.5027808861750264 0.8893709738537324\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3135 - custom_metric: 1.3922\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2889val_aucs: 0.49715487211910214 0.8847953770367563\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2889 - custom_metric: 1.3820\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2733val_aucs: 0.4707899433415664 0.8826702033598586\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2733 - custom_metric: 1.3535\n",
      "Epoch 11/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2544val_aucs: 0.4325061497278808 0.8729148246389625\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2544 - custom_metric: 1.3054\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2409val_aucs: 0.4873446884587523 0.883046240158309\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2409 - custom_metric: 1.3704\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2186val_aucs: 0.47578721047497013 0.8780309250136837\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.2186 - custom_metric: 1.3538\n",
      "Repeat 9 ld 50\n",
      "Num train: 18275 Num valid: 4631\n",
      "CLS/GloVe/M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat9_50ld.h5\n",
      "Epoch 1/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4748val_aucs: 0.5040171293522101 0.8894586377918421\n",
      "572/572 [==============================] - 38s 55ms/step - loss: 0.4748 - custom_metric: 1.3935\n",
      "Epoch 2/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.4209val_aucs: 0.5266987250673308 0.9020299045158345\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.4209 - custom_metric: 1.4287\n",
      "Epoch 3/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3966val_aucs: 0.5500395775728628 0.9030843871367384\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.3966 - custom_metric: 1.4531\n",
      "Epoch 4/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3785val_aucs: 0.5251540701129046 0.8979484964198532\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3785 - custom_metric: 1.4231\n",
      "Epoch 5/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3608val_aucs: 0.5380425190698698 0.8992622696948827\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3608 - custom_metric: 1.4373\n",
      "Epoch 6/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3427val_aucs: 0.5310545970901315 0.8975226280763385\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3427 - custom_metric: 1.4286\n",
      "Epoch 7/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3310val_aucs: 0.5225090565414267 0.8963606595457676\n",
      "572/572 [==============================] - 30s 53ms/step - loss: 0.3310 - custom_metric: 1.4189\n",
      "Epoch 8/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.3040val_aucs: 0.5170254540270205 0.8898050378798684\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.3040 - custom_metric: 1.4068\n",
      "Epoch 9/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2892val_aucs: 0.4771373631015583 0.8835107445156716\n",
      "572/572 [==============================] - 29s 50ms/step - loss: 0.2892 - custom_metric: 1.3606\n",
      "Epoch 10/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2711val_aucs: 0.5087545665774569 0.8899838414547173\n",
      "572/572 [==============================] - 28s 50ms/step - loss: 0.2711 - custom_metric: 1.3987\n",
      "Epoch 11/1000\n",
      "571/572 [============================>.] - ETA: 0s - loss: 0.2567val_aucs: 0.48552721561799417 0.8854964320790933\n",
      "572/572 [==============================] - 28s 49ms/step - loss: 0.2567 - custom_metric: 1.3710\n",
      "Epoch 12/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2400val_aucs: 0.5132571838565111 0.8902869415317405\n",
      "572/572 [==============================] - 30s 53ms/step - loss: 0.2400 - custom_metric: 1.4035\n",
      "Epoch 13/1000\n",
      "572/572 [==============================] - ETA: 0s - loss: 0.2216val_aucs: 0.4775956982731114 0.8814761534104107\n",
      "572/572 [==============================] - 34s 59ms/step - loss: 0.2216 - custom_metric: 1.3591\n",
      "Repeat 0 ld 60\n",
      "Num train: 21930 Num valid: 5557\n",
      "CLS/GloVe/M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat0_60ld.h5\n",
      "Epoch 1/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.4674val_aucs: 0.5188149885200024 0.8942731989471837\n",
      "686/686 [==============================] - 44s 54ms/step - loss: 0.4674 - custom_metric: 1.4131\n",
      "Epoch 2/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.4144val_aucs: 0.5457378805443214 0.9016448235915496\n",
      "686/686 [==============================] - 35s 51ms/step - loss: 0.4143 - custom_metric: 1.4474\n",
      "Epoch 3/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3974val_aucs: 0.5332813253529661 0.9031672642567661\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3974 - custom_metric: 1.4364\n",
      "Epoch 4/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3792val_aucs: 0.5116847433133634 0.8972490286865286\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3792 - custom_metric: 1.4089\n",
      "Epoch 5/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3581val_aucs: 0.5395694536986835 0.8984166617081748\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3581 - custom_metric: 1.4380\n",
      "Epoch 6/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3434val_aucs: 0.5071667997460004 0.8916532787385009\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3434 - custom_metric: 1.3988\n",
      "Epoch 7/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.3280val_aucs: 0.5192717491178098 0.8967928474305096\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3280 - custom_metric: 1.4161\n",
      "Epoch 8/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3116val_aucs: 0.5189819088694784 0.8890976352768418\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3116 - custom_metric: 1.4081\n",
      "Epoch 9/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.3004val_aucs: 0.5397506317092888 0.8911585314342677\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3004 - custom_metric: 1.4309\n",
      "Epoch 10/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.2794val_aucs: 0.5100318243709014 0.8873673052469659\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2794 - custom_metric: 1.3974\n",
      "Epoch 11/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.2699val_aucs: 0.5160732829551401 0.8812466897475283\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2699 - custom_metric: 1.3973\n",
      "Epoch 12/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.2475val_aucs: 0.49022096842346286 0.8724925640986085\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2475 - custom_metric: 1.3627\n",
      "Repeat 1 ld 60\n",
      "Num train: 21930 Num valid: 5557\n",
      "CLS/GloVe/M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat1_60ld.h5\n",
      "Epoch 1/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.4668val_aucs: 0.5050305769826856 0.8954355305167641\n",
      "686/686 [==============================] - 45s 55ms/step - loss: 0.4668 - custom_metric: 1.4005\n",
      "Epoch 2/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.4152val_aucs: 0.5284620078757181 0.8984788418281462\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.4152 - custom_metric: 1.4269\n",
      "Epoch 3/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.4019val_aucs: 0.5421437411612129 0.9019024518951554\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.4019 - custom_metric: 1.4440\n",
      "Epoch 4/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3801val_aucs: 0.5496755967660889 0.900307187056871\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3801 - custom_metric: 1.4500\n",
      "Epoch 5/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3709val_aucs: 0.527385289165338 0.8984355423089736\n",
      "686/686 [==============================] - 35s 51ms/step - loss: 0.3709 - custom_metric: 1.4258\n",
      "Epoch 6/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3522val_aucs: 0.5278298374162985 0.898574100770326\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3522 - custom_metric: 1.4264\n",
      "Epoch 7/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.3334val_aucs: 0.49837248898198594 0.8915584731447169\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3334 - custom_metric: 1.3899\n",
      "Epoch 8/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.3188val_aucs: 0.529346785786014 0.8985987907089182\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3188 - custom_metric: 1.4279\n",
      "Epoch 9/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3051val_aucs: 0.5207269517282136 0.8924322021656392\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3051 - custom_metric: 1.4132\n",
      "Epoch 10/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.2887val_aucs: 0.4924707580589416 0.8824666796380972\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2887 - custom_metric: 1.3749\n",
      "Epoch 11/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.2900val_aucs: 0.505123832529737 0.8885337714139153\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2901 - custom_metric: 1.3937\n",
      "Epoch 12/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.2631val_aucs: 0.47716669412847207 0.8838028843744381\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2631 - custom_metric: 1.3610\n",
      "Epoch 13/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.2492val_aucs: 0.4815705505905695 0.8864174383193741\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2492 - custom_metric: 1.3680\n",
      "Epoch 14/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.3503val_aucs: 0.5258277045247314 0.897419975670425\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3504 - custom_metric: 1.4232\n",
      "Epoch 7/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3420val_aucs: 0.5350083446426821 0.894680538155673\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3420 - custom_metric: 1.4297\n",
      "Epoch 8/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.3224val_aucs: 0.5263868029626391 0.8923427201917317\n",
      "686/686 [==============================] - 34s 49ms/step - loss: 0.3225 - custom_metric: 1.4187\n",
      "Epoch 9/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.3032val_aucs: 0.5238369281080446 0.9004624434882074\n",
      "686/686 [==============================] - 34s 49ms/step - loss: 0.3032 - custom_metric: 1.4243\n",
      "Epoch 10/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.2914val_aucs: 0.5115370407002438 0.8825386277393469\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2914 - custom_metric: 1.3941\n",
      "Epoch 11/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.2828val_aucs: 0.5036016737284584 0.8851182889409372\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2827 - custom_metric: 1.3887\n",
      "Epoch 12/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.2607val_aucs: 0.4963416870262017 0.8850224231530404\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2607 - custom_metric: 1.3814\n",
      "Epoch 13/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.2534val_aucs: 0.4640621853494745 0.8729364344463205\n",
      "686/686 [==============================] - 34s 49ms/step - loss: 0.2534 - custom_metric: 1.3370\n",
      "Epoch 14/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.2423val_aucs: 0.4781953656902205 0.8839183325162954\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2423 - custom_metric: 1.3621\n",
      "Epoch 15/1000\n",
      " 75/686 [==>...........................] - ETA: 27s - loss: 0.2030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685/686 [============================>.] - ETA: 0s - loss: 0.2390val_aucs: 0.5183828806549199 0.8800822013536928\n",
      "686/686 [==============================] - 34s 49ms/step - loss: 0.2390 - custom_metric: 1.3985\n",
      "Epoch 14/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.2206val_aucs: 0.49419306347834974 0.8681147360875469\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2206 - custom_metric: 1.3623\n",
      "Repeat 5 ld 60\n",
      "Num train: 21930 Num valid: 5557\n",
      "CLS/GloVe/M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat5_60ld.h5\n",
      "Epoch 1/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.4655val_aucs: 0.5070464306046499 0.893249087640939\n",
      "686/686 [==============================] - 45s 54ms/step - loss: 0.4655 - custom_metric: 1.4003\n",
      "Epoch 2/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.4139val_aucs: 0.5466156667005292 0.9014031265319461\n",
      "686/686 [==============================] - 35s 50ms/step - loss: 0.4139 - custom_metric: 1.4480\n",
      "Epoch 3/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3942val_aucs: 0.5457971533591451 0.9021016032100515\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3942 - custom_metric: 1.4479\n",
      "Epoch 4/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.3788val_aucs: 0.5575143000771692 0.903781614830147\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3787 - custom_metric: 1.4613\n",
      "Epoch 5/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.3641val_aucs: 0.5395148651423788 0.8991012582384661\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3642 - custom_metric: 1.4386\n",
      "Epoch 6/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.3519val_aucs: 0.5496625841072593 0.8948726328594514\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3518 - custom_metric: 1.4445\n",
      "Epoch 7/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.3354val_aucs: 0.527487983238626 0.8941398405868148\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3354 - custom_metric: 1.4216\n",
      "Epoch 8/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3228val_aucs: 0.53864244571758 0.8940610418141873\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3228 - custom_metric: 1.4327\n",
      "Epoch 9/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3039val_aucs: 0.5222288254874015 0.8934600649999092\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3039 - custom_metric: 1.4157\n",
      "Epoch 10/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.2936val_aucs: 0.5263786326567261 0.8953973527969932\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2936 - custom_metric: 1.4218\n",
      "Epoch 11/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.2758val_aucs: 0.4936204511999139 0.8861768070156326\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2758 - custom_metric: 1.3798\n",
      "Epoch 12/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.2658val_aucs: 0.4965398749714904 0.8852275904642591\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2658 - custom_metric: 1.3818\n",
      "Epoch 13/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.2566val_aucs: 0.5054276313954983 0.8836614194672912\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2566 - custom_metric: 1.3891\n",
      "Epoch 14/1000\n",
      "685/686 [============================>.] - ETA: 0s - loss: 0.2384val_aucs: 0.48170001558567666 0.8807956134139477\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.2384 - custom_metric: 1.3625\n",
      "Repeat 6 ld 60\n",
      "Num train: 21930 Num valid: 5557\n",
      "CLS/GloVe/M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat6_60ld.h5\n",
      "Epoch 1/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.4581val_aucs: 0.5179368091984593 0.8904567902568703\n",
      "686/686 [==============================] - 43s 54ms/step - loss: 0.4581 - custom_metric: 1.4084\n",
      "Epoch 2/1000\n",
      "463/686 [===================>..........] - ETA: 9s - loss: 0.4095 Epoch 1/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.4585val_aucs: 0.5225353949491295 0.8933380073745985\n",
      "686/686 [==============================] - 45s 54ms/step - loss: 0.4585 - custom_metric: 1.4159\n",
      "Epoch 2/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.4089val_aucs: 0.5261903121716965 0.8945313717198247\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.4089 - custom_metric: 1.4207\n",
      "Epoch 3/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3862val_aucs: 0.5473523927786587 0.897288685110582\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3862 - custom_metric: 1.4446\n",
      "Epoch 4/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3719val_aucs: 0.5191959241776076 0.8946923664745116\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3719 - custom_metric: 1.4139\n",
      "Epoch 5/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3539val_aucs: 0.5319640069843957 0.893425923865061\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3539 - custom_metric: 1.4254\n",
      "Epoch 6/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3383val_aucs: 0.5132758099681998 0.8894040226431332\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3383 - custom_metric: 1.4027\n",
      "Epoch 7/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3234val_aucs: 0.5299369350610685 0.8898870069071945\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3234 - custom_metric: 1.4198\n",
      "Epoch 8/1000\n",
      "686/686 [==============================] - ETA: 0s - loss: 0.3052val_aucs: 0.514887307149553 0.8864812630298173\n",
      "686/686 [==============================] - 34s 50ms/step - loss: 0.3052 - custom_metric: 1.4014\n",
      "Epoch 9/1000\n",
      "445/686 [==================>...........] - ETA: 10s - loss: 0.2765"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "repeats = {k:10 for k in [10,20,30,40,50,60]}\n",
    "lds = [50, 60]\n",
    "batch_size, lr, patience = 32, 0.0005, 10\n",
    "d, N, he, dropout = 50,2,4,0.2\n",
    "fore_savepath = 'Exp_GloVe/M/models/forecasting/forecasting_109_epochs.h5'\n",
    "\n",
    "train_inds = np.arange(len(train_op))\n",
    "valid_inds = np.arange(len(valid_op))\n",
    "gen_res = {}\n",
    "\n",
    "np.random.seed(2023)\n",
    "for ld in lds:\n",
    "    np.random.shuffle(train_inds)\n",
    "    np.random.shuffle(valid_inds)\n",
    "    train_starts = [int(i) for i in np.linspace(0, len(train_inds)-int(ld*len(train_inds)/100), repeats[ld])]\n",
    "    valid_starts = [int(i) for i in np.linspace(0, len(valid_inds)-int(ld*len(valid_inds)/100), repeats[ld])]\n",
    "    all_test_res = []\n",
    "    for i in range(repeats[ld]):\n",
    "        print ('Repeat', i, 'ld', ld)\n",
    "        # Get train and validation data.\n",
    "        curr_train_ind = train_inds[np.arange(train_starts[i], train_starts[i]+int(ld*len(train_inds)/100))]\n",
    "        curr_valid_ind = valid_inds[np.arange(valid_starts[i], valid_starts[i]+int(ld*len(valid_inds)/100))]\n",
    "        curr_train_ip = [ip[curr_train_ind] for ip in train_ip]\n",
    "        curr_valid_ip = [ip[curr_valid_ind] for ip in valid_ip]\n",
    "        curr_train_op = train_op[curr_train_ind]\n",
    "        curr_valid_op = valid_op[curr_valid_ind]\n",
    "        print ('Num train:',len(curr_train_op),'Num valid:',len(curr_valid_op))\n",
    "        # Construct save_path.\n",
    "        savepath = 'CLS/GloVe/M/models/new_mimic_iii_24hm_strats_no_interp_with_ss_repeat'+str(i)+'_'+str(ld)+'ld'+'.h5'\n",
    "        print (savepath)\n",
    "        try:\n",
    "            os.remove(savepath)\n",
    "        except OSError:\n",
    "            pass\n",
    "        # Build and compile model.\n",
    "        model, fore_model =  build_strats(D, max_len, V, d, N, he, dropout, forecast=True)\n",
    "        model.compile(loss=mortality_loss, optimizer=Adam(lr))\n",
    "        fore_model.compile(loss=forecast_loss, optimizer=Adam(lr))\n",
    "        # Load pretrained weights here.\n",
    "        fore_model.load_weights(fore_savepath)\n",
    "        # Train model.\n",
    "        es = EarlyStopping(monitor='custom_metric', patience=patience, mode='max', \n",
    "                           restore_best_weights=True)\n",
    "        cus = CustomCallback(validation_data=(curr_valid_ip, curr_valid_op), batch_size=batch_size)\n",
    "        his = model.fit(curr_train_ip, curr_train_op, batch_size=batch_size, epochs=1000,\n",
    "                        verbose=1, callbacks=[cus, es]).history\n",
    "        model.save_weights(savepath)\n",
    "#         # Test and write to log.\n",
    "#         rocauc, prauc, minrp = get_res(test_op, model.predict(test_ip, verbose=0, batch_size=batch_size))\n",
    "#         f.write(str(np.min(his['custom_metric']))+str(rocauc)+str(prauc)+str(minrp)+savepath+'\\n')\n",
    "#         print ('Test res', rocauc, prauc, minrp)\n",
    "#         all_test_res.append([rocauc, prauc, minrp])\n",
    "        \n",
    "#     gen_res[ld] = []\n",
    "#     for i in range(len(all_test_res[0])):\n",
    "#         nums = [test_res[i] for test_res in all_test_res]\n",
    "#         gen_res[ld].append((np.mean(nums), np.std(nums)))\n",
    "#     print ('gen_res', gen_res)\n",
    "\n",
    "\n",
    "# # # save to local\n",
    "# # log_path = '/content/log.csv'\n",
    "# # files.download(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
